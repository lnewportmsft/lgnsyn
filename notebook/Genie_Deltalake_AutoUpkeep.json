{
	"name": "Genie_Deltalake_AutoUpkeep",
	"properties": {
		"description": "Genie_Deltalake_AutoUpkeep process",
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "lgnsynspark32",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "12e5cb54-6c6c-4b70-bd5d-e600482519d3"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/d926eada-2849-4d1f-9a0c-89b8eb87c962/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsyn/bigDataPools/lgnsynspark32",
				"name": "lgnsynspark32",
				"type": "Spark",
				"endpoint": "https://lgnsyn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark32",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# This script collects statistics on various delta lake tables and determines if they need to be vacuumed\r\n",
					"# It optimizes and Vacuum all the delta lake tables"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Configs for Delta lake Maintenance **"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Configs for Delta lake Maintenance \r\n",
					"\r\n",
					"# Decides the recent  timeperiod to retain history , Default 1 days (24 * 7 = 168 hours)\r\n",
					"vacuumRetentionInHours = 168\r\n",
					"\r\n",
					"# Low version count indicates the table has not been through any updates and does not need Maintenance(optimize and Vacuum)\r\n",
					"minVersionCount = 3\r\n",
					"\r\n",
					"# New tables within a certain time period will be ignore from Maintenance(optimize and Vacuum)\r\n",
					"ignore_New_Tabledays = 7"
				],
				"execution_count": 153
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Configs for Delta lake execution\r\n",
					"\r\n",
					"# By default threads is set equal to the number of executor cores. \"threads\" variable can be tuned to a particular number depending on requirements\r\n",
					"\r\n",
					"import os\r\n",
					"executorCores = (sc._jsc.sc().getExecutorMemoryStatus().keySet().size()-1)*os.cpu_count() \r\n",
					"\r\n",
					"min_Workers_Heavyprocess = executorCores / 2\r\n",
					"# atleast 2 cores per thread will be provided\r\n",
					"\r\n",
					"max_Workers_Lightprocess = executorCores\r\n",
					"# atleast 1 core per thread will be provided"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**List all Databases of Spark**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"fullDatabaseList = spark.sql(\"Show Databases\").rdd.map(lambda r : r[0]).collect()"
				],
				"execution_count": 154
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Blacklist databases** that dont need to be optimized through this script"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Blacklist databases that dont need to be optimized through this script\r\n",
					"# ignoreDatabases = ['onehrsi','crm','aurora']\r\n",
					"ignoreDatabases = []\r\n",
					"\r\n",
					"dbList = [i for i in fullDatabaseList if i not in ignoreDatabases]\r\n",
					"\r\n",
					"print(dbList)"
				],
				"execution_count": 155
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# temp restriction for debugging (optional)\r\n",
					"\r\n",
					"# dbList = [ 'genie', 'temp']"
				],
				"execution_count": 156
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class TableDetail:\r\n",
					"  def __init__(self, tableName, location, isDelta):     \r\n",
					"        # Instance Variable    \r\n",
					"        self.tableName = tableName\r\n",
					"        self.location = location \r\n",
					"        self.isDelta = isDelta"
				],
				"execution_count": 157
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Key Method to run tasks in parallel**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import concurrent.futures\r\n",
					"\r\n",
					"workerCount=4\r\n",
					"exceptionDetails = []\r\n",
					"\r\n",
					"def runParallelTasks(taskName, paramList, workerCount):   \r\n",
					"    resultDict = {}\r\n",
					"\r\n",
					"    with concurrent.futures.ThreadPoolExecutor(workerCount) as executor:\r\n",
					"        taskResults = {executor.submit(taskName, param) : (param) for param in paramList}\r\n",
					"        for f in concurrent.futures.as_completed(taskResults.keys()):\r\n",
					"            result = f.result()\r\n",
					"            if result:\r\n",
					"                resultDict[taskResults[f]] = result      \r\n",
					"    return resultDict"
				],
				"execution_count": 158
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def flattenDictionaryValues(resultDict):\r\n",
					"    flatTabList = []\r\n",
					"    for k in resultDict.keys():\r\n",
					"        for t2 in resultDict[k]:\r\n",
					"            flatTabList.append(t2)\r\n",
					"    return flatTabList"
				],
				"execution_count": 159
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Lists all tables in the Hive metastore**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def getAllTableNames(databaseName):\r\n",
					"    #code to fetch tables and verify if delta\r\n",
					"    print(\"Initiate get tables list for database: \"+ databaseName)\r\n",
					"    newTables=[]\r\n",
					"    if databaseName == \"default\":\r\n",
					"        return newTables\r\n",
					"    \r\n",
					"    tablesDF = spark.sql(\"Show tables from \"+databaseName).collect()\r\n",
					"    for t in tablesDF:\r\n",
					"        if databaseName == \"default\":\r\n",
					"            tName = t[1]\r\n",
					"            newTables.append(tName)\r\n",
					"        elif t[2] == False : \r\n",
					"            tName = databaseName + \".\" + t[1] \r\n",
					"            newTables.append(tName)  \r\n",
					"        \r\n",
					"        print( \"TableFullName : \" + databaseName+ \".\" + t[1]  )    \r\n",
					"    return newTables"
				],
				"execution_count": 160
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"max_workers = max_Workers_Lightprocess\r\n",
					"\r\n",
					"tResults = runParallelTasks(getAllTableNames, dbList, max_workers)\r\n",
					"\r\n",
					"allTables = flattenDictionaryValues(tResults)\r\n",
					"print(\"All Tables found : \"+str(len(allTables)))"
				],
				"execution_count": 161
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Blacklist tables** that dont need to be optimized through this script"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Blacklist tables that dont need to be optimized through this script\r\n",
					"# ignoreTables = ['ihr.tabfilesvw']\r\n",
					"ignoreTables = []\r\n",
					"\r\n",
					"allApprovedTables = [i for i in allTables if i not in ignoreTables]\r\n",
					"\r\n",
					"# print(allApprovedTables)"
				],
				"execution_count": 162
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import sys\r\n",
					"\r\n",
					"deltaTablesDetails = []\r\n",
					"\r\n",
					"def getDeltaDetails(tableName):\r\n",
					"  query = \"Describe detail \"+ tableName\r\n",
					"  result = False\r\n",
					"  try:\r\n",
					"    tableDetail = spark.sql(query).collect()\r\n",
					"    if tableDetail[0].format == \"delta\" and tableDetail[0].id is not None :\r\n",
					"      result = True \r\n",
					"      deltaTablesDetails.append(TableDetail(tableName, tableDetail[0].location, 'True') )\r\n",
					"    else:\r\n",
					"      result = False\r\n",
					"  except Exception as e:\r\n",
					"    exceptionInfo = sys.exc_info()[0]\r\n",
					"    # Non delta tables throw exception. Uncomment below to log all errors\r\n",
					"\r\n",
					"    # print(\"Exception occurred :\" + tableName)\r\n",
					"    # print(sys.exc_info()[0])\r\n",
					"    # print(str(e))    \r\n",
					"    # exceptionDetails.append((tableName,\"TableDetailStage\", str(e)))\r\n",
					"    # print(\"Exception occurred :\" + tableName +\" : \" + str(e))\r\n",
					"    \r\n",
					"  return result"
				],
				"execution_count": 163
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"max_workers=max_Workers_Lightprocess\r\n",
					"\r\n",
					"deltaTablesDetails = []\r\n",
					"tResults = runParallelTasks(getDeltaDetails, allApprovedTables, max_workers)\r\n",
					"\r\n",
					"\r\n",
					"print(\"All Delta Tables found : \"+str(len(tResults)))"
				],
				"execution_count": 164
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class TableHistoryDetail:\r\n",
					"  def __init__(self, tableName, version, timestamp):     \r\n",
					"        # Instance Variable    \r\n",
					"        self.tableName = tableName\r\n",
					"        self.version = version \r\n",
					"        self.timestamp = timestamp"
				],
				"execution_count": 165
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# deltaTablesDetails\r\n",
					"tableHistoryDetails = []\r\n",
					"\r\n",
					"def getTableHistory(deltaTableDetail):\r\n",
					"    tabName = deltaTableDetail.tableName\r\n",
					"    try:\r\n",
					"        tabHistory = spark.sql(\"Describe history \"+ tabName).collect()\r\n",
					"        for thd in tabHistory:\r\n",
					"            tableHistoryDetails.append(TableHistoryDetail(tabName, thd.version, thd.timestamp))\r\n",
					"    except Exception as e:    \r\n",
					"        exceptionInfo = sys.exc_info()[0]\r\n",
					"        exceptionDetails.append((tabName,\"TableHistoryVersionStage\", str(e)))\r\n",
					"        print(\"Exception occurred at gathering History :\" + tabName)\r\n",
					"    \r\n",
					"    return True"
				],
				"execution_count": 166
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Get History and version of tables**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"max_workers = max_Workers_Lightprocess\r\n",
					"tableHistoryDetails = []\r\n",
					"tResults = runParallelTasks(getTableHistory, deltaTablesDetails, max_workers)\r\n",
					""
				],
				"execution_count": 167
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import *\r\n",
					"\r\n",
					"field = [StructField(\"tableName\", StringType(), True), StructField(\"version\",LongType(), True),StructField(\"timestamp\", TimestampType(), True)]\r\n",
					"schema = StructType(field)\r\n",
					"\r\n",
					"cols = ['tableName','version', 'timestamp'] "
				],
				"execution_count": 168
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"tableHistoryList = []\r\n",
					"\r\n",
					"for k in tableHistoryDetails:\r\n",
					"        tableHistoryList.append([k.tableName, k.version, k.timestamp])\r\n",
					"    \r\n",
					"thDF = spark.createDataFrame(tableHistoryList, cols)"
				],
				"execution_count": 169
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"thDF.createOrReplaceTempView(\"TabHistoryVw\")"
				],
				"execution_count": 170
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"CREATE OR REPLACE TEMPORARY VIEW TabVersionVW AS (\r\n",
					"Select TableName, count(*) as VersionCount, min(Timestamp) as OldestVersionDate,  max(Timestamp) as MostRecentVersionDate  from TabHistoryVw\r\n",
					"group by tableName\r\n",
					"order by 2 desc, 3 ASC\r\n",
					")"
				],
				"execution_count": 171
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"drop table if exists TabVersion;"
				],
				"execution_count": 172
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"Create table if not exists TabVersion\r\n",
					"Select * from TabVersionVW"
				],
				"execution_count": 173
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class TableFiles:\r\n",
					"  def __init__(self, tableName, location, fileCount, totalSizeInBytes, totalSizeInGBs):     \r\n",
					"        # Instance Variable    \r\n",
					"        self.tableName = tableName\r\n",
					"        self.location = location \r\n",
					"        self.fileCount = fileCount\r\n",
					"        self.totalSizeInBytes = totalSizeInBytes\r\n",
					"        self.totalSizeInGBs = totalSizeInGBs"
				],
				"execution_count": 174
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_dir_content(ls_path):\r\n",
					"  baseFilesInfo = mssparkutils.fs.ls(ls_path)\r\n",
					"  subdir_filesInfo = [get_dir_content(p.path) for p in baseFilesInfo if p.isDir and p.path != ls_path]\r\n",
					"  flat_subdir_FilesInfo = [p for subFileInfo in subdir_filesInfo for p in subFileInfo]\r\n",
					"  return list(map(lambda p: p, baseFilesInfo)) + flat_subdir_FilesInfo"
				],
				"execution_count": 175
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"tabFiles = []\r\n",
					"def getTableFileDetails(tabDetail):\r\n",
					"    tabName = tabDetail.tableName\r\n",
					"    tabLocation = tabDetail.location\r\n",
					"    \r\n",
					"    try:\r\n",
					"        files =  get_dir_content(tabLocation)\r\n",
					"        fileCount = len(files)\r\n",
					"        s = 0\r\n",
					"        for f in files:\r\n",
					"            s = s + f.size\r\n",
					"        totalSizeInBytes = float(s)\r\n",
					"        totalSizeInGBs = float(s) / 1073741824\r\n",
					"        tabFile = TableFiles(tabName, tabLocation, fileCount, totalSizeInBytes, totalSizeInGBs)\r\n",
					"        print(tabName+\" : \"+ str(totalSizeInGBs) + \"GB\")\r\n",
					"        tabFiles.append(tabFile)\r\n",
					"    except Exception as e:    \r\n",
					"        exceptionInfo = sys.exc_info()[0]\r\n",
					"        exceptionDetails.append((tabName,\"ListFilesInfoStage\", str(e)))\r\n",
					"        print(\"Exception occurred at gathering files info stage :\" + tabName)\r\n",
					"\r\n",
					"    return True"
				],
				"execution_count": 176
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"max_workers = max_Workers_Lightprocess\r\n",
					"tabFiles = []\r\n",
					"\r\n",
					"tResults = runParallelTasks(getTableFileDetails, deltaTablesDetails, max_workers)"
				],
				"execution_count": 177
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"tabFilesDF = spark.createDataFrame(tabFiles, None)\r\n",
					"tabFilesDF.createOrReplaceTempView(\"tabFilesVW\")"
				],
				"execution_count": 178
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"CREATE OR REPLACE TEMPORARY VIEW DeltaLakeTablesVW AS \r\n",
					"(\r\n",
					"Select TV.TableName, totalSizeInGBs, OldestversionDate, VersionCount, FileCount, location, totalSizeInBytes, MostRecentVersionDate\r\n",
					"-- from TabVersionVW TV\r\n",
					"from TabVersion TV\r\n",
					"left Join tabFilesVW TF on TF.tableName = TV.tableName \r\n",
					"order by totalSizeInGBs desc, VersionCount desc\r\n",
					")"
				],
				"execution_count": 179
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"Drop table if exists DeltaLakeTables"
				],
				"execution_count": 180
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"Create table if not exists DeltaLakeTables\r\n",
					"Select * from DeltaLakeTablesVW"
				],
				"execution_count": 181
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"-- Display data statistics of Delta lake tables \r\n",
					"Select * from DeltaLakeTables\r\n",
					"order by totalSizeInGBs desc, VersionCount desc"
				],
				"execution_count": 182
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"-- Delta Lake Size for this ADB workspace\r\n",
					"select sum(totalSizeInGBs) from DeltaLakeTables "
				],
				"execution_count": 183
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"-- Delta Lake Size for this ADB workspace that can be cleaned up with Vacuum scripts\r\n",
					"Select sum(totalSizeInGBs) from DeltaLakeTables where VersionCount >= 7  and DateDiff(Current_TimeStamp(),OldestVersionDate) > 7 --  "
				],
				"execution_count": 184
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"scriptGenerateQuery = f\"\"\"Select distinct Concat('Optimize ', tableName) as optimizeScript, Concat('Vacuum ', tableName, ' retain {vacuumRetentionInHours} hours') As vacuumScript \r\n",
					"from DeltaLakeTables \r\n",
					"where VersionCount >= {minVersionCount}  and DateDiff(Current_TimeStamp(),OldestVersionDate) > {ignore_New_Tabledays} \"\"\"\r\n",
					"\r\n",
					"print(scriptGenerateQuery) "
				],
				"execution_count": 185
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"runScripts = []\r\n",
					"\r\n",
					"runScripts = spark.sql(scriptGenerateQuery). select(\"optimizeScript\",\"vacuumScript\").rdd.map(lambda r : (r[0],r[1])).collect()"
				],
				"execution_count": 186
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def execOptimizeVacuum(query):\r\n",
					"    print(\"Initiate optimize :  \"+ query[0])\r\n",
					"    spark.sql(query[0])\r\n",
					"\r\n",
					"    print(\"completed optimize :  \"+ query[0])\r\n",
					"\r\n",
					"    spark.sql(query[1])\r\n",
					"    print(\"completed vacuum :  \"+ query[1])\r\n",
					"    \r\n",
					"    return True"
				],
				"execution_count": 187
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"max_workers = min_Workers_Heavyprocess\r\n",
					"\r\n",
					"tResults = runParallelTasks(execOptimizeVacuum, runScripts, max_workers)"
				],
				"execution_count": 188
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"-- Clean up with below script once done\r\n",
					"drop table if exists TabVersion;\r\n",
					"drop table if exists DeltaLakeTables;"
				],
				"execution_count": 189
			}
		]
	}
}