{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "lgnsyn"
		},
		"AzureSqlDatabase_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSqlDatabase'"
		},
		"SQLServer2022_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'SQLServer2022'"
		},
		"lgnsyn-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'lgnsyn-WorkspaceDefaultSqlServer'"
		},
		"lgnsyn-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://lgnsynlake.dfs.core.windows.net"
		},
		"Trigger_7p7_properties_LNewport2_AdventureWorks_7p7_TopLevel_parameters_windowStart": {
			"type": "string",
			"defaultValue": "@trigger().scheduledTime"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/LNewport2_AdventureWorks_7p7_BottomLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will copy objects from one group. The objects belonging to this group will be copied parallelly.",
				"activities": [
					{
						"name": "ListObjectsFromOneGroup",
						"description": "List objects from one group and iterate each of them to downstream activities",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.ObjectsPerGroupToCopy",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "RouteJobsBasedOnLoadingBehavior",
									"description": "Check the loading behavior for each object if it requires full load or incremental load. If it is Default or FullLoad case, do full load. If it is DeltaLoad case, do incremental load.",
									"type": "Switch",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"on": {
											"value": "@json(item().DataLoadingBehaviorSettings).dataLoadingBehavior",
											"type": "Expression"
										},
										"cases": [
											{
												"value": "FullLoad",
												"activities": [
													{
														"name": "FullLoadOneObject",
														"description": "Take a full snapshot on this object and copy it to the destination",
														"type": "Copy",
														"dependsOn": [],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [
															{
																"name": "Source",
																"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
															},
															{
																"name": "Destination",
																"value": "@{json(item().SinkObjectSettings).fileSystem}/AdventureWorks/@{formatDateTime(pipeline().parameters.windowStart,'yyyy')}/@{formatDateTime(pipeline().parameters.windowStart,'MM')}/@{formatDateTime(pipeline().parameters.windowStart,'dd')}/@{json(item().SinkObjectSettings).fileName}"
															}
														],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "@json(item().CopySourceSettings).sqlReaderQuery",
																	"type": "Expression"
																},
																"partitionOption": {
																	"value": "@json(item().CopySourceSettings).partitionOption",
																	"type": "Expression"
																},
																"partitionSettings": {
																	"partitionColumnName": {
																		"value": "@json(item().CopySourceSettings).partitionColumnName",
																		"type": "Expression"
																	},
																	"partitionUpperBound": {
																		"value": "@json(item().CopySourceSettings).partitionUpperBound",
																		"type": "Expression"
																	},
																	"partitionLowerBound": {
																		"value": "@json(item().CopySourceSettings).partitionLowerBound",
																		"type": "Expression"
																	},
																	"partitionNames": "@json(item().CopySourceSettings).partitionNames"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"value": "@json(item().CopyActivitySettings).translator",
																"type": "Expression"
															}
														},
														"inputs": [
															{
																"referenceName": "LNewport2_AdventureWorks_7p7_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "LNewport2_AdventureWorks_7p7_DestinationDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_fileName": {
																		"value": "@json(item().SinkObjectSettings).fileName",
																		"type": "Expression"
																	},
																	"cw_folderPath": {
																		"value": "AdventureWorks/@{formatDateTime(pipeline().parameters.windowStart,'yyyy')}/@{formatDateTime(pipeline().parameters.windowStart,'MM')}/@{formatDateTime(pipeline().parameters.windowStart,'dd')}",
																		"type": "Expression"
																	},
																	"cw_fileSystem": {
																		"value": "@json(item().SinkObjectSettings).fileSystem",
																		"type": "Expression"
																	}
																}
															}
														]
													}
												]
											},
											{
												"value": "DeltaLoad",
												"activities": [
													{
														"name": "GetMaxWatermarkValue",
														"description": "Query the source object to get the max value from watermark column",
														"type": "Lookup",
														"dependsOn": [],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "select max([@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}]) as CurrentMaxWaterMarkColumnValue from [@{json(item().SourceObjectSettings).schema}].[@{json(item().SourceObjectSettings).table}]",
																	"type": "Expression"
																},
																"partitionOption": "None"
															},
															"dataset": {
																"referenceName": "LNewport2_AdventureWorks_7p7_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	}
																}
															}
														}
													},
													{
														"name": "DeltaLoadOneObject",
														"description": "Copy the changed data only from last time via comparing the value in watermark column to identify changes.",
														"type": "Copy",
														"dependsOn": [
															{
																"activity": "GetMaxWatermarkValue",
																"dependencyConditions": [
																	"Succeeded"
																]
															}
														],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [
															{
																"name": "Source",
																"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
															},
															{
																"name": "Destination",
																"value": "@{json(item().SinkObjectSettings).fileSystem}/AdventureWorks/@{formatDateTime(pipeline().parameters.windowStart,'yyyy')}/@{formatDateTime(pipeline().parameters.windowStart,'MM')}/@{formatDateTime(pipeline().parameters.windowStart,'dd')}/@{json(item().SinkObjectSettings).fileName}"
															}
														],
														"typeProperties": {
															"source": {
																"type": "AzureSqlSource",
																"sqlReaderQuery": {
																	"value": "select * from [@{json(item().SourceObjectSettings).schema}].[@{json(item().SourceObjectSettings).table}] \n    where [@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}] > @{if(contains(json(item().DataLoadingBehaviorSettings).watermarkColumnType, 'Int'),\n    json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue, \n    concat('''', json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue, ''''))}\n    and [@{json(item().DataLoadingBehaviorSettings).watermarkColumnName}] <= @{if(contains(json(item().DataLoadingBehaviorSettings).watermarkColumnType, 'Int'),\n    activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue, \n    concat('''', activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue, ''''))}",
																	"type": "Expression"
																},
																"partitionOption": {
																	"value": "@json(item().CopySourceSettings).partitionOption",
																	"type": "Expression"
																},
																"partitionSettings": {
																	"partitionColumnName": {
																		"value": "@json(item().CopySourceSettings).partitionColumnName",
																		"type": "Expression"
																	},
																	"partitionUpperBound": {
																		"value": "@json(item().CopySourceSettings).partitionUpperBound",
																		"type": "Expression"
																	},
																	"partitionLowerBound": {
																		"value": "@json(item().CopySourceSettings).partitionLowerBound",
																		"type": "Expression"
																	},
																	"partitionNames": "@json(item().CopySourceSettings).partitionNames"
																}
															},
															"sink": {
																"type": "ParquetSink",
																"storeSettings": {
																	"type": "AzureBlobFSWriteSettings"
																},
																"formatSettings": {
																	"type": "ParquetWriteSettings"
																}
															},
															"enableStaging": false,
															"validateDataConsistency": false,
															"translator": {
																"value": "@json(item().CopyActivitySettings).translator",
																"type": "Expression"
															}
														},
														"inputs": [
															{
																"referenceName": "LNewport2_AdventureWorks_7p7_SourceDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_schema": {
																		"value": "@json(item().SourceObjectSettings).schema",
																		"type": "Expression"
																	},
																	"cw_table": {
																		"value": "@json(item().SourceObjectSettings).table",
																		"type": "Expression"
																	}
																}
															}
														],
														"outputs": [
															{
																"referenceName": "LNewport2_AdventureWorks_7p7_DestinationDS",
																"type": "DatasetReference",
																"parameters": {
																	"cw_fileName": {
																		"value": "@{json(item().SinkObjectSettings).fileName}-@{json(item().DataLoadingBehaviorSettings).watermarkColumnStartValue}-@{activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue}",
																		"type": "Expression"
																	},
																	"cw_folderPath": {
																		"value": "AdventureWorks/@{formatDateTime(pipeline().parameters.windowStart,'yyyy')}/@{formatDateTime(pipeline().parameters.windowStart,'MM')}/@{formatDateTime(pipeline().parameters.windowStart,'dd')}",
																		"type": "Expression"
																	},
																	"cw_fileSystem": {
																		"value": "@json(item().SinkObjectSettings).fileSystem",
																		"type": "Expression"
																	}
																}
															}
														]
													},
													{
														"name": "UpdateWatermarkColumnValue",
														"type": "SqlServerStoredProcedure",
														"dependsOn": [
															{
																"activity": "DeltaLoadOneObject",
																"dependencyConditions": [
																	"Succeeded"
																]
															}
														],
														"policy": {
															"timeout": "7.00:00:00",
															"retry": 0,
															"retryIntervalInSeconds": 30,
															"secureOutput": false,
															"secureInput": false
														},
														"userProperties": [],
														"typeProperties": {
															"storedProcedureName": "[dbo].[UpdateWatermarkColumnValue_7p7]",
															"storedProcedureParameters": {
																"Id": {
																	"value": {
																		"value": "@item().Id",
																		"type": "Expression"
																	},
																	"type": "Int32"
																},
																"watermarkColumnStartValue": {
																	"value": {
																		"value": "@activity('GetMaxWatermarkValue').output.firstRow.CurrentMaxWaterMarkColumnValue",
																		"type": "Expression"
																	},
																	"type": "String"
																}
															}
														},
														"linkedServiceName": {
															"referenceName": "AzureSqlDatabase",
															"type": "LinkedServiceReference"
														}
													}
												]
											}
										],
										"defaultActivities": [
											{
												"name": "DefaultFullLoadOneObject",
												"description": "Take a full snapshot on this object and copy it to the destination",
												"type": "Copy",
												"dependsOn": [],
												"policy": {
													"timeout": "7.00:00:00",
													"retry": 0,
													"retryIntervalInSeconds": 30,
													"secureOutput": false,
													"secureInput": false
												},
												"userProperties": [
													{
														"name": "Source",
														"value": "@{json(item().SourceObjectSettings).schema}.@{json(item().SourceObjectSettings).table}"
													},
													{
														"name": "Destination",
														"value": "@{json(item().SinkObjectSettings).fileSystem}/AdventureWorks/@{formatDateTime(pipeline().parameters.windowStart,'yyyy')}/@{formatDateTime(pipeline().parameters.windowStart,'MM')}/@{formatDateTime(pipeline().parameters.windowStart,'dd')}/@{json(item().SinkObjectSettings).fileName}"
													}
												],
												"typeProperties": {
													"source": {
														"type": "AzureSqlSource",
														"sqlReaderQuery": {
															"value": "@json(item().CopySourceSettings).sqlReaderQuery",
															"type": "Expression"
														},
														"partitionOption": {
															"value": "@json(item().CopySourceSettings).partitionOption",
															"type": "Expression"
														},
														"partitionSettings": {
															"partitionColumnName": {
																"value": "@json(item().CopySourceSettings).partitionColumnName",
																"type": "Expression"
															},
															"partitionUpperBound": {
																"value": "@json(item().CopySourceSettings).partitionUpperBound",
																"type": "Expression"
															},
															"partitionLowerBound": {
																"value": "@json(item().CopySourceSettings).partitionLowerBound",
																"type": "Expression"
															},
															"partitionNames": "@json(item().CopySourceSettings).partitionNames"
														}
													},
													"sink": {
														"type": "ParquetSink",
														"storeSettings": {
															"type": "AzureBlobFSWriteSettings"
														},
														"formatSettings": {
															"type": "ParquetWriteSettings"
														}
													},
													"enableStaging": false,
													"validateDataConsistency": false,
													"translator": {
														"value": "@json(item().CopyActivitySettings).translator",
														"type": "Expression"
													}
												},
												"inputs": [
													{
														"referenceName": "LNewport2_AdventureWorks_7p7_SourceDS",
														"type": "DatasetReference",
														"parameters": {
															"cw_schema": {
																"value": "@json(item().SourceObjectSettings).schema",
																"type": "Expression"
															},
															"cw_table": {
																"value": "@json(item().SourceObjectSettings).table",
																"type": "Expression"
															}
														}
													}
												],
												"outputs": [
													{
														"referenceName": "LNewport2_AdventureWorks_7p7_DestinationDS",
														"type": "DatasetReference",
														"parameters": {
															"cw_fileName": {
																"value": "@json(item().SinkObjectSettings).fileName",
																"type": "Expression"
															},
															"cw_folderPath": {
																"value": "AdventureWorks/@{formatDateTime(pipeline().parameters.windowStart,'yyyy')}/@{formatDateTime(pipeline().parameters.windowStart,'MM')}/@{formatDateTime(pipeline().parameters.windowStart,'dd')}",
																"type": "Expression"
															},
															"cw_fileSystem": {
																"value": "@json(item().SinkObjectSettings).fileSystem",
																"type": "Expression"
															}
														}
													}
												]
											}
										]
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"ObjectsPerGroupToCopy": {
						"type": "Array"
					},
					"windowStart": {
						"type": "String"
					}
				},
				"folder": {
					"name": "LNewport2_AdventureWorks_7p7_20220817"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/LNewport2_AdventureWorks_7p7_SourceDS')]",
				"[concat(variables('workspaceId'), '/datasets/LNewport2_AdventureWorks_7p7_DestinationDS')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlDatabase')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LNewport2_AdventureWorks_7p7_MiddleLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will copy one batch of objects. The objects belonging to this batch will be copied parallelly.",
				"activities": [
					{
						"name": "DivideOneBatchIntoMultipleGroups",
						"description": "Divide objects from single batch into multiple sub parallel groups to avoid reaching the output limit of lookup activity.",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0, add(div(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                    if(equals(mod(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), 0), 0, 1)))",
								"type": "Expression"
							},
							"isSequential": false,
							"batchCount": 50,
							"activities": [
								{
									"name": "GetObjectsPerGroupToCopy",
									"description": "Get objects (tables etc.) from control table required to be copied in this group. The order of objects to be copied following the TaskId in control table (ORDER BY [TaskId] DESC).",
									"type": "Lookup",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "AzureSqlSource",
											"sqlReaderQuery": {
												"value": "WITH OrderedControlTable AS (\n                             SELECT *, ROW_NUMBER() OVER (ORDER BY [TaskId], [Id] DESC) AS RowNumber\n                             FROM @{pipeline().parameters.MainControlTableName}\n                             where TopLevelPipelineName = '@{pipeline().parameters.TopLevelPipelineName}'\n                             and TriggerName like '%@{pipeline().parameters.TriggerName}%' and CopyEnabled = 1)\n                             SELECT * FROM OrderedControlTable WHERE RowNumber BETWEEN @{add(mul(int(item()),pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity),\n                             add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch), 1))}\n                             AND @{min(add(mul(int(item()), pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity), add(mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, pipeline().parameters.CurrentSequentialNumberOfBatch),\n                             pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity)),\n                            mul(pipeline().parameters.SumOfObjectsToCopyForCurrentBatch, add(pipeline().parameters.CurrentSequentialNumberOfBatch,1)), pipeline().parameters.SumOfObjectsToCopy)}",
												"type": "Expression"
											},
											"partitionOption": "None"
										},
										"dataset": {
											"referenceName": "LNewport2_AdventureWorks_7p7_ControlDS",
											"type": "DatasetReference",
											"parameters": {}
										},
										"firstRowOnly": false
									}
								},
								{
									"name": "CopyObjectsInOneGroup",
									"description": "Execute another pipeline to copy objects from one group. The objects belonging to this group will be copied parallelly.",
									"type": "ExecutePipeline",
									"dependsOn": [
										{
											"activity": "GetObjectsPerGroupToCopy",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "LNewport2_AdventureWorks_7p7_BottomLevel",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"ObjectsPerGroupToCopy": {
												"value": "@activity('GetObjectsPerGroupToCopy').output.value",
												"type": "Expression"
											},
											"windowStart": {
												"value": "@pipeline().parameters.windowStart",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"MaxNumberOfObjectsReturnedFromLookupActivity": {
						"type": "Int"
					},
					"TopLevelPipelineName": {
						"type": "String"
					},
					"TriggerName": {
						"type": "String"
					},
					"CurrentSequentialNumberOfBatch": {
						"type": "Int"
					},
					"SumOfObjectsToCopy": {
						"type": "Int"
					},
					"SumOfObjectsToCopyForCurrentBatch": {
						"type": "Int"
					},
					"MainControlTableName": {
						"type": "String"
					},
					"windowStart": {
						"type": "String"
					}
				},
				"folder": {
					"name": "LNewport2_AdventureWorks_7p7_20220817"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/LNewport2_AdventureWorks_7p7_ControlDS')]",
				"[concat(variables('workspaceId'), '/pipelines/LNewport2_AdventureWorks_7p7_BottomLevel')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LNewport2_AdventureWorks_7p7_TopLevel')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "This pipeline will count the total number of objects (tables etc.) required to be copied in this run, come up with the number of sequential batches based on the max allowed concurrent copy task, and then execute another pipeline to copy different batches sequentially.",
				"activities": [
					{
						"name": "GetSumOfObjectsToCopy",
						"description": "Count the total number of objects (tables etc.) required to be copied in this run.",
						"type": "Lookup",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"sqlReaderQuery": {
									"value": "SELECT count(*) as count FROM @{pipeline().parameters.MainControlTableName} where TopLevelPipelineName='@{pipeline().Pipeline}' and TriggerName like '%@{pipeline().TriggerName}%' and CopyEnabled = 1",
									"type": "Expression"
								},
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "LNewport2_AdventureWorks_7p7_ControlDS",
								"type": "DatasetReference",
								"parameters": {}
							}
						}
					},
					{
						"name": "CopyBatchesOfObjectsSequentially",
						"description": "Come up with the number of sequential batches based on the max allowed concurrent copy tasks, and then execute another pipeline to copy different batches sequentially.",
						"type": "ForEach",
						"dependsOn": [
							{
								"activity": "GetSumOfObjectsToCopy",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@range(0, add(div(activity('GetSumOfObjectsToCopy').output.firstRow.count,\n                    pipeline().parameters.MaxNumberOfConcurrentTasks),\n                    if(equals(mod(activity('GetSumOfObjectsToCopy').output.firstRow.count,\n                    pipeline().parameters.MaxNumberOfConcurrentTasks), 0), 0, 1)))",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "CopyObjectsInOneBatch",
									"description": "Execute another pipeline to copy one batch of objects. The objects belonging to this batch will be copied parallelly.",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "LNewport2_AdventureWorks_7p7_MiddleLevel",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"MaxNumberOfObjectsReturnedFromLookupActivity": {
												"value": "@pipeline().parameters.MaxNumberOfObjectsReturnedFromLookupActivity",
												"type": "Expression"
											},
											"TopLevelPipelineName": {
												"value": "@{pipeline().Pipeline}",
												"type": "Expression"
											},
											"TriggerName": {
												"value": "@{pipeline().TriggerName}",
												"type": "Expression"
											},
											"CurrentSequentialNumberOfBatch": {
												"value": "@item()",
												"type": "Expression"
											},
											"SumOfObjectsToCopy": {
												"value": "@activity('GetSumOfObjectsToCopy').output.firstRow.count",
												"type": "Expression"
											},
											"SumOfObjectsToCopyForCurrentBatch": {
												"value": "@min(pipeline().parameters.MaxNumberOfConcurrentTasks, activity('GetSumOfObjectsToCopy').output.firstRow.count)",
												"type": "Expression"
											},
											"MainControlTableName": {
												"value": "@pipeline().parameters.MainControlTableName",
												"type": "Expression"
											},
											"windowStart": {
												"value": "@pipeline().parameters.windowStart",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"MaxNumberOfObjectsReturnedFromLookupActivity": {
						"type": "Int",
						"defaultValue": 5000
					},
					"MaxNumberOfConcurrentTasks": {
						"type": "Int",
						"defaultValue": 20
					},
					"MainControlTableName": {
						"type": "String",
						"defaultValue": "dbo.MainControlTable"
					},
					"windowStart": {
						"type": "String"
					}
				},
				"folder": {
					"name": "LNewport2_AdventureWorks_7p7_20220817"
				},
				"annotations": [
					"MetadataDrivenSolution"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/LNewport2_AdventureWorks_7p7_ControlDS')]",
				"[concat(variables('workspaceId'), '/pipelines/LNewport2_AdventureWorks_7p7_MiddleLevel')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LNewport2_AdventureWorks_7p7_ControlDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlDatabase",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "LNewport2_AdventureWorks_7p7_20220817"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "MainControlTable"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlDatabase')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LNewport2_AdventureWorks_7p7_DestinationDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "lgnsyn-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_fileName": {
						"type": "String"
					},
					"cw_folderPath": {
						"type": "String"
					},
					"cw_fileSystem": {
						"type": "String"
					}
				},
				"folder": {
					"name": "LNewport2_AdventureWorks_7p7_20220817"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().cw_fileName",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@dataset().cw_folderPath",
							"type": "Expression"
						},
						"fileSystem": {
							"value": "@dataset().cw_fileSystem",
							"type": "Expression"
						}
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/lgnsyn-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LNewport2_AdventureWorks_7p7_SourceDS')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSqlDatabase",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"cw_schema": {
						"type": "String"
					},
					"cw_table": {
						"type": "String"
					}
				},
				"folder": {
					"name": "LNewport2_AdventureWorks_7p7_20220817"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().cw_schema",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().cw_table",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSqlDatabase')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlServerTable')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SQLServer2022",
					"type": "LinkedServiceReference",
					"parameters": {
						"dbname": "TSQL2012"
					}
				},
				"parameters": {
					"schemaName": {
						"type": "string",
						"defaultValue": "Sales"
					},
					"tableName": {
						"type": "string",
						"defaultValue": "Orders"
					}
				},
				"annotations": [],
				"type": "SqlServerTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().schemaName",
						"type": "Expression"
					},
					"table": {
						"value": "@dataset().tableName",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SQLServer2022')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlDatabase')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AzureSqlDatabase_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLServer2022')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"dbname": {
						"type": "String",
						"defaultValue": "TSQL2012"
					}
				},
				"annotations": [],
				"type": "SqlServer",
				"typeProperties": {
					"connectionString": "[parameters('SQLServer2022_connectionString')]"
				},
				"connectVia": {
					"referenceName": "MSI",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/MSI')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/lgnsyn-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('lgnsyn-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/lgnsyn-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('lgnsyn-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Trigger_7p7')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "LNewport2_AdventureWorks_7p7_TopLevel",
							"type": "PipelineReference"
						},
						"parameters": {
							"windowStart": {
								"type": "Expression",
								"value": "[parameters('Trigger_7p7_properties_LNewport2_AdventureWorks_7p7_TopLevel_parameters_windowStart')]"
							}
						}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Hour",
						"interval": 24,
						"startTime": "2022-08-17T14:01:00Z",
						"timeZone": "UTC"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/LNewport2_AdventureWorks_7p7_TopLevel')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MSI')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01 Read and write data from Azure Data Lake Storage Gen2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PySpark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f0805264-2f14-43b5-baed-53441246b63b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Access data on Azure Data Lake Storage Gen2 (ADLS Gen2) with Synapse Spark\n",
							"\n",
							"Azure Data Lake Storage Gen2 (ADLS Gen2) is used as the storage account associated with a Synapse workspace. A synapse workspace can have a default ADLS Gen2 storage account and additional linked storage accounts. \n",
							"\n",
							"You can access data on ADLS Gen2 with Synapse Spark via following URL:\n",
							"    \n",
							"    abfss://<container_name>@<storage_account_name>.dfs.core.windows.net/<path>\n",
							"\n",
							"This notebook provides examples of how to read data from ADLS Gen2 account into a Spark context and how to write the output of Spark jobs directly into an ADLS Gen2 location.\n",
							"\n",
							"## Pre-requisites\n",
							"Synapse leverage AAD pass-through to access any ADLS Gen2 account (or folder) to which you have a **Blob Storage Contributor** permission. No credentials or access token is required. "
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Load a sample data\n",
							"\n",
							"Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import PublicHolidays\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"from dateutil.relativedelta import relativedelta\n",
							"\n",
							"\n",
							"end_date = datetime.today()\n",
							"start_date = datetime.today() - relativedelta(months=6)\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
							"hol_df = hol.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"# Display 5 rows\n",
							"hol_df.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write data to the default ADLS Gen2 storage\n",
							"\n",
							"We are going to write the spark dateframe to your default ADLS Gen2 storage account.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"\n",
							"# Primary storage info\n",
							"account_name = 'Your primary storage account name' # fill in your primary account name\n",
							"container_name = 'Your container name' # fill in your container name\n",
							"relative_path = 'Your relative path' # fill in your relative folder path\n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\n",
							"print('Primary storage account path: ' + adls_path)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Save a dataframe as Parquet, JSON or CSV\n",
							"If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
							"\n",
							"Dataframes can be saved in any format, regardless of the input format.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = adls_path + 'holiday.parquet'\n",
							"json_path = adls_path + 'holiday.json'\n",
							"csv_path = adls_path + 'holiday.csv'\n",
							"print('parquet file path: ' + parquet_path)\n",
							"print('json file pathï¼š ' + json_path)\n",
							"print('csv file path: ' + csv_path)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"hol_df.write.parquet(parquet_path, mode = 'overwrite')\n",
							"hol_df.write.json(json_path, mode = 'overwrite')\n",
							"hol_df.write.csv(csv_path, mode = 'overwrite', header = 'true')"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Save a dataframe as text files\n",
							"If you have a dataframe that you want ot save as text file, you must first covert it to an RDD and then save that RDD as a text file.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Define the text file path\n",
							"text_path = adls_path + 'holiday.txt'\n",
							"print('text file path: ' + text_path)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"# Covert spark dataframe into RDD \n",
							"hol_RDD = hol_df.rdd\n",
							"type(hol_RDD)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"If you have an RDD, you can convert it to a text file like the following:\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							" # Save RDD as text file\n",
							"hol_RDD.saveAsTextFile(text_path)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Read data from the default ADLS Gen2 storage\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from parquet files\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_parquet = spark.read.parquet(parquet_path)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from JSON files\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_json = spark.read.json(json_path)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from CSV files\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_csv = spark.read.csv(csv_path, header = 'true')"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create an RDD from text file\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"text = sc.textFile(text_path)"
						],
						"outputs": [],
						"execution_count": 17
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02 Read and write data from Azure Blob Storage WASB')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PySpark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "be294df3-a0c5-4fb0-bbe6-99cb71dc94a7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Access data on Azure Storage Blob (WASB) with Synapse Spark\n",
							"\n",
							"You can access data on Azure Storage Blob (WASB) with Synapse Spark via following URL:\n",
							"\n",
							"    wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/<path>\n",
							"\n",
							"This notebook provides examples of how to read data from WASB into a Spark context and how to write the output of Spark jobs directly into a WASB location."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Load a sample data\n",
							"\n",
							"Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import PublicHolidays\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"from dateutil.relativedelta import relativedelta\n",
							"\n",
							"\n",
							"end_date = datetime.today()\n",
							"start_date = datetime.today() - relativedelta(months=6)\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
							"hol_df = hol.to_spark_dataframe()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"# Display 5 rows\n",
							"hol_df.show(5, truncate = False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write data to Azure Storage Blob\n",
							"\n",
							"Synapse leverage **Shared access signature (SAS)** to access Azure Blob Storage. To avoid exposing SAS keys in the code, we recommend creating a new linked service in Synapse workspace to the Azure Blob Storage account you want to access.\n",
							"\n",
							"Follow these steps to add a new linked service for an Azure Blob Storage account:\n",
							"\n",
							"1. Open the [Azure Synapse Studio](https://web.azuresynapse.net/).\n",
							"2. Select **Manage** from the left panel and select **Linked services** under the **External connections**.\n",
							"3. Search **Azure Blob Storage** in the **New linked Service** panel on the right.\n",
							"4. Select **Continue**.\n",
							"5. Select the Azure Blob Storage Account to access and configure the linked service name. Suggest using **Account key** for the **Authentication method**.\n",
							"6. Select **Test connection** to validate the settings are correct.\n",
							"7. Select **Create** first and click **Publish all** to save your changes.\n",
							"\n",
							"You can access data on Azure Blob Storage with Synapse Spark via following URL:\n",
							"\n",
							"```wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/```\n",
							"\n",
							"Please make sure to allow contatiner level read and write permission. Fill in the access info for your Azure storage blob in the cell below. \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"\n",
							"# Azure storage access info\n",
							"blob_account_name = 'Your blob name' # replace with your blob name\n",
							"blob_container_name = 'Your container name' # replace with your container name\n",
							"blob_relative_path = 'Your relative path' # replace with your relative folder path\n",
							"linked_service_name = 'Your linked service name' # replace with your linked service name\n",
							"\n",
							"blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Allow SPARK to access from Blob remotely\n",
							"wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
							"spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\n",
							"print('Remote blob path: ' + wasbs_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Save a dataframe as Parquet, JSON or CSV\n",
							"If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
							"\n",
							"Dataframes can be saved in any format, regardless of the input format.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = wasbs_path + 'holiday.parquet'\n",
							"json_path = wasbs_path + 'holiday.json'\n",
							"csv_path = wasbs_path + 'holiday.csv'\n",
							"print('parquet file path: ' + parquet_path)\n",
							"print('json file pathï¼š ' + json_path)\n",
							"print('csv file path: ' + csv_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"hol_df.write.parquet(parquet_path, mode = 'overwrite')\n",
							"hol_df.write.json(json_path, mode = 'overwrite')\n",
							"hol_df.write.csv(csv_path, mode = 'overwrite', header = 'true')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Save a dataframe as text files\n",
							"If you have a dataframe that you want ot save as text file, you must first covert it to an RDD and then save that RDD as a text file.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Define the text file path\n",
							"text_path = wasbs_path + 'holiday.txt'\n",
							"print('text file path: ' + text_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"# Covert spark dataframe into RDD \n",
							"hol_RDD = hol_df.rdd\n",
							"type(hol_RDD)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"If you have an RDD, you can convert it to a text file like the following:\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							" # Save RDD as text file\n",
							"hol_RDD.saveAsTextFile(text_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Read data from Azure Storage Blob\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from parquet files\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df_parquet = spark.read.parquet(parquet_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from JSON files\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df_json = spark.read.json(json_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create a dataframe from CSV files\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df_csv = spark.read.csv(csv_path, header = 'true')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create an RDD from text file\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"text = sc.textFile(text_path)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/03 Read and write from SQL pool table')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PySpark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "03f08ba7-5f9c-4a58-8cb1-846a37dd61cc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Access Synapse SQL table from Synapse Spark\n",
							"\n",
							"This notebook provides examples of how to read data from Synapse SQL into a Spark context and how to write the output of Spark jobs into an Synapse SQL table.\n",
							"\n",
							"\n",
							"## Limits\n",
							"- Scala is the only supported language by the Spark-SQL connector.\n",
							"- The Spark connector can only read colummns without space in its header in the sql pool.\n",
							"- Columns with time definition in the sql pool not yet supported.\n",
							"- You need to define a container on the workspace's primary or linked storage as the temp data folder.\n",
							"\n",
							"## Pre-requisites\n",
							"You need to be db_owner to read and write in sql pool. Ask your admin to run the following command with your AAD credential:\n",
							"\n",
							"    \n",
							"    EXEC sp_addrolemember 'db_owner', 'AAD@contoso.com'"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Load a sample data\n",
							"\n",
							"Let's first load the [Public Holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"%%pyspark \n",
							"# Load sample data from azure open dataset in pyspark\n",
							"from azureml.opendatasets import PublicHolidays\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"from dateutil.relativedelta import relativedelta\n",
							"\n",
							"\n",
							"end_date = datetime.today()\n",
							"start_date = datetime.today() - relativedelta(months=6)\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
							"hol_df = hol.to_spark_dataframe()\n",
							"\n",
							"print('Register the DataFrame as a SQL temporary view: source')\n",
							"hol_df.createOrReplaceTempView('source')"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"// Remove datetime from the data source\n",
							"val holiday_nodate = spark.sql(\"SELECT countryOrRegion, holidayName, normalizeHolidayName,isPaidTimeOff,countryRegionCode FROM source\")\n",
							"holiday_nodate.show(5,truncate = false)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a Spark dataframe into your sql pool\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"// Write the dataframe into your sql pool\n",
							"import org.apache.spark.sql.SqlAnalyticsConnector._\n",
							"import com.microsoft.spark.sqlanalytics.utils.Constants\n",
							"\n",
							"val sql_pool_name = \"Your sql pool name\" //fill in your sql pool name\n",
							"\n",
							"holiday_nodate.write\n",
							"    .sqlanalytics(s\"$sql_pool_name.dbo.PublicHoliday\", Constants.INTERNAL)\n",
							""
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"Now open Synapse object explorer and go to **Data**->**Databases**->**<your sql pool name>**->**Tables**, you will see the new **dbo.PublicHoliday** table show up there."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read from a SQL Pool table with Spark\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"// Read  the table we just created in the sql pool as a Spark dataframe\n",
							"val spark_read = spark.read.\n",
							"    sqlanalytics(s\"$sql_pool_name.dbo.PublicHoliday\")\n",
							"spark_read.show(5, truncate = false)"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/04 Using Delta Lake in Azure Synapse')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PySpark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "66c08f9b-2a56-4af9-9444-71a5fab5d97a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Using Linux Foundation Delta Lake in Azure Synapse Analytics Spark\n",
							"Azure Synapse is compatible with Linux Foundation Delta Lake. Delta Lake is an open-source storage layer that brings ACID (atomicity, consistency, isolation, and durability) transactions to Apache Spark and big data workloads.\n",
							"\n",
							"This notebook provides examples of how to update, merge and delete delta lake tables in Synapse.\n",
							"\n",
							"## Pre-requisites\n",
							"In this notebook you will save your tables in Delta Lake format to your workspace's primary storage account. You are required to be a **Blob Storage Contributor** in the ADLS Gen2 account (or folder) you will access.\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Load sample data\n",
							"\n",
							"First you will load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) data from last 6 months via Azure Open datasets.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import PublicHolidays\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"from dateutil.relativedelta import relativedelta\n",
							"\n",
							"\n",
							"end_date = datetime.today()\n",
							"start_date = datetime.today() - relativedelta(months=6)\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
							"hol_df = hol.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"display(hol_df)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write data to the Delta Lake table\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Set the strorage path info\n",
							"# Primary storage info\n",
							"account_name = '' # fill in your primary storage account name\n",
							"container_name = '' # fill in your container name\n",
							"relative_path = '' # fill in your relative folder path\n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\n",
							"print('Primary storage account path: ' + adls_path)\n",
							"\n",
							"# Delta Lake relative path\n",
							"delta_relative_path = adls_path + 'delta/holiday/'\n",
							"print('Delta Lake path: ' + delta_relative_path)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Filter out indian holidays\n",
							"hol_df_IN = hol_df[(hol_df.countryRegionCode == \"IN\")]\n",
							"hol_df_IN.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"#Let's write the data in the Delta Lake table. \n",
							"hol_df_IN.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"holidayName\").save(delta_relative_path)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"delta_data = spark.read.format(\"delta\").load(delta_relative_path)\n",
							"delta_data.show()"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Overwrite the entire Delta Lake table\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"#Let's overwrite the entire delta file with 1 record\n",
							"\n",
							"hol_df_JP= hol_df[(hol_df.countryRegionCode == \"JP\")]\n",
							"hol_df_JP.write.format(\"delta\").mode(\"overwrite\").save(delta_relative_path)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"source": [
							"delta_data = spark.read.format(\"delta\").load(delta_relative_path)\n",
							"delta_data.show()"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Merge new data based on given merge condition "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Upsert (merge) the United States' holiday data with Japan's\n",
							" \n",
							"from delta.tables import *\n",
							"\n",
							"deltaTable = DeltaTable.forPath(spark,delta_relative_path)\n",
							"\n",
							"hol_df_US= hol_df[(hol_df.countryRegionCode == \"US\")]\n",
							"\n",
							"\n",
							"deltaTable.alias(\"hol_df_JP\").merge(\n",
							"     source = hol_df_US.alias(\"hol_df_US\"),\n",
							"     condition = \"hol_df_JP.countryRegionCode = hol_df_US.countryRegionCode\"\n",
							"    ).whenMatchedUpdate(set = \n",
							"    {}).whenNotMatchedInsert( values = \n",
							"    {\n",
							"        \"countryOrRegion\" : \"hol_df_US.countryOrRegion\",\n",
							"        \"holidayName\" : \"hol_df_US.holidayName\",\n",
							"        \"normalizeHolidayName\" : \"hol_df_US.normalizeHolidayName\",\n",
							"        \"isPaidTimeOff\":\"hol_df_US.isPaidTimeOff\",\n",
							"        \"countryRegionCode\":\"hol_df_US.countryRegionCode\",\n",
							"        \"date\":\"hol_df_US.date\"\n",
							"    }\n",
							"    ).execute()\n",
							"\n",
							"\n",
							"deltaTable.toDF().show()"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table on the rows that match the given condition\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Update column the 'null' value in 'isPaidTimeOff' with 'false'\n",
							"\n",
							"from pyspark.sql.functions import *\n",
							"deltaTable.update(\n",
							"    condition = (col(\"isPaidTimeOff\").isNull()),\n",
							"    set = {\"isPaidTimeOff\": \"false\"})\n",
							"\n",
							"deltaTable.toDF().show()"
						],
						"outputs": [],
						"execution_count": 71
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Delete data from the table that match the given condition\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"Row count before delete: \")\n",
							"print(deltaTable.toDF().count())\n",
							"\n",
							"\n",
							"# Delte data with date later than 2020-01-01\n",
							"deltaTable.delete (\"date > '2020-01-01'\")\n",
							"\n",
							"\n",
							"print(\"Row count after delete:  \")\n",
							"print(deltaTable.toDF().count())\n",
							"deltaTable.toDF().show()"
						],
						"outputs": [],
						"execution_count": 72
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Get the operation history of the delta table\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"fullHistoryDF = deltaTable.history()\n",
							"lastOperationDF = deltaTable.history(1)\n",
							"\n",
							"print('Full history DF: ')\n",
							"fullHistoryDF.show(truncate = False)\n",
							"\n",
							"print('lastOperationDF: ')\n",
							"lastOperationDF.show(truncate = False)"
						],
						"outputs": [],
						"execution_count": 73
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/05 Using Azure Open Datasets in Synapse')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PySpark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "176568f0-1d50-4fc8-b9de-dbfb7ddd4d89"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Using Azure Open Datasets in Synapse - Enrich NYC Green Taxi Data with Holiday and Weather\n",
							"\n",
							"Synapse has [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/) package pre-installed. This notebook provides examples of how to enrich NYC Green Taxi Data with Holiday and Weather with focusing on :\n",
							"- read Azure Open Dataset\n",
							"- manipulate the data to prepare for further analysis, including column projection, filtering, grouping and joins etc. \n",
							"- create a Spark table to be used in other notebooks for modeling training"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data loading \n",
							"Let's first load the [NYC green taxi trip records](https://azure.microsoft.com/en-us/services/open-datasets/catalog/nyc-taxi-limousine-commission-green-taxi-trip-records/). The Open Datasets package contains a class representing each data source (NycTlcGreen for example) to easily filter date parameters before downloading."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import NycTlcGreen\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"end_date = parser.parse('2018-06-06')\n",
							"start_date = parser.parse('2018-05-01')\n",
							"\n",
							"nyc_tlc = NycTlcGreen(start_date=start_date, end_date=end_date)\n",
							"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"# Display 5 rows\n",
							"\n",
							"nyc_tlc_df.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"Now that the initial data is loaded. Let's do some projection on the data to \n",
							"- create new columns for the month number, day of month, day of week, and hour of day. These info is going to be used in the training model to factor in time-based seasonality.\n",
							"- add a static feature for the country code to join holiday data. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Extract month, day of month, and day of week from pickup datetime and add a static column for the country code to join holiday data. \n",
							"\n",
							"import pyspark.sql.functions as f\n",
							"\n",
							"nyc_tlc_df_expand = nyc_tlc_df.withColumn('datetime',f.to_date('lpepPickupDatetime'))\\\n",
							"                .withColumn('month_num',f.month(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('day_of_month',f.dayofmonth(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('day_of_week',f.dayofweek(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('hour_of_day',f.hour(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('country_code',f.lit('US'))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"Remove some of the columns that won't need for modeling or additional feature building.\n",
							"\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Remove unused columns from nyc green taxi data\n",
							"\n",
							"columns_to_remove = [\"lpepDropoffDatetime\", \"puLocationId\", \"doLocationId\", \"pickupLongitude\", \n",
							"                     \"pickupLatitude\", \"dropoffLongitude\",\"dropoffLatitude\" ,\"rateCodeID\", \n",
							"                     \"storeAndFwdFlag\",\"paymentType\", \"fareAmount\", \"extra\", \"mtaTax\",\n",
							"                     \"improvementSurcharge\", \"tollsAmount\", \"ehailFee\", \"tripType \"  \n",
							"                    ]\n",
							"\n",
							"nyc_tlc_df_clean = nyc_tlc_df_expand.select([column for column in nyc_tlc_df_expand.columns if column not in columns_to_remove])"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"# Display 5 rows\n",
							"nyc_tlc_df_clean.show(5)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Enrich with holiday data\n",
							"Now that we have taxi data downloaded and roughly prepared, add in holiday data as additional features. Holiday-specific features will assist model accuracy, as major holidays are times where taxi demand increases dramatically and supply becomes limited. \n",
							"\n",
							"Let's load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) from Azure Open datasets.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import PublicHolidays\n",
							"\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
							"hol_df = hol.to_spark_dataframe()\n",
							"\n",
							"# Display data\n",
							"hol_df.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"Rename the countryRegionCode and date columns to match the respective field names from the taxi data, and also normalize the time so it can be used as a key. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"hol_df_clean = hol_df.withColumnRenamed('countryRegionCode','country_code')\\\n",
							"            .withColumn('datetime',f.to_date('date'))\n",
							"\n",
							"hol_df_clean.show(5)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"Next, join the holiday data with the taxi data by performing a left-join. This will preserve all records from taxi data, but add in holiday data where it exists for the corresponding datetime and country_code, which in this case is always \"US\". Preview the data to verify that they were merged correctly."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# enrich taxi data with holiday data\n",
							"nyc_taxi_holiday_df = nyc_tlc_df_clean.join(hol_df_clean, on = ['datetime', 'country_code'] , how = 'left')\n",
							"\n",
							"nyc_taxi_holiday_df.show(5)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# Create a temp table and filter out non empty holiday rows\n",
							"\n",
							"nyc_taxi_holiday_df.createOrReplaceTempView(\"nyc_taxi_holiday_df\")\n",
							"spark.sql(\"SELECT * from nyc_taxi_holiday_df WHERE holidayName is NOT NULL \").show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Enrich with weather data\n",
							"\n",
							"Now we append NOAA surface weather data to the taxi and holiday data. Use a similar approach to fetch the [NOAA weather history data](https://azure.microsoft.com/en-us/services/open-datasets/catalog/noaa-integrated-surface-data/) from Azure Open Datasets. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import NoaaIsdWeather\n",
							"\n",
							"isd = NoaaIsdWeather(start_date, end_date)\n",
							"isd_df = isd.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"isd_df.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"# Filter out weather info for new york city, remove the recording with null temperature \n",
							"\n",
							"weather_df = isd_df.filter(isd_df.latitude >= '40.53')\\\n",
							"                        .filter(isd_df.latitude <= '40.88')\\\n",
							"                        .filter(isd_df.longitude >= '-74.09')\\\n",
							"                        .filter(isd_df.longitude <= '-73.72')\\\n",
							"                        .filter(isd_df.temperature.isNotNull())\\\n",
							"                        .withColumnRenamed('datetime','datetime_full')\n",
							"                         "
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"# Remove unused columns\n",
							"\n",
							"columns_to_remove_weather = [\"usaf\", \"wban\", \"longitude\", \"latitude\"]\n",
							"weather_df_clean = weather_df.select([column for column in weather_df.columns if column not in columns_to_remove_weather])\\\n",
							"                        .withColumn('datetime',f.to_date('datetime_full'))\n",
							"\n",
							"weather_df_clean.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"Next group the weather data so that you have daily aggregated weather values. \n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Enrich weather data with aggregation statistics\n",
							"\n",
							"aggregations = {\"snowDepth\": \"mean\", \"precipTime\": \"max\", \"temperature\": \"mean\", \"precipDepth\": \"max\"}\n",
							"weather_df_grouped = weather_df_clean.groupby(\"datetime\").agg(aggregations)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"source": [
							"weather_df_grouped.show(5)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"# Rename columns\n",
							"\n",
							"weather_df_grouped = weather_df_grouped.withColumnRenamed('avg(snowDepth)','avg_snowDepth')\\\n",
							"                                       .withColumnRenamed('avg(temperature)','avg_temperature')\\\n",
							"                                       .withColumnRenamed('max(precipTime)','max_precipTime')\\\n",
							"                                       .withColumnRenamed('max(precipDepth)','max_precipDepth')"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"source": [
							"Merge the taxi and holiday data you prepared with the new weather data. This time you only need the datetime key, and again perform a left-join of the data. Run the describe() function on the new dataframe to see summary statistics for each field."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# enrich taxi data with weather\n",
							"nyc_taxi_holiday_weather_df = nyc_taxi_holiday_df.join(weather_df_grouped, on = 'datetime' , how = 'left')\n",
							"nyc_taxi_holiday_weather_df.cache()"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"nyc_taxi_holiday_weather_df.show(5)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"source": [
							"# Run the describe() function on the new dataframe to see summary statistics for each field.\n",
							"\n",
							"display(nyc_taxi_holiday_weather_df.describe())"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"source": [
							"The summary statistics shows that the totalAmount field has negative values, which don't make sense in the context.\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Remove invalid rows with less than 0 taxi fare or tip\n",
							"final_df = nyc_taxi_holiday_weather_df.filter(nyc_taxi_holiday_weather_df.tipAmount > 0)\\\n",
							"                                      .filter(nyc_taxi_holiday_weather_df.totalAmount > 0)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Cleaning up the existing Database\n",
							"\n",
							"First we need to drop the tables since Spark requires that a database is empty before we can drop the Database.\n",
							"\n",
							"Then we recreate the database and set the default database context to it."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP TABLE IF EXISTS NYCTaxi.nyc_taxi_holiday_weather\"); "
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP DATABASE IF EXISTS NYCTaxi\"); \n",
							"spark.sql(\"CREATE DATABASE NYCTaxi\"); \n",
							"spark.sql(\"USE NYCTaxi\");"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Creating a new table\n",
							"We create a nyc_taxi_holiday_weather table from the nyc_taxi_holiday_weather dataframe.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"\n",
							"final_df.write.saveAsTable(\"nyc_taxi_holiday_weather\");\n",
							"spark.sql(\"SELECT COUNT(*) FROM nyc_taxi_holiday_weather\").show();"
						],
						"outputs": [],
						"execution_count": 25
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/06 Charting in Synapse Notebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PySpark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0f375387-c693-47c3-9e9b-ae197398635f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Charting in Synapse Notebook\n",
							"\n",
							"Synapse has common used data visualization packages pre installed, such as **matplotlib**, **bokeh**, **seaborn**, **altair**, **plotly**. This notebook provides examples to do data visualization using charts in Synapse notebook. \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Matplotlib\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Line charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							" \n",
							"x  = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
							"y1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\n",
							"y2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\n",
							"plt.plot(x, y1, label=\"line L\")\n",
							"plt.plot(x, y2, label=\"line H\")\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"x axis\")\n",
							"plt.ylabel(\"y axis\")\n",
							"plt.title(\"Line Graph Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# Bar chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"# Look at index 4 and 6, which demonstrate overlapping cases.\n",
							"x1 = [1, 3, 4, 5, 6, 7, 9]\n",
							"y1 = [4, 7, 2, 4, 7, 8, 3]\n",
							"\n",
							"x2 = [2, 4, 6, 8, 10]\n",
							"y2 = [5, 6, 2, 6, 2]\n",
							"\n",
							"# Colors: https://matplotlib.org/api/colors_api.html\n",
							"\n",
							"plt.bar(x1, y1, label=\"Blue Bar\", color='b')\n",
							"plt.bar(x2, y2, label=\"Green Bar\", color='g')\n",
							"plt.plot()\n",
							"\n",
							"plt.xlabel(\"bar number\")\n",
							"plt.ylabel(\"bar height\")\n",
							"plt.title(\"Bar Chart Example\")\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"# Histogram\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Use numpy to generate a bunch of random data in a bell curve around 5.\n",
							"n = 5 + np.random.randn(1000)\n",
							"\n",
							"m = [m for m in range(len(n))]\n",
							"plt.bar(m, n)\n",
							"plt.title(\"Raw Data\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, bins=20)\n",
							"plt.title(\"Histogram\")\n",
							"plt.show()\n",
							"\n",
							"plt.hist(n, cumulative=True, bins=20)\n",
							"plt.title(\"Cumulative Histogram\")\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"# Scatter chart\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"x1 = [2, 3, 4]\n",
							"y1 = [5, 5, 5]\n",
							"\n",
							"x2 = [1, 2, 3, 4, 5]\n",
							"y2 = [2, 3, 2, 3, 4]\n",
							"y3 = [6, 8, 7, 8, 7]\n",
							"\n",
							"# Markers: https://matplotlib.org/api/markers_api.html\n",
							"\n",
							"plt.scatter(x1, y1)\n",
							"plt.scatter(x2, y2, marker='v', color='r')\n",
							"plt.scatter(x2, y3, marker='^', color='m')\n",
							"plt.title('Scatter Plot Example')\n",
							"plt.show()\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"# Stack plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"idxes = [ 1,  2,  3,  4,  5,  6,  7,  8,  9]\n",
							"arr1  = [23, 40, 28, 43,  8, 44, 43, 18, 17]\n",
							"arr2  = [17, 30, 22, 14, 17, 17, 29, 22, 30]\n",
							"arr3  = [15, 31, 18, 22, 18, 19, 13, 32, 39]\n",
							"\n",
							"# Adding legend for stack plots is tricky.\n",
							"plt.plot([], [], color='r', label = 'D 1')\n",
							"plt.plot([], [], color='g', label = 'D 2')\n",
							"plt.plot([], [], color='b', label = 'D 3')\n",
							"\n",
							"plt.stackplot(idxes, arr1, arr2, arr3, colors= ['r', 'g', 'b'])\n",
							"plt.title('Stack Plot Example')\n",
							"plt.legend()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"# Pie charts\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"labels = 'S1', 'S2', 'S3'\n",
							"sections = [56, 66, 24]\n",
							"colors = ['c', 'g', 'y']\n",
							"\n",
							"plt.pie(sections, labels=labels, colors=colors,\n",
							"        startangle=90,\n",
							"        explode = (0, 0.1, 0),\n",
							"        autopct = '%1.2f%%')\n",
							"\n",
							"plt.axis('equal') # Try commenting this out.\n",
							"plt.title('Pie Chart Example')\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"# fill_between and alpha\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"ys = 200 + np.random.randn(100)\n",
							"x = [x for x in range(len(ys))]\n",
							"\n",
							"plt.plot(x, ys, '-')\n",
							"plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)\n",
							"\n",
							"plt.title(\"Fills and Alpha Example\")\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"# Subplotting using Subplot2grid\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"def random_plots():\n",
							"  xs = []\n",
							"  ys = []\n",
							"  \n",
							"  for i in range(20):\n",
							"    x = i\n",
							"    y = np.random.randint(10)\n",
							"    \n",
							"    xs.append(x)\n",
							"    ys.append(y)\n",
							"  \n",
							"  return xs, ys\n",
							"\n",
							"fig = plt.figure()\n",
							"ax1 = plt.subplot2grid((5, 2), (0, 0), rowspan=1, colspan=2)\n",
							"ax2 = plt.subplot2grid((5, 2), (1, 0), rowspan=3, colspan=2)\n",
							"ax3 = plt.subplot2grid((5, 2), (4, 0), rowspan=1, colspan=1)\n",
							"ax4 = plt.subplot2grid((5, 2), (4, 1), rowspan=1, colspan=1)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax1.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax2.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax3.plot(x, y)\n",
							"\n",
							"x, y = random_plots()\n",
							"ax4.plot(x, y)\n",
							"\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"# 3D Scatter Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"from mpl_toolkits.mplot3d import axes3d\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y1 = np.random.randint(10, size=10)\n",
							"z1 = np.random.randint(10, size=10)\n",
							"\n",
							"x2 = [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\n",
							"y2 = np.random.randint(-10, 0, size=10)\n",
							"z2 = np.random.randint(10, size=10)\n",
							"\n",
							"ax.scatter(x1, y1, z1, c='b', marker='o', label='blue')\n",
							"ax.scatter(x2, y2, z2, c='g', marker='D', label='green')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Scatter Plot Example\")\n",
							"plt.legend()\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"# 3D Bar Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"y = np.random.randint(10, size=10)\n",
							"z = np.zeros(10)\n",
							"\n",
							"dx = np.ones(10)\n",
							"dy = np.ones(10)\n",
							"dz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
							"\n",
							"ax.bar3d(x, y, z, dx, dy, dz, color='g')\n",
							"\n",
							"ax.set_xlabel('x axis')\n",
							"ax.set_ylabel('y axis')\n",
							"ax.set_zlabel('z axis')\n",
							"plt.title(\"3D Bar Chart Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# Wireframe Plots\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111, projection = '3d')\n",
							"\n",
							"x, y, z = axes3d.get_test_data()\n",
							"\n",
							"ax.plot_wireframe(x, y, z, rstride = 2, cstride = 2)\n",
							"\n",
							"plt.title(\"Wireframe Plot Example\")\n",
							"plt.tight_layout()\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Seaborn\n",
							"Seaborn is a library layered on top of Matplotlib that you can use."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Scatterplot with a nice regression line fit to it, all with just one call to Seaborn's regplot.\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"import seaborn as sns\n",
							"\n",
							"# Generate some random data\n",
							"num_points = 20\n",
							"# x will be 5, 6, 7... but also twiddled randomly\n",
							"x = 5 + np.arange(num_points) + np.random.randn(num_points)\n",
							"# y will be 10, 11, 12... but twiddled even more randomly\n",
							"y = 10 + np.arange(num_points) + 5 * np.random.randn(num_points)\n",
							"sns.regplot(x=x, y=y)\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"# Seanborn heatmap\n",
							"\n",
							"import matplotlib.pyplot as plt\n",
							"import numpy as np\n",
							"\n",
							"# Make a 10 x 10 heatmap of some random data\n",
							"side_length = 10\n",
							"# Start with a 10 x 10 matrix with values randomized around 5\n",
							"data = 5 + np.random.randn(side_length, side_length)\n",
							"# The next two lines make the values larger as we get closer to (9, 9)\n",
							"data += np.arange(side_length)\n",
							"data += np.reshape(np.arange(side_length), (side_length, 1))\n",
							"# Generate the heatmap\n",
							"fig = plt.figure()\n",
							"ax = fig.add_subplot(111)\n",
							"sns.heatmap(data, ax=ax)\n",
							"plt.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Bokeh\n",
							"You can render HTML or interactive libraries, like **bokeh**, using the **displayHTML()**.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import numpy as np\n",
							"from bokeh.plotting import figure, show\n",
							"from bokeh.io import output_notebook\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"\n",
							"N = 4000\n",
							"x = np.random.random(size=N) * 100\n",
							"y = np.random.random(size=N) * 100\n",
							"radii = np.random.random(size=N) * 1.5\n",
							"colors = [\"#%02x%02x%02x\" % (r, g, 150) for r, g in zip(np.floor(50+2*x).astype(int), np.floor(30+2*y).astype(int))]\n",
							"\n",
							"p = figure()\n",
							"p.circle(x, y, radius=radii, fill_color=colors, fill_alpha=0.6, line_color=None)\n",
							"show(p)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"# Plotting glyphs over a map using bokeh.\n",
							"\n",
							"from bokeh.plotting import figure, output_file\n",
							"from bokeh.tile_providers import get_provider, Vendors\n",
							"from bokeh.embed import file_html\n",
							"from bokeh.resources import CDN\n",
							"from bokeh.models import ColumnDataSource\n",
							"\n",
							"tile_provider = get_provider(Vendors.CARTODBPOSITRON)\n",
							"\n",
							"# range bounds supplied in web mercator coordinates\n",
							"p = figure(x_range=(-9000000,-8000000), y_range=(4000000,5000000),\n",
							"           x_axis_type=\"mercator\", y_axis_type=\"mercator\")\n",
							"p.add_tile(tile_provider)\n",
							"\n",
							"# plot datapoints on the map\n",
							"source = ColumnDataSource(\n",
							"    data=dict(x=[ -8800000, -8500000 , -8800000],\n",
							"              y=[4200000, 4500000, 4900000])\n",
							")\n",
							"\n",
							"p.circle(x=\"x\", y=\"y\", size=15, fill_color=\"blue\", fill_alpha=0.8, source=source)\n",
							"\n",
							"# create an html document that embeds the Bokeh plot\n",
							"html = file_html(p, CDN, \"my plot1\")\n",
							"\n",
							"# display this html\n",
							"displayHTML(html)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Plotly\r\n",
							"You can render HTML or interactive libraries like **Plotly**, using the **displayHTML()**."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"from urllib.request import urlopen\r\n",
							"import json\r\n",
							"with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\r\n",
							"    counties = json.load(response)\r\n",
							"\r\n",
							"import pandas as pd\r\n",
							"df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/fips-unemp-16.csv\",\r\n",
							"                   dtype={\"fips\": str})\r\n",
							"\r\n",
							"import plotly\r\n",
							"import plotly.express as px\r\n",
							"\r\n",
							"fig = px.choropleth(df, geojson=counties, locations='fips', color='unemp',\r\n",
							"                           color_continuous_scale=\"Viridis\",\r\n",
							"                           range_color=(0, 12),\r\n",
							"                           scope=\"usa\",\r\n",
							"                           labels={'unemp':'unemployment rate'}\r\n",
							"                          )\r\n",
							"fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\r\n",
							"\r\n",
							"# create an html document that embeds the Plotly plot\r\n",
							"h = plotly.offline.plot(fig, output_type='div')\r\n",
							"\r\n",
							"# display this html\r\n",
							"displayHTML(h)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/07 Data Exploration and ML Modeling - NYC taxi predict using Spark MLlib')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PySpark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "891c3ace-47ca-4f6a-a4c5-95df006e5025"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Predict NYC Taxi Tips using Spark ML and Azure Open Datasets\n",
							"\n",
							"The notebook ingests, visualizes, prepares and then trains a model based on an Open Dataset that tracks NYC Yellow Taxi trips and various attributes around them.\n",
							"The goal is to predict for a given trip whether there will be a trip or not.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import matplotlib.pyplot as plt\n",
							"\n",
							"from pyspark.sql.functions import unix_timestamp\n",
							"\n",
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"from pyspark.ml import Pipeline\n",
							"from pyspark.ml import PipelineModel\n",
							"from pyspark.ml.feature import RFormula\n",
							"from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
							"from pyspark.ml.classification import LogisticRegression\n",
							"from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
							"from pyspark.ml.evaluation import BinaryClassificationEvaluator"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Ingest Data\n",
							"\n",
							"Get a sample data of nyc yellow taxi to make it faster/easier to evaluate different approaches to prep for the modelling phase later in the notebook."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Import NYC yellow cab data from Azure Open Datasets\n",
							"from azureml.opendatasets import NycTlcYellow\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"\n",
							"end_date = parser.parse('2018-05-08 00:00:00')\n",
							"start_date = parser.parse('2018-05-01 00:00:00')\n",
							"\n",
							"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
							"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"#To make development easier, faster and less expensive downsample for now\n",
							"sampled_taxi_df = nyc_tlc_df.sample(True, 0.001, seed=1234)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Exploratory Data Analysis\n",
							"\n",
							"Look at the data and evaluate its suitability for use in a model, do this via some basic charts focussed on tip values and relationships."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"#The charting package needs a Pandas dataframe or numpy array do the conversion\n",
							"sampled_taxi_pd_df = sampled_taxi_df.toPandas()\n",
							"\n",
							"# Look at tips by amount count histogram\n",
							"ax1 = sampled_taxi_pd_df['tipAmount'].plot(kind='hist', bins=25, facecolor='lightblue')\n",
							"ax1.set_title('Tip amount distribution')\n",
							"ax1.set_xlabel('Tip Amount ($)')\n",
							"ax1.set_ylabel('Counts')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# How many passengers tip'd by various amounts\n",
							"ax2 = sampled_taxi_pd_df.boxplot(column=['tipAmount'], by=['passengerCount'])\n",
							"ax2.set_title('Tip amount by Passenger count')\n",
							"ax2.set_xlabel('Passenger count') \n",
							"ax2.set_ylabel('Tip Amount ($)')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# Look at the relationship between fare and tip amounts\n",
							"ax = sampled_taxi_pd_df.plot(kind='scatter', x= 'fareAmount', y = 'tipAmount', c='blue', alpha = 0.10, s=2.5*(sampled_taxi_pd_df['passengerCount']))\n",
							"ax.set_title('Tip amount by Fare amount')\n",
							"ax.set_xlabel('Fare Amount ($)')\n",
							"ax.set_ylabel('Tip Amount ($)')\n",
							"plt.axis([-2, 80, -2, 20])\n",
							"plt.suptitle('')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization\n",
							"\n",
							"It's clear from the visualizations above that there are a bunch of outliers in the data. These will need to be filtered out in addition there are extra variables that are not going to be useful in the model we build at the end.\n",
							"\n",
							"Finally there is a need to create some new (derived) variables that will work better with the model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_df = sampled_taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'rateCodeId', 'passengerCount'\\\n",
							"                                , 'tripDistance', 'tpepPickupDateTime', 'tpepDropoffDateTime'\\\n",
							"                                , date_format('tpepPickupDateTime', 'hh').alias('pickupHour')\\\n",
							"                                , date_format('tpepPickupDateTime', 'EEEE').alias('weekdayString')\\\n",
							"                                , (unix_timestamp(col('tpepDropoffDateTime')) - unix_timestamp(col('tpepPickupDateTime'))).alias('tripTimeSecs')\\\n",
							"                                , (when(col('tipAmount') > 0, 1).otherwise(0)).alias('tipped')\n",
							"                                )\\\n",
							"                        .filter((sampled_taxi_df.passengerCount > 0) & (sampled_taxi_df.passengerCount < 8)\\\n",
							"                                & (sampled_taxi_df.tipAmount >= 0) & (sampled_taxi_df.tipAmount <= 25)\\\n",
							"                                & (sampled_taxi_df.fareAmount >= 1) & (sampled_taxi_df.fareAmount <= 250)\\\n",
							"                                & (sampled_taxi_df.tipAmount < sampled_taxi_df.fareAmount)\\\n",
							"                                & (sampled_taxi_df.tripDistance > 0) & (sampled_taxi_df.tripDistance <= 100)\\\n",
							"                                & (sampled_taxi_df.rateCodeId <= 5)\n",
							"                                & (sampled_taxi_df.paymentType.isin({\"1\", \"2\"}))\n",
							"                                )"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization Part 2\n",
							"\n",
							"Having created new variables its now possible to drop the columns they were derived from so that the dataframe that goes into the model is the smallest in terms of number of variables, that is required.\n",
							"\n",
							"Also create some more features based on new columns from the first round.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_featurised_df = taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'passengerCount'\\\n",
							"                                                , 'tripDistance', 'weekdayString', 'pickupHour','tripTimeSecs','tipped'\\\n",
							"                                                , when((taxi_df.pickupHour <= 6) | (taxi_df.pickupHour >= 20),\"Night\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 7) & (taxi_df.pickupHour <= 10), \"AMRush\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 11) & (taxi_df.pickupHour <= 15), \"Afternoon\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 16) & (taxi_df.pickupHour <= 19), \"PMRush\")\\\n",
							"                                                .otherwise(0).alias('trafficTimeBins')\n",
							"                                              )\\\n",
							"                                       .filter((taxi_df.tripTimeSecs >= 30) & (taxi_df.tripTimeSecs <= 7200))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Encoding\n",
							"\n",
							"Different ML algorithms support different types of input, for this example Logistic Regression is being used for Binary Classification. This means that any Categorical (string) variables must be converted to numbers.\n",
							"\n",
							"The process is not as simple as a \"map\" style function as the relationship between the numbers can introduce a bias in the resulting model, the approach is to index the variable and then encode using a std approach called One Hot Encoding.\n",
							"\n",
							"This approach requires the encoder to \"learn\"/fit a model over the data in the Spark instance and then transform based on what was learnt.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# The sample uses an algorithm that only works with numeric features convert them so they can be consumed\n",
							"sI1 = StringIndexer(inputCol=\"trafficTimeBins\", outputCol=\"trafficTimeBinsIndex\"); \n",
							"en1 = OneHotEncoder(dropLast=False, inputCol=\"trafficTimeBinsIndex\", outputCol=\"trafficTimeBinsVec\");\n",
							"sI2 = StringIndexer(inputCol=\"weekdayString\", outputCol=\"weekdayIndex\"); \n",
							"en2 = OneHotEncoder(dropLast=False, inputCol=\"weekdayIndex\", outputCol=\"weekdayVec\");\n",
							"\n",
							"# Create a new dataframe that has had the encodings applied\n",
							"encoded_final_df = Pipeline(stages=[sI1, en1, sI2, en2]).fit(taxi_featurised_df).transform(taxi_featurised_df)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Generation of Testing and Training Data Sets\n",
							"Simple split, 70% for training and 30% for testing the model. Playing with this ratio may result in different models.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Decide on the split between training and testing data from the dataframe \n",
							"trainingFraction = 0.7\n",
							"testingFraction = (1-trainingFraction)\n",
							"seed = 1234\n",
							"\n",
							"# Split the dataframe into test and training dataframes\n",
							"train_data_df, test_data_df = encoded_final_df.randomSplit([trainingFraction, testingFraction], seed=seed)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Train the Model\n",
							"\n",
							"Train the Logistic Regression model and then evaluate it using Area under ROC as the metric."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"## Create a new LR object for the model\n",
							"logReg = LogisticRegression(maxIter=10, regParam=0.3, labelCol = 'tipped')\n",
							"\n",
							"## The formula for the model\n",
							"classFormula = RFormula(formula=\"tipped ~ pickupHour + weekdayVec + passengerCount + tripTimeSecs + tripDistance + fareAmount + paymentType+ trafficTimeBinsVec\")\n",
							"\n",
							"## Undertake training and create an LR model\n",
							"lrModel = Pipeline(stages=[classFormula, logReg]).fit(train_data_df)\n",
							"\n",
							"## Saving the model is optional but its another for of inter session cache\n",
							"datestamp = datetime.now().strftime('%m-%d-%Y-%s');\n",
							"fileName = \"lrModel_\" + datestamp;\n",
							"logRegDirfilename = fileName;\n",
							"lrModel.save(logRegDirfilename)\n",
							"\n",
							"## Predict tip 1/0 (yes/no) on the test dataset, evaluation using AUROC\n",
							"predictions = lrModel.transform(test_data_df)\n",
							"predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\n",
							"metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
							"print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Evaluate and Visualize\n",
							"\n",
							"Plot the actual curve to develop a better understanding of the model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"## Plot the ROC curve, no need for pandas as this uses the modelSummary object\n",
							"modelSummary = lrModel.stages[-1].summary\n",
							"\n",
							"plt.plot([0, 1], [0, 1], 'r--')\n",
							"plt.plot(modelSummary.roc.select('FPR').collect(),\n",
							"         modelSummary.roc.select('TPR').collect())\n",
							"plt.xlabel('False Positive Rate')\n",
							"plt.ylabel('True Positive Rate')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/08 Creating an unmanaged Spark Table')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PySpark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "67038103-ad37-4a7f-9fbb-6b591ce62035"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Creating an unmanaged (external) Spark table\n",
							"This notebook describes how to create an unmanaged (also known as external) table from Spark. \n",
							"The table is created in /datalake/cities which may exist already (so you can attach to existing data) it can be created when you insert data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"CREATE TABLE cities (name STRING, population INT) USING PARQUET  LOCATION \\'/datalake/cities\\' OPTIONS (\\'compression\\'=\\'snappy\\')\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"Insert a few rows into the table using a list of values.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"INSERT INTO cities VALUES (\\'Seattle\\', 730400), (\\'San Francisco\\', 881549), (\\'Beijing\\', 21540000), (\\'Bangalore\\', 10540000)\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"* Retrieve values back. Click on 'Chart' below to review the visualization.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"name"
									],
									"values": [
										"population"
									],
									"yLabel": "population",
									"xLabel": "name",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"population\":{\"Bangalore\":21080000,\"Beijing\":43080000,\"San Francisco\":1763098,\"Seattle\":1460800}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							}
						},
						"source": [
							"display(spark.sql(\"SELECT * FROM cities ORDER BY name\"))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"Drop the table. Please note the data will remain in the data lake.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP TABLE cities\")"
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/09 Creating a managed Spark Table')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PySpark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d88bcad1-a670-4fd8-a51f-3245ccc8f665"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Creating a managed Spark table\n",
							"This notebook describes how to create a managed table from Spark. \n",
							"The table is created in the Synapse warehouse folder in your primary storage account. The table will be synchronized and available in Synapse SQL Pools. \n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"CREATE TABLE cities  (name STRING, population INT) USING PARQUET\")\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"Insert a few rows into the table using a list of values.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"INSERT INTO cities VALUES (\\'Seattle\\', 730400), (\\'San Francisco\\', 881549), (\\'Beijing\\', 21540000), (\\'Bangalore\\', 10540000)\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"* Retrieve values back. Click on 'Chart' below to review the visualization.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"name"
									],
									"values": [
										"population"
									],
									"yLabel": "population",
									"xLabel": "name",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"population\":{\"Bangalore\":10540000,\"Beijing\":21540000,\"San Francisco\":881549,\"Seattle\":730400}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							}
						},
						"source": [
							"display(spark.sql(\"SELECT * FROM cities ORDER BY name\"))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"Drop the table. Please note the data will get deleted from the primary storage account associated with this workspace.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP TABLE cities\")"
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/BadCSVCleanup')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "718d7481-929c-4910-88de-bf953e7fb76f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark31",
						"name": "lgnsynspark31",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/raw/csv/sale-20170502.txt', format='text')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import numpy as np\r\n",
							"from decimal import *\r\n",
							"from azure.storage.blob import *\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"tokens = df.content.split(',')\r\n",
							"\r\n",
							"print(f'Found {len(tokens)} tokens in content.')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.storage.blob import *"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Challenge 4 - Movies and Actors - Customers - Addresses')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "168eed2d-11ac-4d46-8a15-c6ac17009d0e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark32",
						"name": "lgnsynspark32",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\r\n",
							"\r\n",
							"  #\"abfss://synapsefs@lgnmdwohdatalake.dfs.core.windows.net/raw/vanarsdel/dboMovies.parquet\"\r\n",
							"\r\n",
							"mountPoint = \"abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/mdwoh\"\r\n",
							"print(mountPoint)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Load Southridge Customers"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"sr_sales_customers_parquet = mountPoint + \"/raw/southridge/cloudsales/dboCustomers.parquet\"\r\n",
							"sr_streaming_customers_parquet = mountPoint + \"/raw/southridge/cloudstreaming/dboCustomers.parquet\"\r\n",
							"\r\n",
							"sr_sales_customers = sqlContext.read.parquet(sr_sales_customers_parquet)\r\n",
							"sr_streaming_customers = sqlContext.read.parquet(sr_streaming_customers_parquet)\r\n",
							"\r\n",
							"sr_sales_customers = sr_sales_customers.toPandas()\r\n",
							"sr_streaming_customers = sr_streaming_customers.toPandas()\r\n",
							"\r\n",
							"sr_customers_frame = [sr_sales_customers, sr_streaming_customers]\r\n",
							"sr_customers = pd.concat(sr_customers_frame)\r\n",
							"\r\n",
							"sr_customers['CreatedDate'] = pd.to_datetime(sr_customers['CreatedDate'], errors='coerce')\r\n",
							"sr_customers['UpdatedDate'] = pd.to_datetime(sr_customers['UpdatedDate'], errors='coerce')\r\n",
							"sr_customers.PhoneNumber = sr_customers.PhoneNumber.astype(str)\r\n",
							"\r\n",
							"display(sr_customers)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Load VanArsdel Customers"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"va_customers_filePath = mountPoint + \"/raw/vanarsdel/dboCustomers.parquet\"\r\n",
							"\r\n",
							"va_customers_raw = sqlContext.read.parquet(va_customers_filePath)\r\n",
							"va_customers_pd = va_customers_raw.toPandas()\r\n",
							"\r\n",
							"va_customers = va_customers_pd[['CustomerID', 'LastName', 'FirstName', 'PhoneNumber', 'CreatedDate', 'UpdatedDate']]\r\n",
							"\r\n",
							"#conforming data types\r\n",
							"va_customers['CreatedDate'] = pd.to_datetime(va_customers_pd['CreatedDate'], errors='coerce')\r\n",
							"va_customers['UpdatedDate'] = pd.to_datetime(va_customers_pd['UpdatedDate'], errors='coerce')\r\n",
							"\r\n",
							"va_customers.PhoneNumber = va_customers_pd.PhoneNumber.astype(str)\r\n",
							"\r\n",
							"display(va_customers)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Load FourthCoffee Customers"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"fc_customers_filePath = mountPoint + \"/raw/fourthcoffee/Customers.csv\"\r\n",
							"\r\n",
							"fc_customers_pd = pd.read_csv(fc_customers_filePath)\r\n",
							"\r\n",
							"#select relevant columns\r\n",
							"fc_customers = fc_customers_pd[['CustomerID', 'LastName', 'FirstName', 'PhoneNumber', 'CreatedDate', 'UpdatedDate']]\r\n",
							"\r\n",
							"#conform data types\r\n",
							"fc_customers['CreatedDate'] = pd.to_datetime(fc_customers_pd['CreatedDate'], errors='coerce')\r\n",
							"fc_customers['UpdatedDate'] = pd.to_datetime(fc_customers_pd['UpdatedDate'], errors='coerce')\r\n",
							"fc_customers.PhoneNumber = fc_customers_pd.PhoneNumber.astype(str)\r\n",
							"\r\n",
							"display(fc_customers)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" Loading Southridge Addresses"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"sr_sales_addresses_parquet = mountPoint + \"/raw/southridge/cloudsales/dboAddresses.parquet\"\r\n",
							"sr_streaming_addresses_parquet = mountPoint + \"/raw/southridge/cloudstreaming/dboAddresses.parquet\"\r\n",
							"\r\n",
							"sr_sales_addresses = sqlContext.read.parquet(sr_sales_addresses_parquet)\r\n",
							"sr_streaming_addresses = sqlContext.read.parquet(sr_streaming_addresses_parquet)\r\n",
							"\r\n",
							"sr_sales_addresses = sr_sales_addresses.toPandas()\r\n",
							"sr_streaming_addresses = sr_streaming_addresses.toPandas()\r\n",
							"\r\n",
							"sr_addresses_frame = [sr_sales_addresses, sr_streaming_addresses]\r\n",
							"sr_addresses = pd.concat(sr_addresses_frame)\r\n",
							"\r\n",
							"sr_addresses['CreatedDate'] = pd.to_datetime(sr_addresses['CreatedDate'], errors='coerce')\r\n",
							"sr_addresses['UpdatedDate'] = pd.to_datetime(sr_addresses['UpdatedDate'], errors='coerce')\r\n",
							"\r\n",
							"sr_addresses.AddressLine2 = sr_addresses.AddressLine2.astype(str)\r\n",
							"sr_addresses.ZipCode = sr_addresses.ZipCode.astype(str)\r\n",
							"\r\n",
							"display(sr_addresses)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Loading VanArsdel Addresses"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"va_customers_filePath = mountPoint + \"/raw/vanarsdel/dboCustomers.parquet\"\r\n",
							"\r\n",
							"va_customers_raw = sqlContext.read.parquet(va_customers_filePath)\r\n",
							"va_customers_pd = va_customers_raw.toPandas()\r\n",
							"\r\n",
							"va_addresses = va_customers_pd[['CustomerID', 'AddressLine1', 'AddressLine2', 'City', 'State', 'ZipCode', 'CreatedDate', 'UpdatedDate']]\r\n",
							"\r\n",
							"va_addresses['CreatedDate'] = pd.to_datetime(va_addresses['CreatedDate'], errors='coerce')\r\n",
							"va_addresses['UpdatedDate'] = pd.to_datetime(va_addresses['UpdatedDate'], errors='coerce')\r\n",
							"\r\n",
							"va_addresses.AddressLine2 = va_addresses.AddressLine2.astype(str)\r\n",
							"va_addresses.ZipCode = va_addresses.ZipCode.astype(str)\r\n",
							"\r\n",
							"va_addresses.insert(0, 'AddressID', 'None')\r\n",
							"\r\n",
							"display(va_addresses)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Loading FourthCoffee Addresses"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"fc_customers_filePath = mountPoint + \"/raw/fourthcoffee/Customers.csv\"\r\n",
							"\r\n",
							"fc_customers_pd = pd.read_csv(fc_customers_filePath)\r\n",
							"\r\n",
							"fc_addresses = fc_customers_pd[['CustomerID', 'AddressLine1', 'AddressLine2', 'City', 'State', 'ZipCode', 'CreatedDate', 'UpdatedDate']]\r\n",
							"\r\n",
							"fc_addresses['CreatedDate'] = pd.to_datetime(fc_addresses['CreatedDate'], errors='coerce')\r\n",
							"fc_addresses['UpdatedDate'] = pd.to_datetime(fc_addresses['UpdatedDate'], errors='coerce')\r\n",
							"fc_addresses.AddressLine2 = fc_addresses.AddressLine2.astype(str)\r\n",
							"fc_addresses.ZipCode = fc_addresses.ZipCode.astype(str)\r\n",
							"\r\n",
							"#To conform all the data to a same set of columns, you will add the AddressID to this data frame so it will conform the Southridge's schema.\r\n",
							"fc_addresses.loc[fc_addresses.AddressLine2 == 'nan', 'AddressLine2'] = 'None'\r\n",
							"\r\n",
							"fc_addresses.insert(0, 'AddressID', 'None')\r\n",
							"\r\n",
							"display(fc_addresses)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Loading Southridge Movies and Actors - old school method"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"sr_movies_filepath = mountPoint + \"/raw/southridge/catalog/movies2.json\"\r\n",
							"\r\n",
							"sr_movies_raw = spark.read.option(\"multiLine\", \"true\").options(header='true', inferschema='true').json(sr_movies_filepath)\r\n",
							"sr_movies_pd = sr_movies_raw.toPandas()\r\n",
							"\r\n",
							"sr_movies_pd = sr_movies_pd[['actors', 'availabilityDate', 'genre', 'id', 'rating', 'releaseYear', 'runtime', 'streamingAvailabilityDate', 'tier', 'title']]\r\n",
							"\r\n",
							"#display(sr_movies_pd)\r\n",
							"\r\n",
							"movieactors = sr_movies_pd[['id', 'actors']]\r\n",
							"movies = sr_movies_pd[['id', 'title', 'genre', 'availabilityDate', 'rating', 'releaseYear', 'runtime', 'streamingAvailabilityDate', 'tier']]\r\n",
							"\r\n",
							"import numpy as np\r\n",
							"\r\n",
							"actorslist = movieactors.actors.values.tolist()\r\n",
							"actorcountbymovie = [len(r) for r in actorslist]\r\n",
							"explodedmovieids = np.repeat(movieactors.id, actorcountbymovie)\r\n",
							"\r\n",
							"movieactors = pd.DataFrame(np.column_stack((explodedmovieids, np.concatenate(actorslist))), columns=movieactors.columns)\r\n",
							"\r\n",
							"sr_all_moviesactorsactors = pd.merge(movies, movieactors, on='id')\r\n",
							"\r\n",
							"sr_all_moviesactorsactors = sr_all_moviesactorsactors.rename(index=str, columns={'id': 'MovieID', 'title': 'MovieTitle', 'genre': 'Genre', 'availabilityDate': 'AvailabilityDate', 'rating': 'Rating', 'releaseYear': 'ReleaseYear', 'runtime': 'RuntimeMin', 'streamingAvailabilityDate': 'StreamingAvailabilityDate', 'tier': 'Tier', 'actors': 'ActorName'})\r\n",
							"\r\n",
							"sr_all_moviesactorsactors['ActorID'] = 'None'\r\n",
							"sr_all_moviesactorsactors['MovieActorID'] = 'None'\r\n",
							"sr_all_moviesactorsactors['ActorGender'] = 'None'\r\n",
							"sr_all_moviesactorsactors['ReleaseDate'] = 'None'\r\n",
							"\r\n",
							"sr_all_moviesactorsactors.ReleaseYear = sr_all_moviesactorsactors.ReleaseYear.astype(str)\r\n",
							"sr_all_moviesactorsactors.Tier = sr_all_moviesactorsactors.Tier.astype(str)\r\n",
							"sr_all_moviesactorsactors.RuntimeMin = sr_all_moviesactorsactors.RuntimeMin.astype(str)\r\n",
							"\r\n",
							"sr_all_moviesactorsactors = sr_all_moviesactorsactors[['MovieID', 'MovieTitle', 'Genre', 'ReleaseDate', 'AvailabilityDate', 'StreamingAvailabilityDate', 'ReleaseYear', 'Tier', 'Rating', 'RuntimeMin', 'MovieActorID', 'ActorID', 'ActorName', 'ActorGender']]\r\n",
							"\r\n",
							"display(sr_all_moviesactorsactors)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Alternate (probably better) method to explode Actors array."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# After reading the JSON, explode the actors array to create multiple rows per movie, each having a single actor name\r\n",
							"# Drop the original actors array, as it is then no longer needed\r\n",
							"# Also drop the Cosmos DB metadata, as it will not be valuable here\r\n",
							"\r\n",
							"#import pyspark.sql.functions as F\r\n",
							"\r\n",
							"#sr_catalog = spark.read.json(\"abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/mdwoh/raw/southridge/catalog/movies.json\") \\\r\n",
							"#  .withColumn('actor', F.explode('actors')) \\\r\n",
							"#  .drop('actors', '_attachments', '_etag', '_rid', '_self', '_ts')\r\n",
							"\r\n",
							"#display(sr_catalog)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Loading VanArsdel Movies and Actors"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"va_movies_filepath = mountPoint + \"/raw/vanarsdel/dboMovies.parquet\"\r\n",
							"va_actors_filepath = mountPoint + \"/raw/vanarsdel/dboActors.parquet\"\r\n",
							"va_movieactors_filepath = mountPoint + \"/raw/vanarsdel/dboMovieActors.parquet\"\r\n",
							"\r\n",
							"va_movies_raw = sqlContext.read.parquet(va_movies_filepath)\r\n",
							"va_actors_raw = sqlContext.read.parquet(va_actors_filepath)\r\n",
							"va_movieactors_raw = sqlContext.read.parquet(va_movieactors_filepath)\r\n",
							"\r\n",
							"va_movies_pd = va_movies_raw.toPandas()\r\n",
							"va_actors_pd = va_actors_raw.toPandas()\r\n",
							"va_movieactors_pd = va_movieactors_raw.toPandas()\r\n",
							"\r\n",
							"va_all_movies = pd.merge(va_movieactors_pd, va_movies_pd, on='MovieID')\r\n",
							"va_all_movies = pd.merge(va_all_movies, va_actors_pd, on='ActorID')\r\n",
							"\r\n",
							"va_all_movies = va_all_movies.rename(index=str, columns={'Category': 'Genre', 'RunTimeMin': 'RuntimeMin', 'Gender': 'ActorGender'})\r\n",
							"\r\n",
							"va_all_movies['AvailabilityDate'] = 'None'\r\n",
							"va_all_movies['StreamingAvailabilityDate'] = 'None'\r\n",
							"va_all_movies['ReleaseYear'] = 'None'\r\n",
							"va_all_movies['Tier'] = 'None'\r\n",
							"\r\n",
							"va_all_movies.RuntimeMin = va_all_movies.RuntimeMin.astype(str)\r\n",
							"\r\n",
							"va_all_movies = va_all_movies[['MovieID', 'MovieTitle', 'Genre', 'ReleaseDate', 'AvailabilityDate', 'StreamingAvailabilityDate', 'ReleaseYear', 'Tier', 'Rating', 'RuntimeMin', 'MovieActorID', 'ActorID', 'ActorName', 'ActorGender']]\r\n",
							"\r\n",
							"#va_all_movies.dtypes\r\n",
							"\r\n",
							"display(va_all_movies)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Loading FourthCoffee Movies and Actors"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"fc_movies_filepath = mountPoint + \"/raw/fourthcoffee/Movies.csv\"\r\n",
							"fc_actors_filepath = mountPoint + \"/raw/fourthcoffee/Actors.csv\"\r\n",
							"fc_movieactors_filepath = mountPoint + \"/raw/fourthcoffee/MovieActors.csv\"\r\n",
							"\r\n",
							"fc_movies_pd = pd.read_csv(fc_movies_filepath)\r\n",
							"fc_actors_pd = pd.read_csv(fc_actors_filepath)\r\n",
							"fc_movieactors_pd = pd.read_csv(fc_movieactors_filepath)\r\n",
							"\r\n",
							"fc_all_moviesactors = pd.merge(fc_movieactors_pd, fc_movies_pd, on='MovieID')\r\n",
							"fc_all_moviesactors = pd.merge(fc_all_moviesactors, fc_actors_pd, on='ActorID')\r\n",
							"\r\n",
							"fc_all_moviesactors = fc_all_moviesactors.rename(index=str, columns={'Category': 'Genre', 'RunTimeMin': 'RuntimeMin', 'Gender': 'ActorGender'})\r\n",
							"\r\n",
							"fc_all_moviesactors['AvailabilityDate'] = 'None'\r\n",
							"fc_all_moviesactors['StreamingAvailabilityDate'] = 'None'\r\n",
							"fc_all_moviesactors['ReleaseYear'] = 'None'\r\n",
							"fc_all_moviesactors['Tier'] = 'None'\r\n",
							"\r\n",
							"fc_all_moviesactors.RuntimeMin = fc_all_moviesactors.RuntimeMin.astype(str)\r\n",
							"\r\n",
							"fc_all_moviesactors = fc_all_moviesactors[['MovieID', 'MovieTitle', 'Genre', 'ReleaseDate', 'AvailabilityDate', 'StreamingAvailabilityDate', 'ReleaseYear', 'Tier', 'Rating', 'RuntimeMin', 'MovieActorID', 'ActorID', 'ActorName', 'ActorGender']]\r\n",
							"\r\n",
							"display(fc_all_moviesactors)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Bringing all Customers together"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"sr_customers['SourceSystem'] = 'southridge'\r\n",
							"va_customers['SourceSystem'] = 'vanarsdel'\r\n",
							"fc_customers['SourceSystem'] = 'fourthcoffee'\r\n",
							"customers_frame = [sr_customers, va_customers, fc_customers]\r\n",
							"\r\n",
							"all_customers = pd.concat(customers_frame)\r\n",
							"\r\n",
							"display(all_customers)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Bring all Addresses together"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"sr_addresses['SourceSystem'] = 'southridge'\r\n",
							"va_addresses['SourceSystem'] = 'vanarsdel'\r\n",
							"fc_addresses['SourceSystem'] = 'fourthcoffee'\r\n",
							"addresses_frame = [sr_addresses, va_addresses, fc_addresses]\r\n",
							"\r\n",
							"all_addresses = pd.concat(addresses_frame)\r\n",
							"\r\n",
							"display(all_addresses)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Bringing all Movies and Actors together"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"sr_all_moviesactorsactors['SourceSystem'] = 'southridge'\r\n",
							"va_all_movies['SourceSystem'] = 'vanarsdel'\r\n",
							"fc_all_moviesactors['SourceSystem'] = 'fourthcoffee'\r\n",
							"\r\n",
							"moviesactors_frame = [sr_all_moviesactorsactors, va_all_movies, fc_all_moviesactors]\r\n",
							"\r\n",
							"all_moviesactors = pd.concat(moviesactors_frame)\r\n",
							"\r\n",
							"display(all_moviesactors)"
						],
						"outputs": [],
						"execution_count": 22
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CognitiveServices - Analyze Text')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SynapseML"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "80df2f74-9589-4ead-9773-f82a19384072"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark31",
						"name": "lgnsynspark31",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Cognitive Services - Analyze Text\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": true
							}
						},
						"source": [
							"import os\n",
							"\n",
							"if os.environ.get(\"AZURE_SERVICE\", None) == \"Microsoft.ProjectArcadia\":\n",
							"    from pyspark.sql import SparkSession\n",
							"    spark = SparkSession.builder.getOrCreate()\n",
							"    from notebookutils.mssparkutils.credentials import getSecret\n",
							"    os.environ['19b7a28cda994fb2b50a556327ad4012'] = getSecret(\"mmlspark-keys\", \"mmlspark-cs-key\")\n",
							"\n",
							"#put your service keys here\n",
							"key = os.environ['19b7a28cda994fb2b50a556327ad4012']\n",
							"location = 'eastus2'"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import udf, col\r\n",
							"from synapse.ml.io.http import HTTPTransformer, http_udf\r\n",
							"from requests import Request\r\n",
							"from pyspark.sql.functions import lit\r\n",
							"from pyspark.ml import PipelineModel\r\n",
							"from pyspark.sql.functions import col\r\n",
							"import os\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from mmlspark.cognitive import *\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Add your subscription key from the Language service (or a general Cognitive Service key)\r\n",
							"service_key = \"9f5a72151e634cc196e1b537e8bb4cbc\"\r\n",
							"\r\n",
							"df = spark.createDataFrame([\r\n",
							"  (\"I am so happy today, its sunny!\", \"en-US\"),\r\n",
							"  (\"I am frustrated by this rush hour traffic\", \"en-US\"),\r\n",
							"  (\"The cognitive services on spark aint bad\", \"en-US\"),\r\n",
							"], [\"text\", \"language\"])\r\n",
							"\r\n",
							"sentiment = (TextSentiment()\r\n",
							"    .setTextCol(\"text\")\r\n",
							"    .setLocation(\"eastus2\")\r\n",
							"    .setSubscriptionKey(service_key)\r\n",
							"    .setOutputCol(\"sentiment\")\r\n",
							"    .setErrorCol(\"error\")\r\n",
							"    .setLanguageCol(\"language\"))\r\n",
							"\r\n",
							"results = sentiment.transform(df)\r\n",
							"\r\n",
							"# Show the results in a table\r\n",
							"display(results.select(\"text\", col(\"sentiment\")[0].getItem(\"score\").alias(\"sentiment\")))"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"\n",
							"df = spark.createDataFrame(data=[\n",
							"        [\"en\", \"Hello Seattle\"],\n",
							"        [\"en\", \"There once was a dog who lived in London and thought she was a human\"]\n",
							"    ], \n",
							"    schema=[\"language\",\"text\"])"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"display(df)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"from synapse.ml.cognitive import *\n",
							"\n",
							"text_analyze = (TextAnalyze()\n",
							"    .setLocation(location)\n",
							"    .setSubscriptionKey(key)\n",
							"    .setTextCol(\"text\")\n",
							"    .setOutputCol(\"textAnalysis\")\n",
							"    .setErrorCol(\"error\")\n",
							"    .setLanguageCol(\"language\")\n",
							"    # set the tasks to perform\n",
							"    .setEntityRecognitionTasks([{\"parameters\": { \"model-version\": \"latest\"}}])\n",
							"    .setKeyPhraseExtractionTasks([{\"parameters\": { \"model-version\": \"latest\"}}])\n",
							"    # Uncomment these lines to add more tasks\n",
							"    # .setEntityRecognitionPiiTasks([{\"parameters\": { \"model-version\": \"latest\"}}])\n",
							"    # .setEntityLinkingTasks([{\"parameters\": { \"model-version\": \"latest\"}}])\n",
							"    # .setSentimentAnalysisTasks([{\"parameters\": { \"model-version\": \"latest\"}}])\n",
							"    )\n",
							"\n",
							"df_results = text_analyze.transform(df)\n",
							""
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"display(df_results)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import col\n",
							"\n",
							"# reformat and display for easier viewing\n",
							"display(\n",
							"    df_results.select(\"language\", \"text\", \"error\", col(\"textAnalysis\").getItem(0)) # we are not batching so only have a single result\n",
							"        .select(\"language\", \"text\", \"error\", \"textAnalysis[0].*\") # explode the Text Analytics tasks into columns\n",
							")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta Lake Feature Workarounds - Python')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PySpark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7f58372e-ad30-43f2-9dbe-a064f20e69ee"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Delta Lake 0.7+ feature workarounds\r\n",
							"Azure Synapse Analytics currently runs a fork of Delta Lake 0.6.x, which does not support all SQL commands and features available in Delta Lake 0.7+. This notebook contains Python workarounds for these commands and features.\r\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Generate some test data.\n",
							"df = spark.sql(\"SELECT 'foo' as Col1, 'bar' as Col2\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Creating managed tables (with or without partitions)\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"spark.sql(\"DROP TABLE IF EXISTS ManagedDeltaTable\")\r\n",
							"spark.sql(\"DROP TABLE IF EXISTS ExternalDeltaTable\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delta Lake 0.7+ SQL syntax: \n",
							"# \n",
							"# CREATE TABLE tableName USING DELTA\n",
							"\n",
							"df.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"externalTablePath = \"/tutorial/delta/externaltable\"\r\n",
							"\r\n",
							"df.write.format(\"delta\").mode(\"overwrite\").save(externalTablePath)\r\n",
							"spark.sql(\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{0}'\".format(externalTablePath))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delta Lake 0.7+ SQL syntax: \n",
							"# \n",
							"# CREATE TABLE tableName USING DELTA PARTITIONED BY (...)\n",
							"\n",
							"df.write.format(\"delta\").mode(\"append\").partitionBy(\"Col1\").option(\"__partition_columns\", \"\"\"[\"Col1\"]\"\"\").saveAsTable(\"PartitionedManagedDeltaTable\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Reading from a storage path\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delta Lake 0.7+ SQL syntax: \n",
							"# \n",
							"# SELECT * FROM delta.`/path/`\n",
							"\n",
							"spark.read.format(\"delta\").load(externalTablePath).show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Inserting from one table into another\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delta Lake 0.7+ SQL syntax: \n",
							"# \n",
							"# INSERT INTO table1 SELECT * FROM table2\n",
							"\n",
							"spark.sql(\"SELECT * FROM ManagedDeltaTable\").write.format(\"delta\").mode(\"append\").save(externalTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delta Lake 0.7+ SQL syntax: \n",
							"# \n",
							"# INSERT OVERWRITE table1 SELECT * FROM table2\n",
							"\n",
							"spark.sql(\"SELECT * FROM ManagedDeltaTable\").write.format(\"delta\").mode(\"overwrite\").save(externalTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Updating or deleting rows from a table\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delta Lake 0.7+ SQL syntax: \n",
							"# \n",
							"# DELETE FROM tableName WHERE (...)\n",
							"\n",
							"from delta.tables import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"dt = DeltaTable.forPath(spark, externalTablePath)\n",
							"\n",
							"dt.delete(\"Col1 == 'foo'\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# Delta Lake 0.7+ SQL syntax: \n",
							"# \n",
							"# UPDATE tableName SET (...)\n",
							"\n",
							"describeExtended = spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\")\n",
							"display(describeExtended)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Get the path to the table in storage.\r\n",
							"managedTablePath = describeExtended.where(\"col_name == 'Location'\").select(\"data_type\").collect()[0][0]\r\n",
							"\r\n",
							"# Construct the DeltaTable object from the path.\r\n",
							"managedTable = DeltaTable.forPath(spark, managedTablePath)\r\n",
							"\r\n",
							"# Run the update command.\r\n",
							"managedTable.update(condition = expr(\"Col1 == 'foo'\"), set = {\"Col2\": lit(\"foobar\")})"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delta Lake 0.7+ SQL syntax: \n",
							"# \n",
							"# UPDATE delta.`/path/` WHERE (...)\n",
							"\n",
							"DeltaTable.forPath(spark, externalTablePath).update(\n",
							"        condition = expr(\"Col1 == 'foo'\"),\n",
							"        set = {\"Col2\" : lit(\"foobar\")})"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Merging two tables\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delta Lake 0.7+ SQL syntax: \n",
							"# \n",
							"# MERGE INTO table1\n",
							"# USING table2\n",
							"# ON (...)\n",
							"# WHEN MATCHED THEN (...)\n",
							"# WHEN NOT MATCHED THEN (...)\n",
							"\n",
							"DeltaTable.forPath(spark, externalTablePath).alias(\"ExternalTable\").merge(\n",
							"  managedTable.alias(\"ManagedTable\").toDF(), \"ExternalTable.Col1 == ManagedTable.Col1\"\n",
							").whenMatchedUpdate(\n",
							"  set = {\"Col2\" : lit(\"This row matched.\")}\n",
							").whenNotMatchedInsert(\n",
							"  values = {\"Col2\" : lit(\"This row did not match.\")}).execute()\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Changing the schema of a managed table.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delta Lake 0.7+ SQL syntax: \n",
							"# \n",
							"# ALTER TABLE tableName ADD COLUMNS (...)\n",
							"# ALTER TABLE tableName CHANGE COLUMN (...)\n",
							"# ALTER TABLE tableName REPLACE COLUMNS (...)\n",
							"\n",
							"# Drop external table.\n",
							"spark.sql(\"DROP TABLE ExternalDeltaTable\")\n",
							"\n",
							"# Reconfigure the table using DataFrame APIs...\n",
							"\n",
							"# Recreate the table.\n",
							"df.write.format(\"delta\").mode(\"overwrite\").save(externalTablePath)\n",
							"spark.sql(\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{0}'\".format(externalTablePath))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuring table properties\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delta Lake 0.7+ SQL syntax: \n",
							"# \n",
							"# ALTER TABLE delta.`/path`\n",
							"# SET TBLPROPERTIES(...)\n",
							"# TBLPROPERTIES(\n",
							"# delta.compatibility.symlinkFormatManifest.enabled=true)\n",
							"\n",
							"# No workaround available.\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delta Lake 0.7+ SQL syntax: \n",
							"# \n",
							"# TBLPROPERTIES(delta.logRetentionDuration = \"interval <interval>\")\n",
							"# TBLPROPERTIES(delta.deletedFileRetentionDuration = \"interval <interval>\")\n",
							"\n",
							"# Can only set these globally.\n",
							"spark.conf.set(\"spark.databricks.delta.properties.defaults.delta.logRetentionDuration\", \"interval 2 days\")\n",
							"spark.conf.set(\"spark.databricks.delta.properties.defaults.delta.deletedFileRetentionDuration\", \"interval 1 days\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# SET spark.databricks.delta.commitInfo.userMetadata=â€{custom metadata}â€ INSERT â€¦\n",
							"\n",
							"# df.write.format(\"delta\")\n",
							"#   .mode(...)\n",
							"#   .option(\"userMetadata\", \"{custom metadata}\")\n",
							"#   .save(...)\n",
							"\n",
							"# No workaround available for these.\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## DeltaTable.forName()\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Delta Lake 0.7+ syntax: \n",
							"# \n",
							"# DeltaTable.forName(tableName)\n",
							"\n",
							"managedTablePath = spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").where(\"col_name == 'Location'\").select(\"data_type\").collect()[0][0]\n",
							"\n",
							"DeltaTable.forPath(spark, managedTablePath)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Explore NYC Yellow Taxi Data')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "11a41c90-14d1-4205-8f6a-8a84533aaa57"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/d926eada-2849-4d1f-9a0c-89b8eb87c962/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsyn/bigDataPools/lgnsynspark32",
						"name": "lgnsynspark32",
						"type": "Spark",
						"endpoint": "https://lgnsyn.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Explore NYC Yellow Taxi Data using Spark\n",
							"\n",
							"In this notebook, you'll learn the basic steps to load and analyze an Open Dataset that tracks NYC Yellow Taxi trips with Apache Spark for Azure Synapse.\n",
							"\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Load Data\n",
							"\n",
							"Read NYC Yellow Taxi data as a Spark DataFrame object to manipulate."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Read NYC yellow cab data from Azure Open Datasets\n",
							"from azureml.opendatasets import NycTlcYellow\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"\n",
							"end_date = parser.parse('2018-05-08 00:00:00')\n",
							"start_date = parser.parse('2018-05-01 00:00:00')\n",
							"\n",
							"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
							"df_nyc_tlc = nyc_tlc.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Analyze the NYC Taxi data using Spark and notebooks\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_nyc_tlc.printSchema()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"AvgTripDistance"
									],
									"values": [
										"passengerCount"
									],
									"yLabel": "passengerCount",
									"xLabel": "AvgTripDistance",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"passengerCount\":{\"2.382\":7,\"2.9365876998482907\":0,\"2.955385293728598\":1,\"3.0823106614325835\":6,\"3.1096431007047065\":5,\"3.124120509875713\":3,\"3.132080374155551\":4,\"3.1983281312300624\":2,\"6.23\":9,\"7.831666666666666\":8}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql import functions as F\n",
							"df_nyc = df_nyc_tlc.groupBy(\"passengerCount\").agg(F.avg('tripDistance').alias('AvgTripDistance'), F.sum('tripDistance').alias('SumTripDistance'))\n",
							"display(df_nyc)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Customize data visualization with Spark and notebooks\n",
							"You can control how charts render by using notebooks. The following code shows a simple example. It uses the popular libraries matplotlib and seaborn. The code renders the same kind of line chart as the SQL queries we ran earlier.\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import matplotlib.pyplot\n",
							"import seaborn\n",
							"\n",
							"seaborn.set(style = \"whitegrid\")\n",
							"pdf_nyc = df_nyc.toPandas()\n",
							"seaborn.lineplot(x=\"passengerCount\", y=\"SumTripDistance\" , data = pdf_nyc)\n",
							"seaborn.lineplot(x=\"passengerCount\", y=\"AvgTripDistance\" , data = pdf_nyc)\n",
							"matplotlib.pyplot.show()"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HL7Parsing')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "20651dcc-a0cf-4b40-8001-e7fd717fe202"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark32",
						"name": "lgnsynspark32",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# How to process HL7 2.x messages stored in SQL Server using Python\n",
							"\n",
							"Sample HL7 message:\n",
							"\n",
							"\n",
							"MSH|^~\\&|HIS|MedCenter|LIS|MedCenter|20060307110114||ORM^O01|MSGID20060307110114|P|2.3<br>\n",
							"PID|||12001||Jones^John^^^Mr.||19670824|M|||123 West St.^^Denver^CO^80020^USA|||||||<br>\n",
							"PV1||O|OP^PAREG^||||2342^Jones^Bob|||OP|||||||||2|||||||||||||||||||||||||20060307110111|<br>\n",
							"ORC|NW|20060307110114<br>\n",
							"OBR|1|20060307110114||003038^Urinalysis^L|||20060307110114"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\n",
							"import pyodbc\n",
							"\n",
							"sql_conn = pyodbc.connect('DRIVER={ODBC Driver 13 for SQL Server};SERVER=localhost;DATABASE=Training;Trusted_Connection=yes')"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"query = \"SELECT * FROM HL7Messages\"\n",
							"\n",
							"df = pd.read_sql(query, sql_conn)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"df"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"Python HL7 processing package:\n",
							"\n",
							"https://python-hl7.readthedocs.io/en/latest/"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import hl7"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"for index, row in df.iterrows():\n",
							"    h = hl7.parse(row['HL7Message'])\n",
							"    print(\"Message type: {}, Patient ID: {}\".format(h.segment('MSH')[9], h.segment('PID')[3]))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Hitchhikers Guide to Hyperspace - Python')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PySpark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cc131c31-0aaf-40c1-9a20-c5b82c9660d5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Hyperspace (Python)\n",
							"## An Indexing Subsystem for Apache Sparkâ„¢\n",
							"\n",
							"<img src=\"https://raw.githubusercontent.com/rapoth/hyperspace/master/docs/assets/images/hyperspace-small-banner.png\" alt=\"Hyperspace Indexing Sub-System Logo\" width=\"1000\"/>\n",
							"\n",
							"[Hyperspace](https://github.com/microsoft/hyperspace) introduces the ability for Apache Sparkâ„¢ users to create indexes on their datasets (e.g., CSV, JSON, Parquet etc.) and leverage them for potential query and workload acceleration.\n",
							"\n",
							"In this notebook, we highlight the basics of Hyperspace, emphasizing on its simplicity and show how it can be used by just anyone.\n",
							"\n",
							"**Disclaimer**: Hyperspace helps accelerate your workloads/queries under two circumstances:\n",
							"\n",
							"  1. Queries contain filters on predicates with high selectivity (e.g., you want to select 100 matching rows from a million candidate rows)\n",
							"  2. Queries contain a join that requires heavy-shuffles (e.g., you want to join a 100 GB dataset with a 10 GB dataset)\n",
							"\n",
							"You may want to carefully monitor your workloads and determine whether indexing is helping you on a case-by-case basis."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Setup\n",
							"To begin with, let's start a new Sparkâ„¢ session. Since this notebook is a tutorial merely to illustrate what Hyperspace can offer, we will make a configuration change that allow us to highlight what Hyperspace is doing on small datasets. By default, Sparkâ„¢ uses *broadcast join* to optimize join queries when the data size for one side of join is small (which is the case for the sample data we use in this tutorial). Therefore, we disable broadcast joins so that later when we run join queries, Sparkâ„¢ uses *sort-merge* join. This is mainly to show how Hyperspace indexes would be used at scale for accelerating join queries.\n",
							"\n",
							"The output of running the cell below shows a reference to the successfully created Sparkâ„¢ session and prints out '-1' as the value for the modified join config which indicates that broadcast join is successfully disabled."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000000)\n",
							"data_path = \"/hyperspace/data-{0}\".format(session_id)\n",
							"index_location = \"/hyperspace/indexes-{0}\".format(session_id)\n",
							"\n",
							"# Please note that you DO NOT need to change this configuration in production.\n",
							"# We store all indexes in the system folder within Synapse.\n",
							"spark.conf.set(\"spark.hyperspace.system.path\", index_location)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Start your Spark session\n",
							"spark\n",
							"\n",
							"# Disable BroadcastHashJoin, so Spark will use standard SortMergeJoin. Currently Hyperspace indexes utilize SortMergeJoin to speed up query.\n",
							"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
							"\n",
							"# Verify that BroadcastHashJoin is set correctly \n",
							"print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))\n",
							"spark.conf.set(\"spark.hyperspace.explain.displayMode\", \"html\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Preparation\n",
							"\n",
							"To prepare our environment, we will create sample data records and save them as parquet data files. While we use Parquet for illustration, you can use other formats such as CSV. In the subsequent cells, we will also demonstrate how you can create several Hyperspace indexes on this sample dataset and how one can make Sparkâ„¢ use them when running queries. \n",
							"\n",
							"Our example records correspond to two datasets: *department* and *employee*. You should configure \"empLocation\" and \"deptLocation\" paths so that on the storage account they point to your desired location to save generated data files. \n",
							"\n",
							"The output of running below cell shows contents of our datasets as lists of triplets followed by references to dataFrames created to save the content of each dataset in our preferred location."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
							"\n",
							"# Sample department records\n",
							"departments = [(10, \"Accounting\", \"New York\"), (20, \"Research\", \"Dallas\"), (30, \"Sales\", \"Chicago\"), (40, \"Operations\", \"Boston\")]\n",
							"\n",
							"# Sample employee records\n",
							"employees = [(7369, \"SMITH\", 20), (7499, \"ALLEN\", 30), (7521, \"WARD\", 30), (7566, \"JONES\", 20), (7698, \"BLAKE\", 30)]\n",
							"\n",
							"# Create a schema for the dataframe\n",
							"dept_schema = StructType([StructField('deptId', IntegerType(), True), StructField('deptName', StringType(), True), StructField('location', StringType(), True)])\n",
							"emp_schema = StructType([StructField('empId', IntegerType(), True), StructField('empName', StringType(), True), StructField('deptId', IntegerType(), True)])\n",
							"\n",
							"departments_df = spark.createDataFrame(departments, dept_schema)\n",
							"employees_df = spark.createDataFrame(employees, emp_schema)\n",
							"\n",
							"emp_Location = data_path + \"/employees.parquet\"\n",
							"dept_Location = data_path + \"/departments.parquet\"\n",
							"\n",
							"employees_df.write.mode(\"overwrite\").parquet(emp_Location)\n",
							"departments_df.write.mode(\"overwrite\").parquet(dept_Location)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Let's verify the contents of parquet files we created above to make sure they contain expected records in correct format. We later use these data files to create Hyperspace indexes and run sample queries.\n",
							"\n",
							"Running below cell, the output displays the rows in employee and department dataframes in a tabular form. There should be 14 employees and 4 departments, each matching with one of triplets we created in the previous cell."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# emp_Location and dept_Location are the user defined locations above to save parquet files\n",
							"emp_DF = spark.read.parquet(emp_Location)\n",
							"dept_DF = spark.read.parquet(dept_Location)\n",
							"\n",
							"# Verify the data is available and correct\n",
							"emp_DF.show()\n",
							"dept_DF.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Hello Hyperspace Index!\n",
							"Hyperspace lets users create indexes on records scanned from persisted data files. Once successfully created, an entry corresponding to the index is added to the Hyperspace's metadata. This metadata is later used by Apache Sparkâ„¢'s Hyperspace-enabled optimizer during query processing to find and use proper indexes. \n",
							"\n",
							"Once indexes are created, users can perform several actions:\n",
							"  - **Refresh** If the underlying data changes, users can refresh an existing index to capture that. \n",
							"  - **Delete** If the index is not needed, users can perform a soft-delete i.e., index is not physically deleted but is marked as 'deleted' so it is no longer used in your workloads.\n",
							"  - **Vacuum** If an index is no longer required, users can vacuum it which forces a physical deletion of the index contents and associated metadata completely from Hyperspace's metadata.\n",
							"\n",
							"Below sections show how such index management operations can be done in Hyperspace.\n",
							"\n",
							"First, we need to import the required libraries and create an instance of Hyperspace. We later use this instance to invoke different Hyperspace APIs to create indexes on our sample data and modify those indexes.\n",
							"\n",
							"Output of running below cell shows a reference to the created instance of Hyperspace."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from hyperspace import *\n",
							"\n",
							"# Create an instance of Hyperspace\n",
							"hyperspace = Hyperspace(spark)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Create Indexes\n",
							"To create a Hyperspace index, the user needs to provide 2 pieces of information:\n",
							"* An Apache Sparkâ„¢ DataFrame which references the data to be indexed.\n",
							"* An index configuration object: IndexConfig, which specifies the *index name*, *indexed* and *included* columns of the index. \n",
							"\n",
							"As you might have noticed, in this notebook, we illustrate indexing using the [Covering Index](https://www.red-gate.com/simple-talk/sql/learn-sql-server/using-covering-indexes-to-improve-query-performance/), which are the default index in Hyperspace. In the future, we plan on adding support for other index types. \n",
							"\n",
							"We start by creating three Hyperspace indexes on our sample data: two indexes on the department dataset named \"deptIndex1\" and \"deptIndex2\", and one index on the employee dataset named 'empIndex'. \n",
							"For each index, we need a corresponding IndexConfig to capture the name along with columns lists for the indexed and included columns. Running below cell creates these indexConfigs and its output lists them.\n",
							"\n",
							"**Note**: An *index column* is a column that appears in your filters or join conditions. An *included column* is a column that appears in your select/project.\n",
							"\n",
							"For instance, in the following query:\n",
							"```sql\n",
							"SELECT X\n",
							"FROM Table\n",
							"WHERE Y = 2\n",
							"```\n",
							"X can be an *index column* and Y can be an *included column*."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Create index configurations\n",
							"\n",
							"emp_IndexConfig = IndexConfig(\"empIndex1\", [\"deptId\"], [\"empName\"])\n",
							"dept_IndexConfig1 = IndexConfig(\"deptIndex1\", [\"deptId\"], [\"deptName\"])\n",
							"dept_IndexConfig2 = IndexConfig(\"deptIndex2\", [\"location\"], [\"deptName\"])"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Now, we create three indexes using our index configurations. For this purpose, we invoke \"createIndex\" command on our Hyperspace instance. This command requires an index configuration and the dataFrame containing rows to be indexed.\n",
							"Running below cell creates three indexes.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Create indexes from configurations\n",
							"\n",
							"hyperspace.createIndex(emp_DF, emp_IndexConfig)\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig1)\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig2)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### List Indexes\n",
							"\n",
							"Below code shows how a user can list all available indexes in a Hyperspace instance. It uses the `indexes` API which returns information about existing indexes as a Sparkâ„¢'s DataFrame so you can perform additional operations. For instance, you can invoke valid operations on this DataFrame for checking its content or analyzing it further (for example filtering specific indexes or grouping them according to some desired property). \n",
							"\n",
							"Below cell uses DataFrame's `show` action to fully print the rows and show details of our indexes in a tabular form. For each index, we can see all the information Hyperspace has stored about it in its metadata. \n",
							"\n",
							"You will immediately notice the following:\n",
							"  - `config.indexName`, `config.indexedColumns`, `config.includedColumns` are the fields that a user normally provides during index creation.\n",
							"  - `status.status` indicates if the index is being actively used by the Spark's optimizer.\n",
							"  - `dfSignature` is automatically generated by Hyperspace and is unique for each index. Hyperspace uses this signature internally to maintain the index and exploit it at query time. \n",
							"  \n",
							"In the output below, all three indexes should have \"ACTIVE\" as status and their name, indexed columns, and included columns should match with what we defined in index configurations above."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Delete Indexes\n",
							"A user can drop an existing index by using the `deleteIndex` API and providing the index name. \n",
							"\n",
							"Index deletion is a **soft-delete** operation i.e., only the index's status in the Hyperspace metadata from is changed from \"ACTIVE\" to \"DELETED\". This will exclude the deleted index from any future query optimization and Hyperspace no longer picks that index for any query. However, index files for a deleted index still remain available (since it is a soft-delete), so if you accidentally deleted the index, you could still restore it.\n",
							"\n",
							"The cell below deletes index with name \"deptIndex2\" and lists Hyperspace metadata after that. The output should be similar to above cell for \"List Indexes\" except for \"deptIndex2\" which now should have its status changed into \"DELETED\"."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.deleteIndex(\"deptIndex2\")\n",
							"\n",
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Restore Indexes\n",
							"A user can use the `restoreIndex` API to restore a deleted index. This will bring back the latest version of index into ACTIVE status and makes it usable again for queries. \n",
							"\n",
							"The cell below shows an example of `restoreIndex` API. We delete \"deptIndex1\" and restore it. The output shows \"deptIndex1\" first went into the \"DELETED\" status after invoking \"deleteIndex\" command and came back to the \"ACTIVE\" status after calling \"restoreIndex\"."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.deleteIndex(\"deptIndex1\")\n",
							"\n",
							"hyperspace.indexes().show()\n",
							"\n",
							"hyperspace.restoreIndex(\"deptIndex1\")\n",
							"\n",
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Vacuum Indexes\n",
							"The user can perform a **hard-delete** i.e., fully remove files and the metadata entry for a deleted index using the `vacuumIndex` API. Once done, this action is **irreversible** as it physically deletes all the index files associated with the index.\n",
							"\n",
							"The cell below vacuums the \"deptIndex2\" index and shows Hyperspace metadata after vaccuming. You should see metadata entries for two indexes \"deptIndex1\" and \"empIndex\" both with \"ACTIVE\" status and no entry for \"deptIndex2\"."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.vacuumIndex(\"deptIndex2\")\n",
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Enable/Disable Hyperspace\n",
							"\n",
							"Hyperspace provides APIs to enable or disable index usage with Sparkâ„¢.\n",
							"\n",
							"  - By using `enableHyperspace` API, Hyperspace optimization rules become visible to the Apache Sparkâ„¢ optimizer and it will exploit existing Hyperspace indexes to optimize user queries.\n",
							"  - By using `disableHyperspace` command, Hyperspace rules no longer apply during query optimization. You should note that disabling Hyperspace has no impact on created indexes as they remain intact.\n",
							"\n",
							"Below cell shows how you can use these commands to enable or disable hyperspace. The output simply shows a reference to the existing Sparkâ„¢ session whose configuration is updated."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Enable Hyperspace\n",
							"Hyperspace.enable(spark)\n",
							"\n",
							"# Disable Hyperspace\n",
							"Hyperspace.disable(spark)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Index Usage\n",
							"In order to make Spark use Hyperspace indexes during query processing, the user needs to make sure that Hyperspace is enabled. \n",
							"\n",
							"The cell below enables Hyperspace and creates two DataFrames containing our sample data records which we use for running example queries. For each DataFrame, a few sample rows are printed."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Enable Hyperspace\n",
							"Hyperspace.enable(spark)\n",
							"\n",
							"emp_DF = spark.read.parquet(emp_Location)\n",
							"dept_DF = spark.read.parquet(dept_Location)\n",
							"\n",
							"emp_DF.show(5)\n",
							"dept_DF.show(5)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Hyperspace's Index Types\n",
							"\n",
							"Currently, Hyperspace can exploit indexes for two groups of queries: \n",
							"* Selection queries with lookup or range selection filtering predicates.\n",
							"* Join queries with an equality join predicate (i.e. Equi-joins)."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Indexes for Accelerating Filters\n",
							"\n",
							"Our first example query does a lookup on department records (see below cell). In SQL, this query looks as follows:\n",
							"\n",
							"```sql\n",
							"SELECT deptName \n",
							"FROM departments\n",
							"WHERE deptId = 20\n",
							"```\n",
							"\n",
							"The output of running the cell below shows: \n",
							"- query result, which is a single department name.\n",
							"- query plan that Sparkâ„¢ used to run the query. \n",
							"\n",
							"In the query plan, the \"FileScan\" operator at the bottom of the plan shows the datasource where the records were read from. The location of this file indicates the path to the latest version of the \"deptIndex1\" index. This shows  that according to the query and using Hyperspace optimization rules, Sparkâ„¢ decided to exploit the proper index at runtime.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Filter with equality predicate\n",
							"\n",
							"eqFilter = dept_DF.filter(\"\"\"deptId = 20\"\"\").select(\"\"\"deptName\"\"\")\n",
							"eqFilter.show()\n",
							"\n",
							"hyperspace.explain(eqFilter, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Our second example is a range selection query on department records. In SQL, this query looks as follows:\n",
							"\n",
							"```sql\n",
							"SELECT deptName \n",
							"FROM departments\n",
							"WHERE deptId > 20\n",
							"```\n",
							"Similar to our first example, the output of the cell below shows the query results (names of two departments) and the query plan. The location of data file in the FileScan operator shows that 'deptIndex1\" was used to run the query.   \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Filter with range selection predicate\n",
							"\n",
							"rangeFilter = dept_DF.filter(\"\"\"deptId > 20\"\"\").select(\"deptName\")\n",
							"rangeFilter.show()\n",
							"\n",
							"hyperspace.explain(rangeFilter, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Our third example is a query joining department and employee records on the department id. The equivalent SQL statement is shown below:\n",
							"\n",
							"```sql\n",
							"SELECT employees.deptId, empName, departments.deptId, deptName\n",
							"FROM   employees, departments \n",
							"WHERE  employees.deptId = departments.deptId\n",
							"```\n",
							"\n",
							"The output of running the cell below shows the query results which are the names of 14 employees and the name of department each employee works in. The query plan is also included in the output. Notice how the file locations for two FileScan operators shows that Spark used \"empIndex\" and \"deptIndex1\" indexes to run the query.   \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Join\n",
							"\n",
							"eqJoin = emp_DF.join(dept_DF, emp_DF.deptId == dept_DF.deptId).select(emp_DF.empName, dept_DF.deptName)\n",
							"\n",
							"eqJoin.show()\n",
							"\n",
							"hyperspace.explain(eqJoin, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Support for SQL Semantics\n",
							"\n",
							"The index usage is transparent to whether the user uses DataFrame API or Sparkâ„¢ SQL. The following example shows the same join example as before but using Spark SQL, showing the use of indexes if applicable."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"\n",
							"emp_DF.createOrReplaceTempView(\"EMP\")\n",
							"dept_DF.createOrReplaceTempView(\"DEPT\")\n",
							"\n",
							"joinQuery = spark.sql(\"SELECT EMP.empName, DEPT.deptName FROM EMP, DEPT WHERE EMP.deptId = DEPT.deptId\")\n",
							"\n",
							"joinQuery.show()\n",
							"hyperspace.explain(joinQuery, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Explain API\n",
							"\n",
							"So far, you might have observed we have been using the explain API provided by Hyperspace. The `explain` API from Hyperspace is very similar to Spark's `df.explain` API but allows users to compare their original plan vs the updated index-dependent plan before running their query. You have an option to choose from html/plaintext/console mode to display the command output. \n",
							"\n",
							"The following cell shows an example with HTML. The highlighted section represents the difference between original and updated plans along with the indexes being used."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"eqJoin = emp_DF.join(dept_DF, emp_DF.deptId == dept_DF.deptId).select(emp_DF.empName, dept_DF.deptName)\n",
							"\n",
							"spark.conf.set(\"spark.hyperspace.explain.displayMode\", \"html\")\n",
							"hyperspace.explain(eqJoin, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Refresh Indexes\n",
							"If the original data on which an index was created changes, then the index will no longer capture the latest state of data and hence will not be used by Hyperspace to provide any acceleration. The user can refresh such a stale index using the `refreshIndex` API. This causes the index to be fully rebuilt and updates it according to the latest data records.\n",
							"    \n",
							"    Spoiler alert: if you are worried about fully rebuilding your index every time your data changes, don't worry! We will show you how to *incrementally refresh* your index in subsequent cells below.\n",
							"\n",
							"The two cells below show an example for this scenario:\n",
							"- First cell adds two more departments to the original departments data. It reads and prints list of departments to verify new departments are added correctly. The output shows 6 departments in total: four old ones and two new. Invoking \"refreshIndex\" updates \"deptIndex1\" so index captures new departments.\n",
							"- Second cell runs our range selection query example. The results should now contain four departments: two are the ones, seen before when we ran the query above, and two are the new departments we just added."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"extra_Departments = [(50, \"Inovation\", \"Seattle\"), (60, \"Human Resources\", \"San Francisco\")]\n",
							"\n",
							"extra_departments_df = spark.createDataFrame(extra_Departments, dept_schema)\n",
							"extra_departments_df.write.mode(\"Append\").parquet(dept_Location)\n",
							"\n",
							"\n",
							"dept_DFrame_Updated = spark.read.parquet(dept_Location)\n",
							"\n",
							"dept_DFrame_Updated.show(10)\n",
							"\n",
							"hyperspace.refreshIndex(\"deptIndex1\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"newRangeFilter = dept_DFrame_Updated.filter(\"deptId > 20\").select(\"deptName\")\n",
							"newRangeFilter.show()\n",
							"\n",
							"hyperspace.explain(newRangeFilter, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.indexes().show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Clean-up the remaining indexes\n",
							"hyperspace.deleteIndex(\"empIndex1\")\n",
							"hyperspace.deleteIndex(\"deptIndex1\")\n",
							"\n",
							"hyperspace.vacuumIndex(\"empIndex1\")\n",
							"hyperspace.vacuumIndex(\"deptIndex1\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Hybrid Scan for Mutable Datasets\n",
							"\n",
							"Often times, if your underlying source data had some new files appended or existing files deleted, your index will get stale and Hyperspace decides not to use it. However, there are times where you just want to use the index without having to refresh it everytime. There could be multiple reasons for doing so:\n",
							"\n",
							"  1. You do not want to continuosly refresh your index but instead want to do it periodically since you understand your workloads the best.  \n",
							"  2. You added/removed only a few files and do not want to wait for yet another refresh job to finish. \n",
							"\n",
							"To allow you to still use a stale index, Hyperspace introduces **Hybrid Scan**, a novel technique that allows users to utilize outdated or stale indexes (e.g., the underlying source data had some new files appended or existing files deleted), without refreshing indexes. \n",
							"\n",
							"To achieve this, when you set the appropriate configuration to enable Hybrid Scan, Hyperspace modifies the query plan to leverage the changes as following:\n",
							"- Appended files can be merged to index data by using **`Union` or `BucketUnion` (for join)**. Shuffling appended data can also be applied before merging, if needed.\n",
							"- Deleted files can be handled by injecting `Filter-NOT-IN` condition on **lineage column** of index data, so that the indexed rows from the deleted files can be excluded at query time. \n",
							"\n",
							"You can check the transformation of the query plan in below examples.\n",
							"\n",
							"    Note: Hybrid scan is only supported for non-partitioned data. Support for partitioned data is currently being worked upon."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Hybrid Scan for appended files - non-partitioned data\n",
							"\n",
							"Non-partitioned data is used in below example. In this example, we expect Join index can be used for the query and `BucketUnion` is introduced for appended files."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# GENERATE TEST DATA\n",
							"\n",
							"testdata = [\n",
							"    (\"orange\", 3, \"2020-10-01\"),\n",
							"    (\"banana\", 1, \"2020-10-01\"),\n",
							"    (\"carrot\", 5, \"2020-10-02\"),\n",
							"    (\"beetroot\", 12, \"2020-10-02\"),\n",
							"    (\"orange\", 2, \"2020-10-03\"),\n",
							"    (\"banana\", 11, \"2020-10-03\"),\n",
							"    (\"carrot\", 3, \"2020-10-03\"),\n",
							"    (\"beetroot\", 2, \"2020-10-04\"),\n",
							"    (\"cucumber\", 7, \"2020-10-05\"),\n",
							"    (\"pepper\", 20, \"2020-10-06\")\n",
							"]\n",
							"\n",
							"testdata_location = data_path + \"/productTable\"\n",
							"from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
							"testdata_schema = StructType([\n",
							"    StructField('name', StringType(), True),\n",
							"    StructField('qty', IntegerType(), True),\n",
							"    StructField('date', StringType(), True)])\n",
							"\n",
							"test_df = spark.createDataFrame(testdata, testdata_schema)\n",
							"test_df.write.mode(\"overwrite\").parquet(testdata_location)\n",
							"test_df = spark.read.parquet(testdata_location)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# CREATE INDEX\n",
							"hyperspace.createIndex(test_df, IndexConfig(\"productIndex2\", [\"name\"], [\"date\", \"qty\"]))\n",
							"\n",
							"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
							"filter1 = test_df.filter(\"name = 'banana'\")\n",
							"filter2 = test_df.filter(\"qty > 10\")\n",
							"query = filter1.join(filter2, \"name\")\n",
							"\n",
							"# Check Join index rule is applied properly.\n",
							"hyperspace.explain(query, True, displayHTML)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Append new files.\r\n",
							"append_data = [\r\n",
							"    (\"orange\", 13, \"2020-11-01\"),\r\n",
							"    (\"banana\", 5, \"2020-11-01\")\r\n",
							"]\r\n",
							"append_df = spark.createDataFrame(append_data, testdata_schema)\r\n",
							"append_df.write.mode(\"append\").parquet(testdata_location)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Hybrid scan is disabled by default. Therefore, you will see that since we appended new data, Hyperspace will decide NOT to use the index.\n",
							"\n",
							"In the output, you will see no plan differences (hence no highlighting)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Hybrid Scan configs are false by default.\n",
							"spark.conf.set(\"spark.hyperspace.index.hybridscan.enabled\", \"false\")\n",
							"spark.conf.set(\"spark.hyperspace.index.hybridscan.delete.enabled\", \"false\")\n",
							"\n",
							"test_df_with_append = spark.read.parquet(testdata_location)\n",
							"filter1 = test_df_with_append.filter(\"name = 'banana'\")\n",
							"filter2 = test_df_with_append.filter(\"qty > 10\")\n",
							"query = filter1.join(filter2, \"name\")\n",
							"hyperspace.explain(query, True, displayHTML)\n",
							"query.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Enable Hybrid Scan\r\n",
							"\r\n",
							"In plan with indexes, you can see\r\n",
							"`Exchange hashpartitioning` required only for appended files so that we could still utilize the \"shuffled\" index data with appended files. `BucketUnion` is used to merge \"shuffled\" appended files with the index data."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Enable Hybrid Scan config. \"delete\" config is not necessary.\n",
							"spark.conf.set(\"spark.hyperspace.index.hybridscan.enabled\", \"true\")\n",
							"# spark.conf.set(\"spark.hyperspace.index.hybridscan.delete.enabled\", \"true\")\n",
							"\n",
							"# Need to redefine query to recalculate the query plan.\n",
							"query = filter1.join(filter2, \"name\")\n",
							"hyperspace.explain(query, True, displayHTML)\n",
							"query.show()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Incremental Index Refresh\n",
							"When you ready to update your indexes but do not want to rebuild your entire index, Hyperspace supports updating indexes in an incremental manner using `hs.refreshIndex(\"name\", \"incremental\")` API. This will allow eliminate the need for a full rebuild of index from scratch, utilizing previously created index files as well as updating indexes on only the newly added data.\n",
							"\n",
							"Of course, please be sure to use the complementary `optimizeIndex` API (shown below) periodically to make sure you do not see performance regressions. We recommend calling `optimize` at least once for every 10 times you call `refreshIndex(..., \"incremental\")`, assuming the data you added/removed is < 10% of the original dataset. For instance, if your original dataset is 100 GB, and you've added/removed data in increments/decrements of 1 GB, you can call `refreshIndex` 10 times before calling `optimizeIndex`. Please note that this example is simply used for illustration and you have to adapt this for your workloads.\n",
							"\n",
							"In the example below, notice the addition of a `Sort` node in the query plan when indexes are used. This is because partial indexes are created on the appended data files, causing Spark to introduce a `Sort`. Please also note that `Shuffle` i.e. `Exchange` is still eliminated from the plan, giving you the appropriate acceleration."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def query():\n",
							"    test_df_with_append = spark.read.parquet(testdata_location)\n",
							"    filter1 = test_df_with_append.filter(\"name = 'banana'\")\n",
							"    filter2 = test_df_with_append.filter(\"qty > 10\")\n",
							"    return filter1.join(filter2, \"name\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.refreshIndex(\"productIndex2\", \"incremental\")\n",
							"\n",
							"hyperspace.explain(query(), True, displayHTML)\n",
							"query().show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Optimize Index layout\n",
							"After calling incremental refreshes multiple times on newly appended data (e.g. if the user writes to data in small batches or in case of streaming scenarios), the number of index files tend to become large affecting the performance of the index (large number of small files problem). Hyperspace provides `hyperspace.optimizeIndex(\"indexName\")` API to optimize the index layout and reduce the large files problem.\n",
							"\n",
							"In the plan below, notice that Hyperspace has removed the additional `Sort` node in the query plan. Optimize can help avoiding sorting for any index bucket which contains only one file. However, this will only be true if ALL the index buckets have at most 1 file per bucket, after `optimizeIndex`."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Append some more data and call refresh again.\n",
							"append_data = [\n",
							"    (\"orange\", 13, \"2020-11-01\"),\n",
							"    (\"banana\", 5, \"2020-11-01\")\n",
							"]\n",
							"append_df = spark.createDataFrame(append_data, testdata_schema)\n",
							"append_df.write.mode(\"append\").parquet(testdata_location)\n",
							"\n",
							"hyperspace.refreshIndex(\"productIndex2\", \"incremental\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# Call optimize. Ensure that Sort is removed after optimization (This is possible here because after optimize, in this case, every bucket contains only 1 file.).\n",
							"hyperspace.optimizeIndex(\"productIndex2\")\n",
							"\n",
							"hyperspace.explain(query(), True, displayHTML)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Optimize modes\n",
							"\n",
							"The default mode for optimization is \"quick\" mode where files smaller than a predefined threshold are picked for optmization. To maximize the effect of optimization, Hyperspace allows another optimize mode \"full\" as shown below. This mode picks ALL index files for optimization irrespective of their file size and creates the best possible layout of the index. This is also slower than the default optimize mode because more data is being processed here.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"hyperspace.optimizeIndex(\"productIndex2\", \"full\")\n",
							"\n",
							"hyperspace.explain(query(), True, displayHTML)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Clean Up\n",
							"To make this notebook self-contained and not leave any dangling data, we have some small clean-up code below. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.rm(data_path, True)\n",
							"mssparkutils.fs.rm(index_location, True)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Hitchikers Guide to Delta Lake - CSharp')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": ".NET"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "808631e7-0614-4d58-8f6d-21c0fb8bea42"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "csharp"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark32",
						"name": "lgnsynspark32",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Delta Lake (.NET for Spark C#)\n",
							"\n",
							"This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"Here's what we will cover:\n",
							"* Create a table\n",
							"* Understanding meta-data\n",
							"* Read data\n",
							"* Update table data\n",
							"* Overwrite table data\n",
							"* Conditional update without overwrite\n",
							"* Read older versions of data using Time Travel\n",
							"* Write a stream of data to a table\n",
							"* Read a stream of changes from a table"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuration\n",
							"Make sure you modify this as appropriate."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"var sessionId = (new Random()).Next(10000000);\n",
							"var deltaTablePath = $\"/delta/delta-table-{sessionId}\";\n",
							"\n",
							"deltaTablePath"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a table\n",
							"To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
							"\n",
							"These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"var data = spark.Range(0,5);\n",
							"data.Show();\n",
							"data.Write().Format(\"delta\").Save(deltaTablePath);"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"using System.Linq;\n",
							"spark.Read().Text($\"{deltaTablePath}/_delta_log/\").Collect().ToList().ForEach(x => Console.WriteLine(x.GetAs<string>(\"value\")));"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read data\n",
							"\n",
							"You read data in your Delta Lake table by specifying the path to the files."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"var df = spark.Read().Format(\"delta\").Load(deltaTablePath);\n",
							"df.Show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table data\n",
							"\n",
							"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"var data = spark.Range(5,10);\n",
							"data.Write().Format(\"delta\").Mode(\"overwrite\").Save(deltaTablePath);\n",
							"df.Show();"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.Read().Text($\"{deltaTablePath}/_delta_log/\").Collect().ToList().ForEach(x => Console.WriteLine(x.GetAs<string>(\"value\")));"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as catalog tables\n",
							"\n",
							"Delta Lake can write to managed or external catalog tables.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"// Write data to a new managed catalog table.\n",
							"data.Write().Format(\"delta\").SaveAsTable(\"ManagedDeltaTable\");"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"// Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.Sql($\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{deltaTablePath}'\");"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// List the 2 new tables.\n",
							"spark.Sql(\"SHOW TABLES\").Show();\n",
							"\n",
							"// Explore their properties.\n",
							"spark.Sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").Show(truncate: 0);\n",
							"spark.Sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").Show(truncate: 0);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Conditional update without overwrite\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"using Microsoft.Spark.Extensions.Delta;\n",
							"using Microsoft.Spark.Extensions.Delta.Tables;\n",
							"using Microsoft.Spark.Sql;\n",
							"using static Microsoft.Spark.Sql.Functions;\n",
							"\n",
							"var deltaTable = DeltaTable.ForPath(deltaTablePath);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Update every even value by adding 100 to it\n",
							"deltaTable.Update(\n",
							"  condition: Expr(\"id % 2 == 0\"),\n",
							"  set: new Dictionary<string, Column>(){{ \"id\", Expr(\"id + 100\") }});\n",
							"deltaTable.ToDF().Show();"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Delete every even value\n",
							"deltaTable.Delete(condition: Expr(\"id % 2 == 0\"));\n",
							"deltaTable.ToDF().Show();"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Upsert (merge) new data\n",
							"var newData = spark.Range(20).As(\"newData\");\n",
							"\n",
							"deltaTable\n",
							"    .As(\"oldData\")\n",
							"    .Merge(newData, \"oldData.id = newData.id\")\n",
							"    .WhenMatched()\n",
							"        .Update(new Dictionary<string, Column>() {{\"id\", Lit(\"-1\")}})\n",
							"    .WhenNotMatched()\n",
							"        .Insert(new Dictionary<string, Column>() {{\"id\", Col(\"newData.id\")}})\n",
							"    .Execute();\n",
							"\n",
							"deltaTable.ToDF().Show(100);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## History\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.History().Show(20, 1000, false);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read older versions of data using Time Travel\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"var df = spark.Read().Format(\"delta\").Option(\"versionAsOf\", 0).Load(deltaTablePath);\n",
							"df.Show();"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a stream of data to a table\n",
							"\n",
							"You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
							"\n",
							"For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
							"\n",
							"In the cells below, here's what we are doing:\n",
							"\n",
							"1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
							"2. *Cell 30* Show the newly appended data\n",
							"3. *Cell 31* Inspect history\n",
							"4. *Cell 32* Stop the structured streaming job\n",
							"5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"var streamingDf = spark.ReadStream().Format(\"rate\").Load();\n",
							"var stream = streamingDf.SelectExpr(\"value as id\").WriteStream().Format(\"delta\").Option(\"checkpointLocation\", $\"/tmp/checkpoint-{sessionId}\").Start(deltaTablePath);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read a stream of changes from a table\n",
							"\n",
							"While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.ToDF().Sort(Col(\"id\").Desc()).Show(100);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.History().Drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").Show(20, 1000, false);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"stream.Stop();"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.History().Drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").Show(100, 1000, false);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Compaction\n",
							"\n",
							"If a Delta Table is growing too large, you can compact it by repartitioning into a smaller number of files.\n",
							"\n",
							"The option `dataChange = false` is an optimization that tells Delta Lake to do the repartition without marking the underlying data as \"modified\". This ensures that any other concurrent operations (such as streaming reads/writes) aren't negatively impacted.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"int partitionCount = 2;\n",
							"\n",
							"spark.Read()\n",
							"    .Format(\"delta\")\n",
							"    .Load(deltaTablePath)\n",
							"    .Repartition(partitionCount)\n",
							"    .Write()\n",
							"    .Option(\"dataChange\", \"false\")\n",
							"    .Format(\"delta\")\n",
							"    .Mode(\"overwrite\")\n",
							"    .Save(deltaTablePath);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert Parquet to Delta\n",
							"You can do an in-place conversion from the Parquet format to Delta."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"var parquetPath = $\"/parquet/parquet-table-{sessionId}\";\n",
							"\n",
							"var data = spark.Range(0,5);\n",
							"data.Write().Parquet(parquetPath);\n",
							"\n",
							"// Confirm that the data isn't in the Delta format\n",
							"DeltaTable.IsDeltaTable(parquetPath)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.ConvertToDelta(spark, $\"parquet.`{parquetPath}`\");\n",
							"\n",
							"//Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.IsDeltaTable(parquetPath)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## SQL Support\n",
							"Delta supports table utility commands through SQL.  You can use SQL to:\n",
							"* Get a DeltaTable's history\n",
							"* Vacuum a DeltaTable\n",
							"* Convert a Parquet file to Delta\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.Sql($\"DESCRIBE HISTORY delta.`{deltaTablePath}`\").Show();"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.Sql($\"VACUUM delta.`{deltaTablePath}`\").Show();"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"var parquetId =  (new Random()).Next(10000000);\n",
							"var parquetPath = $\"/parquet/parquet-table-{sessionId}-{parquetId}\";\n",
							"\n",
							"var data = spark.Range(0,5);\n",
							"data.Write().Parquet(parquetPath);\n",
							"\n",
							"// Confirm that the data isn't in the Delta format\n",
							"DeltaTable.IsDeltaTable(parquetPath);\n",
							"\n",
							"// Use SQL to convert the parquet table to Delta\n",
							"spark.Sql($\"CONVERT TO DELTA parquet.`{parquetPath}`\");\n",
							"\n",
							"DeltaTable.IsDeltaTable(parquetPath);"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Hitchikers Guide to Delta Lake - Python')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PySpark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "efd0e350-2ba7-424b-abef-40cbe59ffbc4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark32",
						"name": "lgnsynspark32",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Delta Lake (Python)\n",
							"\n",
							"This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"Here's what we will cover:\n",
							"* Create a table\n",
							"* Understanding meta-data\n",
							"* Read data\n",
							"* Update table data\n",
							"* Overwrite table data\n",
							"* Conditional update without overwrite\n",
							"* Read older versions of data using Time Travel\n",
							"* Write a stream of data to a table\n",
							"* Read a stream of changes from a table"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuration\n",
							"Make sure you modify this as appropriate."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000000)\n",
							"delta_table_path = \"/delta/delta-table-{0}\".format(session_id)\n",
							"\n",
							"delta_table_path"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a table\n",
							"To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
							"\n",
							"These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(0,5)\n",
							"data.show()\n",
							"data.write.format(\"delta\").save(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read data\n",
							"\n",
							"You read data in your Delta Lake table by specifying the path to the files."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").load(delta_table_path)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table data\n",
							"\n",
							"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(5,10)\n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as catalog tables\n",
							"\n",
							"Delta Lake can write to managed or external catalog tables."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Write data to a new managed catalog table.\n",
							"data.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable71422\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"# Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.sql(\"CREATE TABLE ExternalDeltaTable71422 USING DELTA LOCATION '{0}'\".format(delta_table_path))"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# List the 2 new tables.\n",
							"spark.sql(\"SHOW TABLES\").show()\n",
							"\n",
							"# Explore their properties.\n",
							"spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable71422\").show(truncate=False)\n",
							"spark.sql(\"DESCRIBE EXTENDED ExternalDeltaTable71422\").show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Conditional update without overwrite\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"delta_table = DeltaTable.forPath(spark, delta_table_path)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"# Update every even value by adding 100 to it\n",
							"delta_table.update(\n",
							"  condition = expr(\"id % 2 == 0\"),\n",
							"  set = { \"id\": expr(\"id + 100\") })\n",
							"delta_table.toDF().show()"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"# Delete every even value\n",
							"delta_table.delete(\"id % 2 == 0\")\n",
							"delta_table.toDF().show()"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"# Upsert (merge) new data\n",
							"new_data = spark.range(0,20).alias(\"newData\")\n",
							"\n",
							"delta_table.alias(\"oldData\")\\\n",
							"    .merge(new_data.alias(\"newData\"), \"oldData.id = newData.id\")\\\n",
							"    .whenMatchedUpdate(set = { \"id\": lit(\"-1\")})\\\n",
							"    .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") })\\\n",
							"    .execute()\n",
							"\n",
							"delta_table.toDF().show(100)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"## History\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().show(20, 1000, False)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read older versions of data using Time Travel\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a stream of data to a table\n",
							"\n",
							"You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
							"\n",
							"For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
							"\n",
							"In the cells below, here's what we are doing:\n",
							"\n",
							"1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
							"2. *Cell 30* Show the newly appended data\n",
							"3. *Cell 31* Inspect history\n",
							"4. *Cell 32* Stop the structured streaming job\n",
							"5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"streaming_df = spark.readStream.format(\"rate\").load()\n",
							"stream = streaming_df\\\n",
							"    .selectExpr(\"value as id\")\\\n",
							"    .writeStream\\\n",
							"    .format(\"delta\")\\\n",
							"    .option(\"checkpointLocation\", \"/tmp/checkpoint-{0}\".format(session_id))\\\n",
							"    .start(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read a stream of changes from a table\n",
							"\n",
							"While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.toDF().sort(col(\"id\").desc()).show(100)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(20, 1000, False)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							"stream.stop()"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(100, 1000, False)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Compaction\n",
							"\n",
							"If a Delta Table is growing too large, you can compact it by repartitioning into a smaller number of files.\n",
							"\n",
							"The option `dataChange = false` is an optimization that tells Delta Lake to do the repartition without marking the underlying data as \"modified\". This ensures that any other concurrent operations (such as streaming reads/writes) aren't negatively impacted.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"partition_count = 2\n",
							"\n",
							"spark.read\\\n",
							"    .format(\"delta\")\\\n",
							"    .load(delta_table_path)\\\n",
							"    .repartition(partition_count)\\\n",
							"    .write.option(\"dataChange\", \"false\")\\\n",
							"    .format(\"delta\")\\\n",
							"    .mode(\"overwrite\")\\\n",
							"    .save(delta_table_path)    "
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert Parquet to Delta\n",
							"You can do an in-place conversion from the Parquet format to Delta."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = \"/parquet/parquet-table-{0}\".format(session_id)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.convertToDelta(spark, \"parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"# Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"source": [
							"## SQL Support\n",
							"Delta supports table utility commands through SQL.  You can use SQL to:\n",
							"* Get a DeltaTable's history\n",
							"* Vacuum a DeltaTable\n",
							"* Convert a Parquet file to Delta\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DESCRIBE HISTORY delta.`{0}`\".format(delta_table_path)).show()"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"VACUUM delta.`{0}`\".format(delta_table_path)).show()"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_id = random.randint(0,1000)\n",
							"parquet_path = \"/parquet/parquet-table-{0}-{1}\".format(session_id, parquet_path)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)\n",
							"\n",
							"# Use SQL to convert the parquet table to Delta\n",
							"spark.sql(\"CONVERT TO DELTA parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"outputs": [],
						"execution_count": 26
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Intermediate_Catalog')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3dba69b9-6d8a-4459-ad2d-3ba36dde2cc0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark32",
						"name": "lgnsynspark32",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import uuid\n",
							"import pyspark.sql.functions as F\n",
							"from pyspark.sql.functions import col\n",
							"from pyspark.sql.types import StringType,DateType,LongType,IntegerType,TimestampType"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Collect the raw catalog data from all three source systems"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## From Fourth Coffee"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"fc_movies = spark.read.csv(\"abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/mdwoh/raw/fourthcoffee/Movies.csv\", header='true', inferSchema='true')\n",
							"fc_actors = spark.read.csv(\"abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/mdwoh/raw/fourthcoffee/Actors.csv\", header='true', inferSchema='true')\n",
							"fc_movie_actors = spark.read.csv(\"abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/mdwoh/raw/fourthcoffee/MovieActors.csv\", header='true', inferSchema='true')\n",
							"fc_mappings = spark.read.csv(\"abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/mdwoh/raw/fourthcoffee/OnlineMovieMappings.csv\", header='true', inferSchema='true')\n",
							"\n",
							"# Use the MovieActors data to join the actor information. Left join to preserve movies where the actors are not found\n",
							"# Use the OnlineMovieMappings data to bring in the ids of movies which are also found in the Southridge Video catalog,\n",
							"# again using the left join to preserve movies which are not matched.\n",
							"\n",
							"fc_catalog = fc_movies \\\n",
							"  .join(fc_movie_actors, on='MovieId', how='left') \\\n",
							"  .join(fc_actors, on='ActorId', how='left') \\\n",
							"  .join(fc_mappings, on='MovieId', how='left')"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## From VanArsdel"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"va_movies = spark.read.parquet(\"abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/mdwoh/raw/vanarsdel/dboMovies.parquet\")\n",
							"va_actors = spark.read.parquet(\"abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/mdwoh/raw/vanarsdel/dboActors.parquet\")\n",
							"va_movie_actors = spark.read.parquet(\"abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/mdwoh/raw/vanarsdel/dboMovieActors.parquet\")\n",
							"va_mappings = spark.read.parquet(\"abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/mdwoh/raw/vanarsdel/dboOnlineMovieMappings.parquet\")\n",
							"\n",
							"# Use the MovieActors data to join the actor information. Left join to preserve movies where the actors are not found\n",
							"# Use the OnlineMovieMappings data to bring in the ids of movies which are also found in the Southridge Video catalog,\n",
							"# again using the left join to preserve movies which are not matched.\n",
							"\n",
							"va_catalog = va_movies \\\n",
							"  .join(va_movie_actors, on='MovieId', how='left') \\\n",
							"  .join(va_actors, on='ActorId', how='left') \\\n",
							"  .join(va_mappings, on='MovieId', how='left')"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"## From Southridge"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# After reading the JSON, explode the actors array to create multiple rows per movie, each having a single actor name\n",
							"# Drop the original actors array, as it is then no longer needed\n",
							"# Also drop the Cosmos DB metadata, as it will not be valuable here\n",
							"\n",
							"import pyspark.sql.functions as F\n",
							"\n",
							"sr_catalog = spark.read.json(\"abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/mdwoh/raw/southridge/catalog/movies.json\") \\\n",
							"  .withColumn('actor', F.explode('actors')) \\\n",
							"  .drop('actors', '_attachments', '_etag', '_rid', '_self', '_ts')\n",
							"\n",
							"display(sr_catalog)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Observe inconsistencies in the data types and formats\n",
							"\n",
							"Let's take a look at the following discrepencies which could cause fatal errors in downstream processing."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Release dates and availability dates\n",
							"\n",
							"FourthCoffee and VanArsdel, Ltd. both seem to track the ReleaseDate for each movie. These are stored as strings, and we'll need to look into the formats they've used.\n",
							"\n",
							"Southridge Video is storing a releaseYear, an availabilityDate, and a streamingAvailabilityDate."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"fc_dates = fc_catalog.select(col('ReleaseDate').alias('FcReleaseDate'), col('MovieId'), col('OnlineMovieId'))\n",
							"va_dates = va_catalog.select(col('ReleaseDate').alias('VaReleaseDate'), col('MovieId'), col('OnlineMovieId'))\n",
							"sr_dates = sr_catalog.select(*['id', 'releaseYear', 'availabilityDate', 'streamingAvailabilityDate'])\n",
							"\n",
							"joined_dates = sr_dates \\\n",
							"  .join(fc_dates, F.upper(sr_dates.id) == fc_dates.OnlineMovieId, how='left') \\\n",
							"  .join(va_dates, sr_dates.id == va_dates.OnlineMovieId, how='left')\n",
							"\n",
							"joined_dates \\\n",
							"  .filter('FcReleaseDate is not null AND VaReleaseDate is not null') \\\n",
							"  .limit(1) \\\n",
							"  .collect()"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Looking at the dates\n",
							"\n",
							"```text\n",
							"[Row(id='300e034e-4260-49ed-85e2-39a8d5030713',\n",
							"    releaseYear=2015,\n",
							"    availabilityDate='2017-07-25 00:00:00',\n",
							"    streamingAvailabilityDate='2017-09-19 00:00:00',\n",
							"    FcReleaseDate='07-25-2017',\n",
							"    OnlineMovieId='300E034E-4260-49ED-85E2-39A8D5030713',\n",
							"    VaReleaseDate='07-25-2017',\n",
							"    OnlineMovieId='300e034e-4260-49ed-85e2-39a8d5030713')]\n",
							"```\n",
							"\n",
							"Above, we see that:  \n",
							"  - Southridge has recorded a 2015 releaseYear for this movie\n",
							"  - Southridge has recorded that the movie is available as of 25 Jul 2017\n",
							"  - Southridge has recorded that the movie is available for streaming as of 19 Sep 2017\n",
							"  - Fourth Coffee has recorded that the movie has a ReleaseDate of 25 Jul 2017\n",
							"  - VanArsdel, Ltd. has similarly recorded the ReleaseDate as 25 Jul 2017\n",
							"\n",
							"It looks like Fourth Coffee is tracking the \"release\" in terms of when it became available to rent. Only Southridge Video has tracked the original theatrical release year. This detail was useful for their web-based front-end, but the brick and mortar stores previously had no business need for it.\n",
							"\n",
							"Each of the source system has stored dates in different formats, but we will use true date types in our conformed intermediate schema."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"> Note: We are not only parsing the dates here, but also adding a column to track the SourceSystemId"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"fc_catalog = fc_catalog \\\n",
							"  .withColumn('ReleaseDate', F.to_date(col('ReleaseDate'), 'MM-dd-yyyy')) \\\n",
							"  .withColumn('SourceSystemId', F.lit('FC'))\n",
							"\n",
							"va_catalog = va_catalog \\\n",
							"  .withColumn('ReleaseDate', F.to_date(col('ReleaseDate'), 'MM-dd-yyyy')) \\\n",
							"  .withColumn('SourceSystemId', F.lit('VA'))\n",
							"\n",
							"sr_catalog = sr_catalog \\\n",
							"  .withColumn('availabilityDate', F.to_date(col('availabilityDate'), 'yyyy-MM-dd HH:mm:ss')) \\\n",
							"  .withColumn('streamingAvailabilityDate', F.to_date(col('streamingAvailabilityDate'), 'yyyy-MM-dd HH:mm:ss')) \\\n",
							"  .withColumn('SourceSystemId', F.lit('SR'))"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Unioning the data\n",
							"\n",
							"### Target schema\n",
							"\n",
							"Looking ahead, we will keep every source record from every catalog, so we don't need to join here.\n",
							"However, we will need to map columns to a consistent schema.\n",
							"\n",
							"```\n",
							"SourceSystemId:              Use the SourceSystemId we added in the previous cell.\n",
							"CatalogId:                   Use a new unique identifier.\n",
							"SourceSystemMovieId:         From Southridge, use the source id. From the others, use the source MovieId.\n",
							"SouthridgeMovieId:           From Southridge, use the source id. From the others, use the source OnlineMovieId.\n",
							"ActorID:                     From Southridge, this is null. From the others, it's the source ActorId.\n",
							"ActorName:                   From Southridge, it's the exploded actor name. From the others, it is the ActorName.\n",
							"ActorGender:                 Southridge does not track this data. The on premises stores have Gender.\n",
							"Title:                       From Southridge, use title. From others, MovieTitle.\n",
							"Genre:                       From Southridge, use genre. From others, Category.\n",
							"Rating:                      Southridge has rating and the others have Rating.\n",
							"RuntimeMinutes:              Southridge has runtime, the others have RunTimeMin.\n",
							"TheatricalReleaseYear:       Southridge has releaseYear. The others don't have this data.\n",
							"PhysicalAvailabilityDate:    Southridge has availabilityDate. The others have ReleaseDate.\n",
							"StreamingAvailabilityDate:   Southridge has streamingAvailabilityDate. The others have no such data, as it does not apply to physical rentals.\n",
							"```\n",
							"\n",
							"### To join, cleanse, drop duplicates, etc. ... or not?\n",
							"\n",
							"At this stage, we want to focus on the **fatal** anomalies that would cause exceptions in downstream processing;\n",
							"e.g., inconsistent data types or formats.\n",
							"If we were loading this data directly into a final reporting schema, we would likely apply additional cleansing such as:\n",
							"\n",
							"- Look for and eliminate typos, e.g., PGg instead of PG\n",
							"- Normalize capitalization of titles, names, ratings, etc.\n",
							"- Look for and resolve conflicts in matched movies, e.g., Southridge thinks Mysterious Cube is a G-rated family movie while VanArsdel, Ltd. had it as a PG-13 rated Comedy\n",
							"- Look for variations in actor names and choose one to use consistently throughout the reporting schema, e.g., Vivica A. Fox vs Vivica Fox\n",
							"- Drop duplicates\n",
							"- etc., etc., etc.\n",
							"\n",
							"However, if we perform these operations now, then we may eliminate the opportunity to discover previously unrecognized value in the data.\n",
							"As a contrived example, consider a possibility that some actors and actresses would occassionally use their middle initial, but sometimes would not.\n",
							"Now, imagine that data scientists uncover a trend where films are more marketable when the cast does use their middle initial versus when they do not.\n",
							"Or maybe that only holds true in the Drama genre, but it does not hold in Family movies.\n",
							"If we have already chosen the person's \"usual\" billing and only kept that version in our conformed dataset,\n",
							"the the data scientists would never be able to see this."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# This is used in the following cells to create a new unique identifier\n",
							"\n",
							"uuidUdf = F.udf(lambda : str(uuid.uuid4()), StringType())"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"sr_conformed = sr_catalog \\\n",
							"  .select([ \\\n",
							"    col(\"SourceSystemId\"), \\\n",
							"    col(\"id\").alias(\"SourceSystemMovieId\"), \\\n",
							"    col(\"id\").alias(\"SouthridgeMovieId\"), \\\n",
							"    col(\"availabilityDate\").cast(TimestampType()).alias(\"PhysicalAvailabilityDate\"), \\\n",
							"    col(\"streamingAvailabilityDate\").cast(TimestampType()).alias(\"StreamingAvailabilityDate\"), \\\n",
							"    col(\"genre\").alias(\"Genre\"), \\\n",
							"    col(\"title\").alias(\"Title\"), \\\n",
							"    col(\"rating\").alias(\"Rating\"), \\\n",
							"    col(\"runtime\").alias(\"RuntimeMinutes\"), \\\n",
							"    col(\"releaseYear\").alias(\"TheatricalReleaseYear\"), \\\n",
							"    sr_catalog[\"actor.name\"].alias(\"ActorName\")]) \\\n",
							"  .withColumn(\"ActorId\", F.lit(None).cast(StringType())) \\\n",
							"  .withColumn(\"ActorGender\", F.lit(None).cast(StringType())) \\\n",
							"  .withColumn(\"CatalogId\", uuidUdf())"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# VanArsdel and Fourth Coffee are extremely similar\n",
							"\n",
							"va_conformed = va_catalog \\\n",
							"  .select([ \\\n",
							"    col(\"SourceSystemId\"), \\\n",
							"    col(\"MovieID\").alias(\"SourceSystemMovieId\"), \\\n",
							"    col(\"OnlineMovieID\").alias(\"SouthridgeMovieId\"), \\\n",
							"    col(\"ReleaseDate\").cast(TimestampType()).alias(\"PhysicalAvailabilityDate\"), \\\n",
							"    F.lit(None).cast(TimestampType()).alias(\"StreamingAvailabilityDate\"), \\\n",
							"    col(\"Category\").alias(\"Genre\"), \\\n",
							"    col(\"MovieTitle\").alias(\"Title\"), \\\n",
							"    col(\"Rating\").alias(\"Rating\"), \\\n",
							"    col(\"RunTimeMin\").cast(LongType()).alias(\"RuntimeMinutes\"), \\\n",
							"    F.lit(None).cast(LongType()).alias(\"TheatricalReleaseYear\"), \\\n",
							"    col(\"ActorName\"), \\\n",
							"    col(\"MovieActorID\").alias(\"ActorID\"), \\\n",
							"    col(\"Gender\").alias(\"ActorGender\")]) \\\n",
							"  .withColumn(\"CatalogId\", uuidUdf())\n",
							"\n",
							"fc_conformed = fc_catalog \\\n",
							"  .select([ \\\n",
							"    col(\"SourceSystemId\"), \\\n",
							"    col(\"MovieID\").alias(\"SourceSystemMovieId\"), \\\n",
							"    col(\"OnlineMovieID\").alias(\"SouthridgeMovieId\"), \\\n",
							"    col(\"ReleaseDate\").cast(TimestampType()).alias(\"PhysicalAvailabilityDate\"), \\\n",
							"    F.lit(None).cast(TimestampType()).alias(\"StreamingAvailabilityDate\"), \\\n",
							"    col(\"Category\").alias(\"Genre\"), \\\n",
							"    col(\"MovieTitle\").alias(\"Title\"), \\\n",
							"    col(\"Rating\").alias(\"Rating\"), \\\n",
							"    col(\"RunTimeMin\").cast(LongType()).alias(\"RuntimeMinutes\"), \\\n",
							"    F.lit(None).cast(LongType()).alias(\"TheatricalReleaseYear\"), \\\n",
							"    col(\"ActorName\"), \\\n",
							"    col(\"MovieActorID\").alias(\"ActorID\"), \\\n",
							"    col(\"Gender\").alias(\"ActorGender\")]) \\\n",
							"  .withColumn(\"CatalogId\", uuidUdf())"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"# The full catalog is now a straightforward union\n",
							"\n",
							"full_catalog = sr_conformed.union(va_conformed).union(fc_conformed)\n",
							"full_catalog.write.mode(\"overwrite\").parquet(\"abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/mdwoh/conformed/catalog\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"# Let's rehydrate and confirm that everything checks out\n",
							"\n",
							"rehydrated_catalog = spark.read.parquet(\"abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/mdwoh/conformed/catalog\")\n",
							"\n",
							"sr_rehydrated = rehydrated_catalog.filter(\"SourceSystemID=='SR'\")\n",
							"\n",
							"sr_actors_per_movie = sr_rehydrated \\\n",
							"  .groupby(col('SourceSystemMovieId')) \\\n",
							"  .agg(F.count(F.lit(1)).alias('ActorCount'))\n",
							"\n",
							"print('The number of rows from Southridge is ', sr_rehydrated.count())\n",
							"print('The number of distinct movies from Southridge is', sr_actors_per_movie.count())\n",
							"print(sr_actors_per_movie.limit(1).collect())"
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Machine Learning')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "c48e6d44-a058-4b48-a593-c8cc8f637ba0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark31",
						"name": "lgnsynspark31",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Train a classifier to determine product seasonality\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"See installed packages\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import pkg_resources\n",
							"for d in pkg_resources.working_set:\n",
							"     print(d)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Import all necessary libraries.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
							"from sklearn.decomposition import PCA\n",
							"from sklearn.model_selection import train_test_split\n",
							"from sklearn.metrics import accuracy_score\n",
							"\n",
							"from xgboost import XGBClassifier\n",
							"\n",
							"from onnxmltools.convert import convert_xgboost\n",
							"from onnxmltools.convert.common.data_types import FloatTensorType\n",
							"\n",
							"import numpy as np\n",
							"import pandas as pd\n",
							"import matplotlib.pyplot as plt"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Exploratory data analysis (basic stats)\n",
							"\n",
							"Create Spark temporary views for sales and products.\n",
							"\n",
							"**IMPORTANT!** Make sure the name of the SQL pool (`SQLPool01` below) matches the name of your SQL pool.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"Seasonality"
									],
									"values": [
										"ProductId"
									],
									"yLabel": "ProductId",
									"xLabel": "Seasonality",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"ProductId\":{\"1\":771373,\"2\":247930,\"3\":262978}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							}
						},
						"source": [
							"%%spark\n",
							"val df = spark.read.sqlanalytics(\"#SQL_POOL_NAME#.wwi.SaleSmall\") \n",
							"df.createOrReplaceTempView(\"sale\")\n",
							"\n",
							"val df2 = spark.read.sqlanalytics(\"#SQL_POOL_NAME#.wwi.Product\") \n",
							"df2.createOrReplaceTempView(\"product\")\n",
							"display(df2)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Load daily product sales from the SQL pool.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"sqlQuery = \"\"\"\n",
							"SELECT\n",
							"    P.ProductId\n",
							"    ,P.Seasonality\n",
							"    ,S.TransactionDateId\n",
							"    ,COUNT(*) as TransactionItemsCount\n",
							"FROM\n",
							"    sale S\n",
							"    JOIN product P ON\n",
							"        S.ProductId = P.ProductId\n",
							"WHERE\n",
							"    S.TransactionDateId NOT IN (20120229, 20160229)\n",
							"GROUP BY\n",
							"    P.ProductId\n",
							"    ,P.Seasonality\n",
							"    ,S.TransactionDateId\n",
							"\"\"\"\n",
							"\n",
							"prod_df = spark.sql(sqlQuery)\n",
							"prod_df.cache()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Check the number of records in the data farame (should be around 13 million rows)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"prod_df.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Display some statistics about the data frame.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"summary"
									],
									"values": [
										"summary"
									],
									"yLabel": "summary",
									"xLabel": "summary",
									"aggregation": "COUNT",
									"aggByBackend": false
								},
								"aggData": "{\"summary\":{\"count\":1,\"max\":1,\"mean\":1,\"min\":1,\"stddev\":1}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							}
						},
						"source": [
							"display(prod_df.describe())"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Pivot the data frame to make daily sale items counts columns. \n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"prod_prep_df = prod_df.groupBy(['ProductId', 'Seasonality']).pivot('TransactionDateId').sum('TransactionItemsCount').toPandas()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Clean up the nulls and take a look at the result.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"prod_prep_df = prod_prep_df.fillna(0)\n",
							"prod_prep_df.head(10)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Isloate features and prediction classes.\n",
							"\n",
							"Standardize features by removing the mean and scaling to unit variance.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"X = prod_prep_df.iloc[:, 2:].values\n",
							"y = prod_prep_df['Seasonality'].values\n",
							"\n",
							"X_scale = StandardScaler().fit_transform(X)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Use PCA for dimensionality reduction\n",
							"\n",
							"Perform dimensionality reduction using Principal Components Analysis and two target components.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"pca = PCA(n_components=2)\n",
							"principal_components = pca.fit_transform(X_scale)\n",
							"principal_components = MinMaxScaler().fit_transform(principal_components)\n",
							"\n",
							"pca_df = pd.DataFrame(data = principal_components, columns = ['pc1', 'pc2'])\n",
							"pca_df = pd.concat([pca_df, prod_prep_df[['Seasonality']]], axis = 1)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Display the products data frame in two dimensions (mapped to the two principal components).\n",
							"\n",
							"Note the clear separation of clusters.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"fig = plt.figure(figsize = (6,6))\n",
							"ax = fig.add_subplot(1,1,1) \n",
							"ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
							"ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
							"ax.set_title('2 component PCA', fontsize = 20)\n",
							"targets = [1, 2, 3]\n",
							"colors = ['r', 'g', 'b']\n",
							"for target, color in zip(targets,colors):\n",
							"    indicesToKeep = pca_df['Seasonality'] == target\n",
							"    ax.scatter(pca_df.loc[indicesToKeep, 'pc1']\n",
							"               , pca_df.loc[indicesToKeep, 'pc2']\n",
							"               , c = color\n",
							"               , s = 1)\n",
							"ax.legend(['All Season Products', 'Summer Products', 'Winter Products'])\n",
							"ax.plot([-0.05, 1.05], [0.77, 1.0], linestyle=':', linewidth=1, color='y')\n",
							"ax.plot([-0.05, 1.05], [0.37, 0.6], linestyle=':', linewidth=1, color='y')\n",
							"ax.grid()\n",
							"\n",
							"plt.show()\n",
							"plt.close()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Redo the Principal Components Analysis, this time with twenty dimensions.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"def col_name(x):\n",
							"    return f'f{x:02}'\n",
							"\n",
							"pca = PCA(n_components=20)\n",
							"principal_components = pca.fit_transform(X_scale)\n",
							"principal_components = MinMaxScaler().fit_transform(principal_components)\n",
							"\n",
							"X = pd.DataFrame(data = principal_components, columns = list(map(col_name, np.arange(0, 20))))\n",
							"pca_df = pd.concat([X, prod_prep_df[['ProductId']]], axis = 1)\n",
							"pca_automl_df = pd.concat([X, prod_prep_df[['Seasonality']]], axis = 1)\n",
							"\n",
							"X = X[:4500]\n",
							"y = prod_prep_df['Seasonality'][:4500]\n",
							"pca_automl_df = pca_automl_df[:4500]"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Save the PCA components to the SQL pool. - the `spark.sql.execution.arrow.fallback.enabled` warning can be safely ignored.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"pca_sdf = spark.createDataFrame(pca_df)\n",
							"pca_sdf.createOrReplaceTempView(\"productpca\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"// Make sure the name of the SQL pool (#SQL_POOL_NAME# below) matches the name of your SQL pool.\n",
							"val df = spark.sqlContext.sql(\"select * from productpca\")\n",
							"df.write.sqlanalytics(\"#SQL_POOL_NAME#.wwi_ml.ProductPCA\", Constants.INTERNAL)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Train ensemble of trees classifier (using XGBoost)\n",
							"\n",
							"Split into test and training data sets.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Train the ensemble classifier using XGBoost.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"model = XGBClassifier()\n",
							"model.fit(X_train, y_train)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Perform predictions with the newly trained model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"y_pred = model.predict(X_test)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Calculate the accuracy of the model using test data.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"accuracy = accuracy_score(y_test, y_pred)\n",
							"print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Convert trained model to ONNX format.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"initial_types = [\n",
							"    ('input', FloatTensorType([1, 20]))\n",
							"]\n",
							"\n",
							"onnx_model = convert_xgboost(model, initial_types=initial_types)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Train classifier using Auto ML\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.core.experiment import Experiment\n",
							"from azureml.core.workspace import Workspace\n",
							"from azureml.train.automl.run import AutoMLRun\n",
							"from azureml.train.automl import AutoMLConfig\n",
							"from azureml.automl.runtime.onnx_convert import OnnxConverter"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"pca_automl_df.head(10)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Configure the connection to the Azure Machine Learning workspace. The Azure portal provides all the values below.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"subscription_id='#SUBSCRIPTION_ID#'         # ensure it matches your Azure subscription id\n",
							"resource_group='#RESOURCE_GROUP_NAME#'      # ensure it matches your resource group name\n",
							"workspace_name='#AML_WORKSPACE_NAME#'       # ensure it matches your Azure Machine Learning workspace name\n",
							"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
							"ws.write_config()\n",
							"ws = Workspace.from_config()\n",
							"experiment = Experiment(ws, \"Product_Seasonality\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Configure the Automated Machine Learning experiment and start it (will run on local compute resources).\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"automl_classifier_config = AutoMLConfig(\n",
							"        task='classification',\n",
							"        #experiment_exit_score = 0.995,\n",
							"        experiment_timeout_minutes=15,\n",
							"        enable_onnx_compatible_models=True,\n",
							"        training_data=pca_automl_df,\n",
							"        label_column_name='Seasonality',\n",
							"        n_cross_validations=5,\n",
							"        enable_voting_ensemble=False,\n",
							"        enable_stack_ensemble=False\n",
							"        )\n",
							"\n",
							"local_run = experiment.submit(automl_classifier_config, show_output=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Retrieve the best model directly in ONNX format and take a look at it.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"best_run, onnx_model2 = local_run.get_output(return_onnx_model=True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"outputCollapsed": true
						},
						"source": [
							"onnx_model2"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Replace below the placeholders with the name of the primary data lake account and one of it's security keys."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"modelPath = \"abfss://wwi-02@#DATA_LAKE_ACCOUNT_NAME#.dfs.core.windows.net/ml/onnx/product_seasonality_classifier.onnx\"\n",
							"modelString = str(onnx_model2.SerializeToString())\n",
							"mssparkutils.fs.put(modelPath, modelString)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Model Training')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7d3480ec-90a9-4916-9082-00a2c258f571"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Model training and registration\n",
							"This notebook show the process for training the model, converting the model to ONNX and uploading the ONNX model to Azure Storage."
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Explore the training data\n",
							"The following cells load the source CSV file into a Spark DataFrame and create a temporary view that can be used to query the data with Spark SQL.\n",
							"\n",
							"WWI has provided a small CSV file you can use for showing the process of training a simple model.\n",
							"\n",
							"They have already loaded for you in the data lake. \n",
							"It is located under the `wwi-02` container with the path `/sale-csv/wwi-factsale.csv`.\n",
							"You need to build the correct path to the file and the run the cells that follow to load and query the data.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.load('abfss://wwi-02@asadatalakeSUFFIX.dfs.core.windows.net/sale-csv/sale-csv/wwi-factsale.csv', format=\"csv\"\n",
							", header=True, sep=\"|\"\n",
							")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Next, WWI would like you to show them how create a temporary view over the loaded dataframe.\n",
							"\n",
							"The view should be named `facts`.\n",
							"\n",
							"Complete the code in the cell and run it.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df.createOrReplaceTempView(\"facts\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"In the next cell, WWI would like you to explore the data with an initial query.\n",
							"\n",
							"You want to preview all of the sales having the `Customer Key` of `11`.\n",
							"\n",
							"You should order the results by `Stock Item Key`.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"display(spark.sql(\"SELECT * FROM facts WHERE `Customer Key` == '11' ORDER BY `Stock Item Key`\"))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Predict Quantity given Customer Key and Stock Item Key\n",
							"In the following cells we load a subset of the data that just contains the fields needed for training. \n",
							"\n",
							"WWI's data scientists have already provided some of the code for you. \n",
							"\n",
							"Read thru and run the following cells.\n",
							"\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import col\n",
							"df3 = spark.sql(\"SELECT double(`Customer Key`) as customerkey, double(`Stock Item Key`) as stockitemkey, double(`Quantity`) as quantity FROM facts\").where(col(\"quantity\").isNotNull())\n",
							"df3.cache()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Next, we package the data into the format expected by Spark ML's LinearRegression. It requires a DataFrame with two columns- `features` and a column with the labels to predict (`quantity` in this case).\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.ml.feature import VectorAssembler\n",
							"\n",
							"vectorAssembler = VectorAssembler(inputCols = ['customerkey', 'stockitemkey'], outputCol = 'features')\n",
							"df4 = vectorAssembler.transform(df3)\n",
							"df5 = df4.select(['features', 'quantity'])\n",
							"df5.show(10)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Now, we split our DataFrame into training and testing DataFrames.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"A best practice is to split data into training and test sets.\n",
							"\n",
							"WWI would like you to complete the final line that produces the train and test dataframes. \n",
							"\n",
							"Once you have completed the cell, run it.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"trainingFraction = 0.7\n",
							"testingFraction = (1-trainingFraction)\n",
							"seed = 42\n",
							"\n",
							"# Split the dataframe into test and training dataframes\n",
							"df_train, df_test = df5.randomSplit([trainingFraction, testingFraction], seed=seed)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"In the following cell, you will train your LinearRegression model.\n",
							"\n",
							"The goal of this regressor is to predict the `quantity` field given all of the features. \n",
							"\n",
							"Complete the missing parameters and the last line to train the model.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.ml.regression import LinearRegression\n",
							"\n",
							"lin_reg = LinearRegression(featuresCol = 'features', labelCol='quantity', maxIter = 10, regParam=0.3)\n",
							"lin_reg_model = lin_reg.fit(df_train)\n",
							"\n",
							"print(\"Coefficients: \" + str(lin_reg_model.coefficients))\n",
							"print(\"Intercept: \" + str(lin_reg_model.intercept))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Now that you have a trained model in hand, WWI wants to verify you can use it to make predictions against the test DataFrame.\n",
							"\n",
							"Complete the first line to use your trained model to make predictions against the `df_test` dataframe.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"df_pred = lin_reg_model.transform(df_test)\n",
							"display(df_pred)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert model to ONNX\n",
							"In the cells that follow, WWI wants you to show how you convert the model to ONNX and show how an output of how ONNX represents the Spark ML model.\n",
							"\n",
							"They have already provided you the code, you just need to run the cells.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from onnxmltools import convert_sparkml\n",
							"from onnxmltools.convert.common.data_types import FloatTensorType\n",
							"\n",
							"initial_types = [ \n",
							"    (\"features\", FloatTensorType([1, lin_reg_model.numFeatures])),\n",
							"    # (repeat for the required inputs)\n",
							"]"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"model_onnx = convert_sparkml(lin_reg_model, 'sparkml GeneralizedLinearRegression', initial_types)\n",
							"model_onnx"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Upload the model to Azure Storage\n",
							"\n",
							"In order for an ONNX model to be used by the T-SQL predict statement, it must be uploaded to Azure Storage.\n",
							"\n",
							"WWI wants you to show them how they would serialize the model to disk and then upload the model file to Azure Storage.\n",
							"\n",
							"Run the following cell to save  the ONNX model to the storage of the Spark driver node temporarily. "
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"with open(\"model.onnx\", \"wb\") as f:\n",
							"    f.write(model_onnx.SerializeToString())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Next, you need to show WWI how to use the Azure Storage Python SDK to upload the ONNX model to Azure Storage.\n",
							"\n",
							"Complete the connection string with the correct values for your non-hierarchical Storage Account.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from azure.storage.blob import BlockBlobService\n",
							" \n",
							"block_blob_service = BlockBlobService(\n",
							" account_name='asadatalakeSUFFIX', account_key='<data_lake_account_key>') \n",
							" \n",
							"block_blob_service.create_blob_from_text('wwi-02', '/ml/onnx/model.onnx', model_onnx.SerializeToString())"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Obtain credentials with TokenLibrary')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PySpark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "233f006a-21ef-4170-b4dc-766b9cfafbba"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"For latest documentation please run TokenLibrary.help() from Notebook\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"# By default Synapse uses AAD passthrough for authentication\n",
							"# However, Linked services can be used for storing and retreiving credentials (e.g, account key)\n",
							"# Example connection string (for storage): \"DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>\"\n",
							"\n",
							"connection_string = TokenLibrary.getConnectionString(\"<linkedServiceName>\")\n",
							"account_key = TokenLibrary.getConnectionStringAsMap(\"<linkedServiceName>\").get(\"AccountKey\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ParquetMetadata')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8da255aa-d14b-42bd-8ecb-66c9561353bb"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark32",
						"name": "lgnsynspark32",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pyarrow.parquet as pq"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"parquet_file = spark.read.load('abfss://lgnsynapsefs@lgnsynapselake.dfs.core.windows.net/mdwoh/raw/southridge/cloudsales/dboAddresses.parquet', format='parquet')\r\n",
							"display(parquet_file.limit(10))"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"parquet_file.printSchema()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"parquet_file.dtypes"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Read KeyVault Secret')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "221f302b-e7ed-409f-b07f-8dca50f27a48"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark31",
						"name": "lgnsynspark31",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"synapseusername = TokenLibrary.getSecret('lnakv', 'lnewportdbuser')\r\n",
							"synapsepassword = TokenLibrary.getSecret('lnakv', 'lnewportdbpassword')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							" %%pyspark\r\n",
							"jdbcDF = spark.read.format(\"jdbc\") \\\r\n",
							"     .option(\"url\", f\"jdbc:sqlserver://lnewport.database.windows.net:1433;database=AdventureWorks\") \\\r\n",
							"     .option(\"dbtable\",\"[Auxiliary].[Calendar]\") \\\r\n",
							"     .option(\"user\", TokenLibrary.getSecret('lnakv', 'lnewportdbuser')) \\\r\n",
							"     .option(\"password\",TokenLibrary.getSecret('lnakv', 'lnewportdbpassword')) \\\r\n",
							"     .option(\"driver\",\"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\r\n",
							"     .load()\r\n",
							"jdbcDF.show()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import sys\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"sc = SparkSession.builder.getOrCreate()\r\n",
							"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
							"\r\n",
							"connection_string = token_library.getSecret(\"lnakv\", \"lnewportdbuser\", \"lnakv\")\r\n",
							"print(connection_string)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.notebook.help()"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# By default Synapse uses AAD passthrough for authentication\r\n",
							"# However, Linked services can be used for storing and retreiving credentials (e.g, account key)\r\n",
							"# Example connection string (for storage): \"DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>\"\r\n",
							"\r\n",
							"connection_string = TokenLibrary.getConnectionString(\"<linkedServiceName>\")\r\n",
							"account_key = TokenLibrary.getConnectionStringAsMap(\"<linkedServiceName>\").get(\"AccountKey\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ReadDeltafromSQLIngest')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "b9e532dd-8e05-4f50-9c1f-5ce5ac8de31c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark31",
						"name": "lgnsynspark31",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.format('delta').load('abfss://raw@lgndatalake.dfs.core.windows.net/bronze/AdventureWorksLT2017/SalesLT.Product')\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ReadEHTolls')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "ef4ab206-2360-470f-9082-178b0f22bf9b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark32",
						"name": "lgnsynspark32",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://tolls@lgnsynapselake.dfs.core.windows.net/*/*/*.parquet', format='parquet')\r\n",
							"display(df.limit(10))\r\n",
							"df.count()\r\n",
							"df.printSchema()"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://container@adlsname.dfs.core.windows.net/*/*/*.parquet', format='parquet')\r\n",
							"display(df.limit(10))\r\n",
							"df.count()\r\n",
							"df.printSchema()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RegisterDeltaLakeTables')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "1565f601-ee22-4792-8b2a-7ec0fa478221"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark31",
						"name": "lgnsynspark31",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"CREATE DATABASE IF NOT EXISTS bronze2 LOCATION \"/delta/bronze/\""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"USE bronze;\r\n",
							"CREATE TABLE IF NOT EXISTS Address USING DELTA LOCATION \"/delta/bronze/Address/\";\r\n",
							"CREATE TABLE IF NOT EXISTS Customer USING DELTA LOCATION \"/delta/bronze/Customer/\";\r\n",
							"CREATE TABLE IF NOT EXISTS CustomerAddress USING DELTA LOCATION \"/delta/bronze/CustomerAddress/\";\r\n",
							"CREATE TABLE IF NOT EXISTS Product USING DELTA LOCATION \"/delta/bronze/Product/\";\r\n",
							"CREATE TABLE IF NOT EXISTS ProductCategory USING DELTA LOCATION \"/delta/bronze/ProductCategory/\";\r\n",
							"CREATE TABLE IF NOT EXISTS ProductDescription USING DELTA LOCATION \"/delta/bronze/ProductDescription/\";\r\n",
							"CREATE TABLE IF NOT EXISTS ProductModel USING DELTA LOCATION \"/delta/bronze/ProductModel/\";\r\n",
							"CREATE TABLE IF NOT EXISTS SalesOrderDetail USING DELTA LOCATION \"/delta/bronze/SalesOrderDetail/\";\r\n",
							"CREATE TABLE IF NOT EXISTS SalesOrderHeader USING DELTA LOCATION \"/delta/bronze/SalesOrderHeader/\";"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"select * from bronze.salesorderdetail"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"SELECT 'SalesOrderDetail' as TableName, count(*) as RowCnt from bronze.salesorderdetail\r\n",
							"UNION ALL\r\n",
							"Select 'Product' as TableName, count(*) as RowCnt from bronze.product"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"drop DATABASE bronze"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Retail - Product recommendations')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Retail Recommendation"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "91cdb5cd-6800-4628-9f07-658b9188c31b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark31",
						"name": "lgnsynspark31",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Retail Recommendation Accelerator Quickstart: Model Training\r\n",
							"\r\n",
							"\r\n",
							"This notebook uses sample data to train a LightGBM model for retail product recommendation. The data is randomly generated."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import logging\r\n",
							"logging.getLogger(\"py4j\").setLevel(logging.ERROR)\r\n",
							"\r\n",
							"import pandas as pd\r\n",
							"import seaborn as sns\r\n",
							"from matplotlib import pyplot as plt\r\n",
							"\r\n",
							"from pyspark.version import __version__ as pyspark_version\r\n",
							"\r\n",
							"import mmlspark\r\n",
							"from mmlspark.train import ComputeModelStatistics\r\n",
							"from mmlspark.lightgbm import LightGBMClassifier\r\n",
							"from pyspark.ml.feature import VectorAssembler\r\n",
							"\r\n",
							"pd.set_option('display.max_columns', 50)\r\n",
							"\r\n",
							"print(f\"PySpark version: {pyspark_version}\")\r\n",
							"print(f\"MMLSpark version: {mmlspark.core.__spark_package_version__}\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Parameters\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"Note: If you're using a Managed VNet enabled workspace, please download the dataset from the \n",
							"[url](https://synapsemlpublic.blob.core.windows.net/files/PersonalizedData.csv) and then upload it to your own storage account in order to access it."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# Blob url\r\n",
							"# Original blob: \"https://recodatasets.z20.web.core.windows.net/random-dataset/PersonalizedData.csv\"\r\n",
							"url = \"wasbs://files@synapsemlpublic.blob.core.windows.net/PersonalizedData.csv\"\r\n",
							"\r\n",
							"# Data parameters\r\n",
							"LABEL_COL = \"Rating\"\r\n",
							"FEATURE_COL = \"features\"\r\n",
							"RATIO = 0.8\r\n",
							"SEED = 42\r\n",
							"\r\n",
							"# Model parameters\r\n",
							"OBJECTIVE = \"binary\"\r\n",
							"BOOSTING = \"gbdt\"\r\n",
							"NUM_LEAVES = 32\r\n",
							"NUM_ITERATIONS = 100\r\n",
							"LEARNING_RATE = 0.1\r\n",
							"FEATURE_FRACTION = 0.8\r\n",
							"EARLY_STOPPING_ROUND = 10\r\n",
							"MODEL_NAME = \"lgb-quickstart\"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Read the data from Blob"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Added the file to linked ADLSv2\r\n",
							"raw_data = spark.read.csv(url, header=True, inferSchema=True)\r\n",
							"print(\"Schema: \")\r\n",
							"raw_data.printSchema()\r\n",
							"\r\n",
							"df = raw_data.toPandas()\r\n",
							"print(\"Shape: \", df.shape)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"display(df.iloc[:10, :])"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Data visualization"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"df.describe()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"# calculate the correlation matrix\r\n",
							"corr = df.corr()\r\n",
							"\r\n",
							"# plot the correlation heatmap\r\n",
							"fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\r\n",
							"\r\n",
							"sns.heatmap(corr, \r\n",
							"            xticklabels=corr.columns, \r\n",
							"            yticklabels=corr.columns, \r\n",
							"            cmap='RdBu', \r\n",
							"            vmin=-1, \r\n",
							"            vmax=1, \r\n",
							"            ax=ax, \r\n",
							"            annot=True,\r\n",
							"            fmt='.2f', \r\n",
							"            annot_kws={'size': 10})\r\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"#scatterplot\r\n",
							"sns.set()\r\n",
							"sns.pairplot(df, height=2.5)\r\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Split the data into train, test\r\n",
							"\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"raw_train, raw_test = raw_data.randomSplit([RATIO, 1 - RATIO], seed=SEED)\n",
							"print(\"Train: (rows, columns) = {}\".format((raw_train.count(), len(raw_train.columns))))\n",
							"print(\"Test: (rows, columns) = {}\".format((raw_test.count(), len(raw_test.columns))))"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Feature engineering \n",
							"Transform the original data feature columns into feature vectors"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"columns = raw_data.columns[3:]\n",
							"featurizer = VectorAssembler(inputCols=columns, outputCol=FEATURE_COL)\n",
							"train = featurizer.transform(raw_train)[LABEL_COL, FEATURE_COL]\n",
							"test = featurizer.transform(raw_test)[LABEL_COL, FEATURE_COL]"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Check if data is unbalanced\n",
							"display(train.groupBy(LABEL_COL).count())\n",
							""
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Model Training\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"lgbm = LightGBMClassifier(\n",
							"    labelCol=LABEL_COL,\n",
							"    featuresCol=FEATURE_COL,\n",
							"    objective=OBJECTIVE,\n",
							"    isUnbalance=False,\n",
							"    boostingType=BOOSTING,\n",
							"    boostFromAverage=True,\n",
							"    baggingSeed=SEED,\n",
							"    numLeaves=NUM_LEAVES,\n",
							"    numIterations=NUM_ITERATIONS,\n",
							"    learningRate=LEARNING_RATE,\n",
							"    featureFraction=FEATURE_FRACTION,\n",
							"    earlyStoppingRound=EARLY_STOPPING_ROUND\n",
							")\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"model = lgbm.fit(train)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Feature Importances"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"feature_importances = model.getFeatureImportances()\n",
							"fi = pd.Series(feature_importances,index = columns)\n",
							"fi = fi.sort_values(ascending = True)\n",
							"f_index = fi.index\n",
							"f_values = fi.values\n",
							" \n",
							"# print feature importances \n",
							"print ('f_index:',f_index)\n",
							"print ('f_values:',f_values)\n",
							"\n",
							"# plot\n",
							"x_index = list(range(len(fi)))\n",
							"x_index = [x/len(fi) for x in x_index]\n",
							"plt.rcParams['figure.figsize'] = (10,10)\n",
							"plt.barh(x_index,f_values,height = 0.028 ,align=\"center\",color = 'tan',tick_label=f_index)\n",
							"plt.xlabel('importances')\n",
							"plt.ylabel('features')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Model Prediction"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"predictions = model.transform(test)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"display(predictions.limit(10))"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Evaluation"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"evaluator = (\n",
							"    ComputeModelStatistics()\n",
							"    .setScoredLabelsCol(\"prediction\")\n",
							"    .setLabelCol(LABEL_COL)\n",
							"    .setEvaluationMetric(\"classification\")\n",
							")\n",
							"\n",
							"metrics = evaluator.transform(predictions)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(metrics)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Save the model\r\n",
							"\r\n",
							"Save the model to linked ADLS"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"print(MODEL_NAME)\r\n",
							"(model\r\n",
							" .write()\r\n",
							" .overwrite()\r\n",
							" .save(MODEL_NAME))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 20
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL-OD-Code-Gen-Example')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "3f90158e-e310-44af-a5d6-b8f6f571e039"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "Synapse Spark"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark31",
						"name": "lgnsynspark31",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## Spark Hive Table CodeGen to SQL OD\n",
							"- This will import two datasets from Azure Open Datasets. Taxi data and US Population by County\n",
							"- A Hive table will be created for each of the imported datasets.  Most customers overlay a Hive table on their data lake assets\n",
							"    - The taxi dataset will be partitioned by Year and Month so we can see how this is code generated for partitions\n",
							"    - The population dataset will not be partitioned\n",
							"    - The Hive table must also be created as \"STORED AS PARQUET\"\n",
							"- From the Hive table metadata a script will run that will code generate the SQL OD CREATE VIEW statements\n",
							"\n",
							"### Steps to run\n",
							"- Import this notebook into Synapse\n",
							"- In Cells 4, 5, 10 and 11 you need to change the abfs path \"abfss://\" to point to your storage\n",
							"\n",
							"### Creating the SQL OD views\n",
							"- Copy the generated code from Cell 13 and click on Develop hub and then create a new SQL Script.\n",
							"    - NOTE: You do not need to copy the first line \"dfTable: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [database: string, tableName: string ... 1 more field]\"\n",
							"- Paste the code\n",
							"- You should create a new SQL OD database ```CREATE DATABASE DataLake```  (You can name this what you like)\n",
							"- Run the code on a SQL OD database (DataLake) \n",
							"- Run the SELECT TOP 10 * statements to verify thing work\n",
							"- You can now call the SQL OD view from any tool that supports SQL Server (Excel, Java, Python, .NET, PowerBI, etc.)\n",
							"\n",
							"### Hive metastore sync\n",
							"- Also note, if you navigate to the Data hub and the click on Workspace and filter the databases to \"default (Spark)\" you will also see the tables.  You can technically query this table with SQL OD.  Your Hive metastore is kept in sync from Spark and SQL OD (only for tables that are STORED AS PARQUET).  The reason you might not want to use this metastore is that strings are all VARCHAR(MAX) and you have less control of the datatypes.  Using the smallest datatype possible is always best."
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Taxi Data (Partitioned)\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							},
							"jupyter": {
								"outputs_hidden": true
							},
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"puLocationId"
									],
									"values": [
										"rateCodeID"
									],
									"yLabel": "rateCodeID",
									"xLabel": "puLocationId",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"rateCodeID\":{\"\":1071}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							},
							"collapsed": false
						},
						"source": [
							"%%spark\n",
							"// Azure Open Datasets.  Read in NYC Green taxi data.\n",
							"val wasbs_path = \"wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/green\"\n",
							"spark.conf.set(\"fs.azure.sas.nyctlc.azureopendatastorage.blob.core.windows.net\",\"\")\n",
							"val df = spark.read.parquet(wasbs_path)\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"// Write the data locally, this will save the data in the partitioned format\n",
							"df.write.partitionBy(\"puYear\", \"puMonth\").save(\"abfss://data-lake@lgnsynapselake.dfs.core.windows.net/nyx_taxi_green\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [],
									"values": [],
									"yLabel": "",
									"xLabel": "",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": true
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- Create an external table on your data lake.  Note, we store as PARQUET (Spark format)\n",
							"CREATE EXTERNAL TABLE nyx_taxi_green \n",
							"    (doLocationId STRING, dropoffLatitude DOUBLE, dropoffLongitude DOUBLE, extra DOUBLE, fareAmount DOUBLE, \n",
							"    improvementSurcharge STRING, lpepDropoffDatetime TIMESTAMP, lpepPickupDatetime TIMESTAMP, mtaTax DOUBLE,\n",
							"    passengerCount INT, paymentType INT, pickupLatitude DOUBLE, pickupLongitude DOUBLE, puLocationId STRING, \n",
							"    rateCodeID INT, storeAndFwdFlag STRING, tipAmount DOUBLE, tollsAmount DOUBLE, \n",
							"    totalAmount DOUBLE, tripDistance DOUBLE, tripType INT, vendorID INT)\n",
							"PARTITIONED BY (puYear INT, puMonth INT)\n",
							"STORED AS PARQUET\n",
							"LOCATION 'abfss://data-lake@lgnsynapselake.dfs.core.windows.net/nyx_taxi_green/'\n",
							"\n",
							"-- NOTE: \n",
							"-- If you get this error \"The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rwx-wx---;\", you need to grant \"other\" access to /tmp to correct.\n",
							"-- Open your data lake and find the folder /tmp and grant read/write/execute to other.  Set is as Access and Default.\n",
							"-- Do the same for the /tmp/hive folder."
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [],
									"values": [],
									"yLabel": "",
									"xLabel": "",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": true
							}
						},
						"source": [
							"%%sql\n",
							"-- Since the data was not inserted into the talbe with an INSERT..INTO, Hive needs to refresh its partitions from storage\n",
							"MSCK REPAIR TABLE nyx_taxi_green"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"doLocationId"
									],
									"values": [
										"dropoffLatitude"
									],
									"yLabel": "dropoffLatitude",
									"xLabel": "doLocationId",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"dropoffLatitude\":{\"\":407.5659828186035}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": true
							},
							"collapsed": false
						},
						"source": [
							"%%sql\n",
							"-- Make sure it works!\n",
							"SELECT *\n",
							"  FROM nyx_taxi_green\n",
							" LIMIT 10"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Population Data (Non-Partitioned)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							},
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"decennialTime"
									],
									"values": [
										"population"
									],
									"yLabel": "population",
									"xLabel": "decennialTime",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"population\":{\"2010\":667985}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": false
							}
						},
						"source": [
							"%%spark\n",
							"// Azure Open Datasets.  Read in NYC Green taxi data.\n",
							"val wasbs_path = \"wasbs://censusdatacontainer@azureopendatastorage.blob.core.windows.net/release/us_population_county/\"\n",
							"spark.conf.set(\"fs.azure.sas.censusdatacontainer.azureopendatastorage.blob.core.windows.net\",\"\")\n",
							"val df = spark.read.parquet(wasbs_path)\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"// Write the data locally (no partitioning)\n",
							"df.write.save(\"abfss://data-lake@lgnsynapselake.dfs.core.windows.net/us_population_by_county\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [],
									"values": [],
									"yLabel": "",
									"xLabel": "",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": true
							}
						},
						"source": [
							"%%sql\n",
							"-- Create an external table on your data lake.  Note, we store as PARQUET (Spark format)\n",
							"CREATE EXTERNAL TABLE us_population_by_county \n",
							"    (decennialTime STRING, stateName STRING, countyName STRING, population INT, race STRING, sex STRING, minAge INT, maxAge INT, year INT)\n",
							"STORED AS PARQUET\n",
							"LOCATION 'abfss://data-lake@lgnsynapselake.dfs.core.windows.net/us_population_by_county/'\n",
							"\n",
							"-- NOTE: \n",
							"-- If you get this error \"The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rwx-wx---;\", you need to grant \"other\" access to /tmp to correct.\n",
							"-- Open your data lake and find the folder /tmp and grant read/write/execute to other.  Set is as Access and Default.\n",
							"-- Do the same for the /tmp/hive folder."
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"diagram": {
								"activateDiagramType": 1,
								"chartConfig": {
									"category": "bar",
									"keys": [
										"decennialTime"
									],
									"values": [
										"population"
									],
									"yLabel": "population",
									"xLabel": "decennialTime",
									"aggregation": "SUM",
									"aggByBackend": false
								},
								"aggData": "{\"population\":{\"2000\":8737}}",
								"isSummary": false,
								"previewData": {
									"filter": null
								},
								"isSql": true
							}
						},
						"source": [
							"%%sql\n",
							"-- Make sure it works!\n",
							"SELECT *\n",
							"  FROM us_population_by_county\n",
							" LIMIT 10"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"\n",
							"val dfTable = spark.sql(\"SHOW TABLES\").filter(\"tableName == 'nyx_taxi_green' OR tableName == 'us_population_by_county'\") // you can run for all tables, but better to test with a few first\n",
							"  \n",
							"for (row <- dfTable.collect()) \n",
							"{   \n",
							"    var sqlOD = new StringBuilder(); \n",
							"    var sqlODPartitionSelect = new StringBuilder(); \n",
							"    var sqlODPartitionLocation = new StringBuilder(); \n",
							"    var sqlODRemovePartitionColFromTable = new StringBuilder(); \n",
							"    sqlOD.clear()\n",
							"    sqlODPartitionSelect.clear()\n",
							"    sqlODPartitionLocation.clear()\n",
							"    sqlODRemovePartitionColFromTable.clear()\n",
							"\n",
							"    var database = row.mkString(\",\").split(\",\")(0)\n",
							"    var tableName = row.mkString(\",\").split(\",\")(1)\n",
							"\n",
							"    val location = spark.sql(\"DESCRIBE EXTENDED \" + tableName).filter(\"col_name == 'Location'\").select($\"data_type\")\n",
							"    \n",
							"    sqlOD.append(\"-----------------------------------------------------------------------------------------------\\n\")\n",
							"    sqlOD.append(\"-- \" + tableName + \"\\n\")\n",
							"    sqlOD.append(\"-----------------------------------------------------------------------------------------------\\n\")\n",
							"    sqlOD.append(\"IF EXISTS (SELECT * FROM sys.objects WHERE name ='\" + tableName + \"')\\n\")\n",
							"    sqlOD.append(\"  BEGIN\\n\")\n",
							"    sqlOD.append(\"  DROP VIEW \" + tableName + \";\\n\")\n",
							"    sqlOD.append(\"  END\\n\")\n",
							"    sqlOD.append(\"GO\\n\\n\")\n",
							"    sqlOD.append(\"CREATE VIEW \" + tableName + \" AS\\n\")\n",
							"    sqlOD.append(\"SELECT\\n\")\n",
							"    sqlOD.append(\"REPLACE_ME_sqlODPartitionSelect\")\n",
							"    sqlOD.append(\"  *\\n\")\n",
							"    sqlOD.append(\"FROM OPENROWSET(\\n\")\n",
							"    sqlOD.append(\"   BULK '\")\n",
							"    for (loc <- location.collect())\n",
							"    {\n",
							"        var abfsPath = loc.mkString(\",\").split(\",\")(0)\n",
							"        sqlOD.append(abfsPath)\n",
							"        sqlOD.append(\"REPLACE_ME_sqlODPartitionLocation\")\n",
							"    }\n",
							"    sqlOD.append(\"/*.parquet',\\n\")\n",
							"    sqlOD.append(\"   FORMAT='PARQUET'\\n\")\n",
							"    sqlOD.append(\")\\n\")\n",
							"    sqlOD.append(\"WITH (\\n\")\n",
							"\n",
							"    val cols = spark.sql(\"DESCRIBE \" + tableName)\n",
							"    var i = 1;\n",
							"    var partitionIndex = 1;\n",
							"    var numberOfColumns = cols.collect().length\n",
							"    var processingPartitions = false\n",
							"\n",
							"    for (col <- cols.collect())\n",
							"    {\n",
							"        var colString = col.mkString(\"|\")\n",
							"        //println(colString)\n",
							"        var colSplit = colString.split('|')\n",
							"        //colSplit.foreach(println) \n",
							"        var col_name = colSplit(0)\n",
							"        var data_type = \"\"\n",
							"        try\n",
							"        {\n",
							"            data_type = colSplit(1)\n",
							"        }\n",
							"        catch {\n",
							"            case e: Exception => var void = \"\"\n",
							"        }\n",
							"\n",
							"        // Map data types (MORE WORK needs to be done here, this is just a few mappings)\n",
							"        if (data_type == \"string\")\n",
							"        {\n",
							"            data_type = \"VARCHAR(255)\"    // NOTE: This is just a fixed number that was picked since it covered most of the string sizes.  \n",
							"        }\n",
							"        if (data_type == \"double\")\n",
							"        {\n",
							"            data_type = \"FLOAT\"\n",
							"        }\n",
							"         if (data_type == \"timestamp\")\n",
							"        {\n",
							"            data_type = \"DATETIME2\"\n",
							"        }      \n",
							"\n",
							"        if (col_name == \"# Partition Information\" || col_name== \"# col_name\")\n",
							"        {\n",
							"            processingPartitions = true;\n",
							"        }\n",
							"\n",
							"        if (processingPartitions == true && col_name != \"# Partition Information\" && col_name != \"# col_name\")\n",
							"        {\n",
							"            sqlODPartitionSelect.append(\"   CAST(r.filepath(\" + partitionIndex.toString() + \") AS \" + data_type + \") AS \" + col_name + \",\\n\")\n",
							"            sqlODPartitionLocation.append(\"/\" + col_name + \"=*\")\n",
							"            partitionIndex = partitionIndex + 1 \n",
							"            sqlODRemovePartitionColFromTable.append(\"  \" + col_name + \" \" + data_type).append(\",\\n\")\n",
							"       }\n",
							"        \n",
							"        if (processingPartitions == false)\n",
							"        {\n",
							"            sqlOD.append(\"  \" + col_name + \" \" + data_type).append(\",\\n\")\n",
							"        }\n",
							"        i = i + 1\n",
							"    }\n",
							"          \n",
							"    sqlOD.append(\") AS [r];\\n\")\n",
							"    sqlOD.append(\"GO\\n\\n\")\n",
							"    sqlOD.append(\"-- SELECT TOP 10 * FROM \" + tableName + \";\\n\")\n",
							"    sqlOD.append(\"-----------------------------------------------------------------------------------------------\\n\")\n",
							"    sqlOD.append(\"\\n\")\n",
							"    sqlOD.append(\"\\n\")\n",
							"\n",
							"    println(sqlOD.toString()\n",
							"    .replace(\"REPLACE_ME_sqlODPartitionSelect\",sqlODPartitionSelect.toString())\n",
							"    .replace(\"REPLACE_ME_sqlODPartitionLocation\",sqlODPartitionLocation.toString())\n",
							"    .replace(sqlODRemovePartitionColFromTable.toString(),\"\") // remove partition columns from WITH statement\n",
							"    .replace(\",\\n) AS [r];\",\"\\n) AS [r];\") // remove the trailing comma from the WITH statement\n",
							"    )\n",
							"}"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark ML')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c5cfd151-2cd1-4e9c-8905-f8fa2cdd66f9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## Making Product Recommendations\n",
							"\n",
							"In this notebook, you will use sales data to create product recommendations for customers. \n",
							"\n",
							"When creating recommendation models there are generally two approaches that vary only on the data you use to compute the \"strength\" of a recommendation:\n",
							"- **Explicit ratings**: In this case, each user and product has a star rating attached. Think you might review restaurants in your favorite app.\n",
							"- **Implicit ratings**: In this case each user and product has a rating that is derived from some behavioral metric. Typically that metric is a count, like number purchases of that product or the number of product page views. The actual rating is \"implicit\" in the sense that it is computed algorithmically instead of directly using the value provided.\n",
							"\n",
							"In this notebook, you train a model that makes product recommendation based purchase history. For each user and product that appears in the history, you will sum the quantity of items purchased across each transaction. This sum will create an **explicit** rating for the user to product mapping, effectively your model is saying the more of a product a user buys across all transactions, the more relevant it is to that user.\n",
							"\n",
							"The model then goes one step further, and enables you to calculate the recommendations for a user, who may not have bought the product before, but because her purchases are similar to another's, she might like the strongest recommendations inferred from other users like her. Think of this as the algorthim filling in the blanks for the user and a given product, it predicts what that rating should be between them.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"### Using Spark ML's ALS Algorithm\n",
							"\n",
							"Run the following cell to import the ALS class.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.ml.recommendation import ALS\n",
							"from pyspark.sql import Row\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Run the following cell to load the SaleSmall table from the SQL Pool. Make sure the SQL pool name (#SQL_POOL_NAME#) matches the name of your SQL Pool.\n",
							"\n",
							"Remember, that in order to read from table like this, we need to use Scala to create a DataFrame around it.\n",
							"\n",
							"Once you have the DataFrame, you can create a named view from it. Since named views are registered in the shared metastore, you can access view by name from both Scala and Python cells.\n",
							"\n",
							"In this cell we use the Spark magic to run this cells content's using Scala, create the DataFrame and then register it as a view that we will use from later Python cells.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"%%spark\n",
							"val df = spark.read.sqlanalytics(\"#SQL_POOL_NAME#.wwi.SaleSmall\") \n",
							"df.createOrReplaceTempView(\"sales\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Next, you need to get just the data you need to train the recommender.\n",
							"\n",
							"A recommender really only operates on three fields: the customerid/userid, the itemid/productid and the rating.\n",
							"\n",
							"In the following cell, we select just those columns, summing the quantity field to get a toal purchase count for any given product by that user in the history. This field, which we alias as numpurchases, is our rating.\n",
							"\n",
							"Then we call `cache()` on the dataframe so that resultant dataset is cached in Spark memory (or disk) and does not have to be recomputed everytime we make a subequent query for it. Machine learning algorithms like ALS make several passes thru data, so caching the DataFrame provides a significant performance boost.\n",
							"\n",
							"Run the following cell to prepare and cached the implicit ratings (IR) table.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"# To be able to complete this lab in under an hour, let's just work with 1,500k rows\n",
							"sample = spark.sql(\"SELECT productid, customerid, quantity FROM sales\").limit(15000000)\n",
							"sample.createOrReplaceTempView(\"salessample\")\n",
							"\n",
							"ir = spark.sql(\"SELECT productid, customerid, SUM(quantity) as numpurchases FROM salessample GROUP BY productid, customerid LIMIT 500000\")\n",
							"ir.cache()\n",
							"display(ir)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"In training a model, we withold a subset of the data to use when evaluating the model. This is the test set. The train set is what we show to the algorithm for it to learn from.\n",
							"\n",
							"The basic idea is if you show the model all of the data you have, it may effectively memorize all the answers- meaning it will perform very well if it has seen the question before, but probably perform poorly against new questions. This problem that is avoided is called overfitting the model to the data.\n",
							"\n",
							"In the first line, we split our data into those train and test subsets.\n",
							"\n",
							"In the sceond line, we instantiate the ALS algorithm, telling it which columns in our data are the user, item and rating. THe `maxIter` controls how many passes the training takes over the data, and the `regParam` controls how big of an adjustment the model makes during each pass. These are set to the common defaults so you can ignore those values for the purposes of this lab. \n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"train, test = ir.randomSplit([0.7,0.3])\n",
							"als = ALS(maxIter=5,regParam=0.01,userCol=\"customerid\",itemCol=\"productid\",ratingCol=\"numpurchases\")\n",
							"model = als.fit(train)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"With a trained model in hand, we can now use it to make some recommendations. \n",
							"\n",
							"In reccommender systems it is very common to pre-compute the recommendations in batch and then simply query their reccommendations for a single customer later.\n",
							"\n",
							"Run the following cell to batch compute the top 5 product recommendations for each customer, and see a sampling of the result.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"source": [
							"product_recommendations = model.recommendForAllUsers(5).selectExpr(\"customerid\",\"explode(recommendations) as rec\")\n",
							"product_recommendations = product_recommendations.selectExpr(\"customerId\", \"rec.productid\", \"rec.rating\")\n",
							"product_recommendations.createOrReplaceTempView(\"recommendations\")\n",
							"display(product_recommendations)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"Now that you have the batch recommendations, save the results to a table in a SQL Pool by running the following cell. With this, downstream applications can look up recommendations by issuing a traditional T-SQL query.\n",
							"\n",
							"Don't forget to check the name of your SQL Pool used on Line 3.\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "scala"
							}
						},
						"source": [
							"%%spark\n",
							"val recommendations = spark.sql(\"SELECT * from recommendations\")\n",
							"recommendations.write.sqlanalytics(\"#SQL_POOL_NAME#.wwi.Recommendations\", Constants.INTERNAL) "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/StartingExample')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SynapseML"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "c747068e-bfcc-4c11-a0e1-6a832a23cb61"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark31",
						"name": "lgnsynspark31",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%%configure -f\r\n",
							"{\r\n",
							"  \"name\": \"synapseml\",\r\n",
							"  \"conf\": {\r\n",
							"      \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.9.5-13-d1b51517-SNAPSHOT\",\r\n",
							"      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\r\n",
							"      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12\",\r\n",
							"      \"spark.yarn.user.classpath.first\": \"true\"\r\n",
							"  }\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *\r\n",
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"# A general Cognitive Services key for Text Analytics and Computer Vision (or use separate keys that belong to each service)\r\n",
							"cognitive_service_key = \"19b7a28cda994fb2b50a556327ad4012\"\r\n",
							"# A Bing Search v7 subscription key\r\n",
							"#bingsearch_service_key = mssparkutils.credentials.getSecret(\"ADD_YOUR_KEY_VAULT_NAME\", \"ADD_YOUR_BING_SEARCH_KEY\",\"ADD_YOUR_KEY_VAULT_LINKED_SERVICE_NAME\")\r\n",
							"# An Anomaly Dectector subscription key\r\n",
							"#anomalydetector_key = mssparkutils.credentials.getSecret(\"ADD_YOUR_KEY_VAULT_NAME\", \"ADD_YOUR_ANOMALY_KEY\",\"ADD_YOUR_KEY_VAULT_LINKED_SERVICE_NAME\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"# Create a dataframe that's tied to it's column names\r\n",
							"df_sentences = spark.createDataFrame([\r\n",
							"  (\"I am so happy today, its sunny!\", \"en-US\"),\r\n",
							"  (\"this is a dog\", \"en-US\"),\r\n",
							"  (\"I am frustrated by this rush hour traffic!\", \"en-US\")\r\n",
							"], [\"text\", \"language\"])\r\n",
							"\r\n",
							"# Run the Text Analytics service with options\r\n",
							"sentiment = (TextSentiment()\r\n",
							"    .setTextCol(\"text\")\r\n",
							"    .setLocation(\"eastus2\") # Set the location of your cognitive service\r\n",
							"    .setSubscriptionKey(cognitive_service_key)\r\n",
							"    .setOutputCol(\"sentiment\")\r\n",
							"    .setErrorCol(\"error\")\r\n",
							"    .setLanguageCol(\"language\"))\r\n",
							"\r\n",
							"# Show the results of your text query in a table format\r\n",
							"\r\n",
							"display(sentiment.transform(df_sentences).select(\"text\", col(\"sentiment\")[0].getItem(\"sentiment\").alias(\"sentiment\")))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#first model\r\n",
							"import numpy as np\r\n",
							"import pandas as pd"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dataFile = \"AdultCensusIncome.csv\"\r\n",
							"import os, urllib\r\n",
							"if not os.path.isfile(dataFile):\r\n",
							"    urllib.request.urlretrieve(\"https://mmlspark.azureedge.net/datasets/\" + dataFile, dataFile)\r\n",
							"data = spark.createDataFrame(pd.read_csv(dataFile, dtype={\" hours-per-week\": np.float64}))\r\n",
							"data.show(5)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data = data.select([\" education\", \" marital-status\", \" hours-per-week\", \" income\"])\r\n",
							"train, test = data.randomSplit([0.75, 0.25], seed=123)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from synapse.ml.train import TrainClassifier\r\n",
							"from pyspark.ml.classification import LogisticRegression\r\n",
							"model = TrainClassifier(model=LogisticRegression(), labelCol=\" income\").fit(train)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from synapse.ml.train import ComputeModelStatistics\r\n",
							"prediction = model.transform(test)\r\n",
							"metrics = ComputeModelStatistics().transform(prediction)\r\n",
							"metrics.select('accuracy').show()"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Stream-Processor')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": ".NET"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "90934516-07f9-4781-b247-086aec7770a0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparkdotnet",
						"display_name": "csharp"
					},
					"language_info": {
						"name": "csharp"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark31",
						"name": "lgnsynspark31",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"## Stream processing\r\n",
							"\r\n",
							"This notebook shows how to connect to an Event Hub, read the stream and process the messages using .NET for Apache Spark on Azure Synapse Analytics.\r\n",
							"\r\n",
							"In order to reproduce a common use case, we have the code for the following steps: \r\n",
							"\r\n",
							"- Read stream from Event Hubs\r\n",
							"- Decompress gzip body\r\n",
							"- Apply schema\r\n",
							"- Apply stream processing\r\n",
							"- (*optional*) Save to Delta table\r\n",
							"\r\n",
							"Please refer to the official documentation for additional details:\r\n",
							"- [.NET for Apache Spark](https://dotnet.microsoft.com/apps/data/spark)\r\n",
							"- [.NET for Spark on Azure Synapse Analytics](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/spark-dotnet)\r\n",
							"- [.NET APIs for Spark](https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark?view=spark-dotnet)\r\n",
							"- [UDFs on .NET for Spark](https://docs.microsoft.com/en-us/dotnet/spark/how-to-guides/udf-guide)\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Parameters\r\n",
							"These are just placeholders, the real values can be either inserted here before running the notebook, or even better overwritten using pipeline variables: https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks?tabs=preview#assign-parameters-values-from-a-pipeline"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"var eh_connstr = \"<connection string of your Event Hub>\";\r\n",
							"var eh_consumergroup = \"<consumer group of your Event Hub you want to use to receive the messages>\";\r\n",
							"var eh_data_type = \"<can be arbitrary, its main goal is to differentiate between message checkpoint locations>\"; //The idea here is to use the type of data you are reading to create the checkpoint location for your delta table."
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"\r\n",
							"var eh_checkpoint_location = $\"/delta/_checkpoints/{eh_data_type}/\";\r\n",
							"if(eh_connstr == \"placeholder\") \r\n",
							"{\r\n",
							"    Console.Error.WriteLine(\"Please insert your EH connection string in the parameters\");\r\n",
							"}\r\n",
							"\r\n",
							"Console.WriteLine(eh_checkpoint_location);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Connect to Event Hub\r\n",
							"We will now connect to Event Hub and create a streaming dataframe (**streamingDf**). \r\n",
							"\r\n",
							"We need to manually encrypt the connection string. \r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"// The connection string must be encrypted\r\n",
							"\r\n",
							"using Microsoft.Spark.Interop;\r\n",
							"var eventHubConnectionStringEncrypted = (string)SparkEnvironment.JvmBridge.CallStaticJavaMethod(\"org.apache.spark.eventhubs.EventHubsUtils\", \"encrypt\", eh_connstr);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Connect to EH\r\n",
							"var ehConf = new Dictionary<string, string>();\r\n",
							"ehConf.Add(\"eventhubs.connectionString\", eventHubConnectionStringEncrypted);\r\n",
							"ehConf.Add(\"eventhubs.consumerGroup\", eh_consumergroup);\r\n",
							"\r\n",
							"DataFrame streamingDf = spark\r\n",
							"    .ReadStream()\r\n",
							"    .Format(\"eventhubs\")\r\n",
							"    .Options(ehConf)\r\n",
							"    .Load();"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Decompress gzip body\r\n",
							"\r\n",
							"The message body is compressed (gzip). We need to create a UDF to decompress each row in our **streamingDF**.\r\n",
							"We need to add the *SharpZipLib* nuget package. [Here](https://documentation.help/ICSharpCode.SharpZipLib/documentation.pdf) you can find more information about the library.\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"// We use SharpZipLib to decompress the body\r\n",
							"#r \"nuget:SharpZipLib\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"using System.IO;\r\n",
							"using ICSharpCode.SharpZipLib.GZip;\r\n",
							"\r\n",
							"// Define an UDF to decompress EH body\r\n",
							"public static string DecompressFunction(byte[] data)\r\n",
							"{\r\n",
							"    using var source = new MemoryStream(data);\r\n",
							"    using GZipInputStream zipStream = new GZipInputStream(source);\r\n",
							"    using StreamReader sr = new StreamReader(zipStream);\r\n",
							"    return sr.ReadToEnd();\r\n",
							"}\r\n",
							"\r\n",
							"Func<Column, Column> DecompressAvro = Udf<byte[], string>(DecompressFunction);"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Decompress EH body\r\n",
							"var decompDf = streamingDf.WithColumn(\"DecompressedBody\", DecompressAvro(streamingDf[\"body\"]));"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Apply schema to EH messages\n",
							"\n",
							"The schema of the decompressed body is defined and applied below.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"// Define a schema for EH body\r\n",
							"using Microsoft.Spark.Sql.Types;\r\n",
							"var schema = new StructType(new[]\r\n",
							"{\r\n",
							"    new StructField(\"DataType\", new StringType()),\r\n",
							"    new StructField(\"MessageContent\", new StringType()),\r\n",
							"    new StructField(\"MessageId\", new StringType()),\r\n",
							"    new StructField(\"Timestamp\", new TimestampType()),\r\n",
							"});"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// Parse json\r\n",
							"var jsonDf = decompDf\r\n",
							"    .WithColumn(\"jsonBody\", FromJson(decompDf[\"DecompressedBody\"], schema.SimpleString))\r\n",
							"    .Select(\"enqueuedTime\", \"jsonBody\", \"body\");"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Process the stream\r\n",
							"\r\n",
							"Extract meaningful data from the decompressed body and add them as separate columns.\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"// Process stream: Extract meaningful fields\r\n",
							"var finalDf = jsonDf\r\n",
							"    .WithColumn(\"MessageId\", jsonDf[\"jsonBody\"].GetField(\"MessageId\"))\r\n",
							"    .WithColumn(\"DocType\", Lit(eh_data_type))\r\n",
							"    .WithColumn(\"SourceType\", jsonDf[\"jsonBody\"].GetField(\"DataType\"))\r\n",
							"    .WithColumn(\"TimeStamp\", jsonDf[\"jsonBody\"].GetField(\"Timestamp\"));"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as a Delta Table\r\n",
							"Save the dataframe as a delta table, which will be created if does not exist.\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"// ToTable API is available from Spark 3.1\r\n",
							"var deltaStream = finalDf\r\n",
							"    .WriteStream()\r\n",
							"    .Format(\"delta\")\r\n",
							"    .OutputMode(\"append\")\r\n",
							"    .Option(\"checkpointLocation\", eh_checkpoint_location)\r\n",
							"    .ToTable(\"deltatablesample\");"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"spark.Sql(\"SELECT * FROM deltatablesample LIMIT 10\").Show();"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Debug\r\n",
							"\r\n",
							"The following cells can be used to verify if the data is correclty streamed and manipulated in any of the steps above.\r\n",
							"\r\n",
							"Just add those 2 cells after the manipulation you need to test, replace the name of the DataStreamWriter (here *finalDf*) with the one you want to test, and run both cells. Please notice that the second one might look like it's empty in the beginning, but it just needs few seconds before being able to show the actual results. \r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"// DEBUG\r\n",
							"var sQuery = finalDf.WriteStream().Format(\"memory\").QueryName(\"finalDf\").Start();"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"// DEBUG\r\n",
							"spark.Sql(\"select * from finalDf\").Show();"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SynapseMLTesting')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "bd4799f9-4992-4cce-a991-d72a068b7094"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark31",
						"name": "lgnsynspark31",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%%configure -f\r\n",
							"{\r\n",
							"    \"name\": \"synapseml\",\r\n",
							"    \"conf\": {\r\n",
							"        \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.9.5\",\r\n",
							"        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12\",\r\n",
							"        \"spark.yarn.user.classpath.first\": \"true\"\r\n",
							"    }\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import synapse.ml\r\n",
							"from synapse.ml.cognitive import *\r\n",
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"# A general Cognitive Services key for Text Analytics and Computer Vision (or use separate keys that belong to each service)\r\n",
							"cognitive_service_key = mssparkutils.credentials.getSecret(\"ADD_YOUR_KEY_VAULT_NAME\", \"ADD_YOUR_SERVICE_KEY\",\"ADD_YOUR_KEY_VAULT_LINKED_SERVICE_NAME\")\r\n",
							"# A Bing Search v7 subscription key\r\n",
							"bingsearch_service_key = mssparkutils.credentials.getSecret(\"ADD_YOUR_KEY_VAULT_NAME\", \"ADD_YOUR_BING_SEARCH_KEY\",\"ADD_YOUR_KEY_VAULT_LINKED_SERVICE_NAME\")\r\n",
							"# An Anomaly Dectector subscription key\r\n",
							"anomalydetector_key = mssparkutils.credentials.getSecret(\"ADD_YOUR_KEY_VAULT_NAME\", \"ADD_YOUR_ANOMALY_KEY\",\"ADD_YOUR_KEY_VAULT_LINKED_SERVICE_NAME\")\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Tokens and String Splitting')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 0,
				"nbformat_minor": 0,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from azure.storage.blob import BlockBlobService\n",
							"\n",
							"block_blob_service = BlockBlobService(\n",
							"    account_name='asadatalake176496', account_key='1XM/nC/u7/JAC+FRHhFoyNeo1n3ipBY86quFoY9D3ITmPRoDFx8EMdjWwdCmN6Yg3RCsm7AFESDiaszF9VTxPw==') "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"outputCollapsed": true
						},
						"source": [
							"import numpy as np\n",
							"\n",
							"for i in np.arange(1, 32):\n",
							"\n",
							"    file_path = f'abfss://wwi-02@asadatalake176496.dfs.core.windows.net/sale-small/Year=2017/Quarter=Q2/Month=5/Day=201705{i:02}/sale-small-201705{i:02}-snappy.parquet'\n",
							"    print(file_path)\n",
							"\n",
							"    data_path = spark.read.load(file_path, format='parquet')\n",
							"    sale_df = data_path.toPandas()\n",
							"    block_blob_service.create_blob_from_text('wwi-02', f'sale-poc-v2/sale-201705{i:02}.csv', sale_df.to_csv(quotechar='\\'', index=False, header=True, escapechar='\\\\', doublequote=False))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"data_path = spark.read.load('abfss://wwi-02@asadatalake176496.dfs.core.windows.net/sale-small/Year=2017/Quarter=Q2/Month=5/Day=20170501/sale-small-20170501-snappy.parquet', format='parquet')\n",
							"sale_df = data_path.toPandas()\n",
							"\n",
							"from decimal import *\n",
							"\n",
							"sale_df['TotalAmount'] = sale_df['TotalAmount'] * Decimal(1.1)\n",
							"sale_df['ProfitAmount'] = sale_df['ProfitAmount'] * Decimal(1.1)\n",
							"\n",
							"block_blob_service.create_blob_from_text('wwi-02', 'sale-poc-v2/sale-20170501.csv', sale_df.to_csv(quotechar='\\'', index=False, header=True, escapechar='\\\\', doublequote=False))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"data_path = spark.read.load('abfss://wwi-02@asadatalake176496.dfs.core.windows.net/sale-small/Year=2017/Quarter=Q2/Month=5/Day=20170502/sale-small-20170502-snappy.parquet', format='parquet')\n",
							"sale_df = data_path.toPandas()\n",
							"\n",
							"from decimal import *\n",
							"\n",
							"sale_df['TotalAmount'] = sale_df['TotalAmount'] * Decimal(1.1)\n",
							"sale_df['ProfitAmount'] = sale_df['ProfitAmount'] * Decimal(1.1)\n",
							"\n",
							"block_blob_service.create_blob_from_text('wwi-02', 'sale-poc-v2/sale-20170502.csv', sale_df.to_csv(quotechar='\\'', index=False, header=False, escapechar='\\\\', doublequote=False, line_terminator=','))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"from azure.storage.blob import BlockBlobService\n",
							"\n",
							"block_blob_service = BlockBlobService(\n",
							"    account_name='asadatalake176496', account_key='1XM/nC/u7/JAC+FRHhFoyNeo1n3ipBY86quFoY9D3ITmPRoDFx8EMdjWwdCmN6Yg3RCsm7AFESDiaszF9VTxPw==') \n",
							"\n",
							"file_content = block_blob_service.get_blob_to_text('wwi-02', 'sale-poc-v2/sale-20170502.csv')\n",
							"\n",
							"tokens = file_content.content.split(',')\n",
							"\n",
							"print(f'Found {len(tokens)} tokens in content.')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"[tokens[i] for i in np.arange(0, 10)] "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"import numpy as np\n",
							"import pandas as pd\n",
							"\n",
							"row_list = []\n",
							"\n",
							"max_index = 11\n",
							"while max_index <= len(tokens):\n",
							"\n",
							"    row = [tokens[i] for i in np.arange(max_index - 11, max_index)] \n",
							"    row_list.append(row)\n",
							"\n",
							"    max_index += 11\n",
							"\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"outputCollapsed": true
						},
						"source": [
							"df = pd.DataFrame(row_list, columns = ['TransactionId', 'CustomerId', 'ProductId', 'Quantity', 'Price', 'TotalAmount', 'TransactionDateId', 'ProfitAmount', 'Hour', 'Minute', 'StoreId'])\n",
							"df.head(100)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"import numpy as np\n",
							"import pandas as pd\n",
							"\n",
							"data_path = spark.read.load('abfss://wwi-02@asadatalake176496.dfs.core.windows.net/data-generators/generator-customer.csv', format='CSV', header=True, quote='\\'')\n",
							"df = data_path.toPandas()\n",
							"df.columns"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"df.Address.str.split(',')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Updates and GDPR using Delta Lake - PySpark')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a4964640-eeaf-4d2f-b252-8eaa8aaf92ee"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"#  Updates and GDPR using Delta Lake - PySpark\n",
							"\n",
							"In this notebook, we will review Delta Lake's end-to-end capabilities in PySpark. You can also look at the original Quick Start guide if you are not familiar with [Delta Lake](https://github.com/delta-io/delta) [here](https://docs.delta.io/latest/quick-start.html). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"In this notebook, we will cover the following:\n",
							"\n",
							"- Creating sample mock data containing customer orders\n",
							"- Writing this data into storage in Delta Lake table format (or in short, Delta table)\n",
							"- Querying the Delta table using functional and SQL\n",
							"- The Curious Case of Forgotten Discount - Making corrections to data\n",
							"- Enforcing GDPR on your data\n",
							"- Oops, enforced it on the wrong customer! - Looking at the audit log to find mistakes in operations\n",
							"- Rollback all the way!\n",
							"- Closing the loop - 'defrag' your data"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Creating sample mock data containing customer orders\n",
							"\n",
							"For this tutorial, we will setup a sample file containing customer orders with a simple schema: (order_id, order_date, customer_name, price)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP TABLE IF EXISTS input\");\n",
							"spark.sql(\"\"\"\n",
							"          CREATE TEMPORARY VIEW input \n",
							"          AS SELECT 1 order_id, '2019-11-01' order_date, 'Saveen' customer_name, 100 price\n",
							"          UNION ALL SELECT 2, '2019-11-01', 'Terry', 50\n",
							"          UNION ALL SELECT 3, '2019-11-01', 'Priyanka', 100\n",
							"          UNION ALL SELECT 4, '2019-11-02', 'Steve', 10\n",
							"          UNION ALL SELECT 5, '2019-11-03', 'Rahul', 10\n",
							"          UNION ALL SELECT 6, '2019-11-03', 'Niharika', 75\n",
							"          UNION ALL SELECT 7, '2019-11-03', 'Elva', 90\n",
							"          UNION ALL SELECT 8, '2019-11-04', 'Andrew', 70\n",
							"          UNION ALL SELECT 9, '2019-11-05', 'Michael', 20\n",
							"          UNION ALL SELECT 10, '2019-11-05', 'Brigit', 25\"\"\")\n",
							"orders = spark.sql(\"SELECT * FROM input\")\n",
							"orders.show()\n",
							"orders.printSchema()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Writing this data into storage in Delta Lake table format (or in short, Delta table)\n",
							"\n",
							"To create a Delta Lake table, you can write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta. These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. \n",
							"\n",
							"If you already have existing data in Parquet format, you can do an \"in-place\" conversion to Delta Lake format. The code would look like following:\n",
							"\n",
							"DeltaTable.convertToDelta(spark, $\"parquet.`{path_to_data}`\");\n",
							"\n",
							"//Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(parquetPath)"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000)\n",
							"path = \"/delta/delta-table-{0}\".format(session_id)\n",
							"path"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"# Here's how you'd do this in Parquet: \n",
							"# orders.repartition(1).write().format(\"parquet\").save(path)\n",
							"\n",
							"orders.repartition(1).write.format(\"delta\").save(path)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Querying the Delta table using functional and SQL\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"ordersDataFrame = spark.read.format(\"delta\").load(path)\n",
							"ordersDataFrame.show()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"ordersDataFrame.createOrReplaceTempView(\"ordersDeltaTable\")\n",
							"spark.sql(\"SELECT * FROM ordersDeltaTable\").show"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(path + \"/_delta_log/\").collect()]"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"# The Curious Case of Forgotten Discount - Making corrections to data\n",
							"\n",
							"Now that you are able to look at the orders table, you realize that you forgot to discount the orders that came in on November 1, 2019. Worry not! You can quickly make that correction."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"table = DeltaTable.forPath(spark, path)\n",
							"\n",
							"# Update every transaction that took place on November 1, 2019 and apply a discount of 10%\n",
							"table.update(\n",
							"  condition = expr(\"order_date == '2019-11-01'\"),\n",
							"  set = {\"price\": expr(\"price - price*0.1\") })"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"table.toDF()"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(path + \"/_delta_log/\").collect()]"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Enforcing GDPR on your data\n",
							"\n",
							"One of your customers wanted their data to be deleted. But wait, you are working with data stored on an immutable file system (e.g., HDFS, ADLS, WASB). How would you delete it? Using Delta Lake's Delete API.\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditionally update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Delete the appropriate customer\n",
							"table.delete(condition = expr(\"customer_name == 'Saveen'\"))\n",
							"table.toDF().show()"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Oops, enforced it on the wrong customer! - Looking at the audit/history log to find mistakes in operations\n",
							"\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(20, 1000, False)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Rollback all the way using Time Travel!\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(path).write.mode(\"overwrite\").format(\"delta\").save(path)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"# Delete the correct customer - REMOVE\n",
							"table.delete(condition = expr(\"customer_name == 'Rahul'\"))\n",
							"table.toDF().show()"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"source": [
							"table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(20, 1000, False)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"source": [
							"# Closing the loop - 'defrag' your data\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
							"table.vacuum(0.01)\n",
							"\n",
							"# Alternate Syntax: spark.sql($\"VACUUM delta.`{path}`\").show"
						],
						"outputs": [],
						"execution_count": 19
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dimProductWithSCDColumns')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "ca6345e2-5d47-4784-83e3-6c6aa0d12b6a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark31",
						"name": "lgnsynspark31",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"DROP TABLE dimProduct"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"CREATE TABLE dimProduct (\r\n",
							"    ProductID int,\r\n",
							"    Name string,\r\n",
							"    ProductNumber string,\r\n",
							"    Color string,\r\n",
							"    StandardCost decimal(19,4),\r\n",
							"    ListPrice decimal(19,4),\r\n",
							"    Size string,\r\n",
							"    Weight decimal(8,2),\r\n",
							"    ModifiedDate timestamp,\r\n",
							"    DateInserted timestamp,\r\n",
							"    ProductCategory string,\r\n",
							"    ProductModel string,\r\n",
							"    RowSignature bigint,\r\n",
							"    EffectiveFromDate timestamp,\r\n",
							"    EffectiveToDate timestamp,\r\n",
							"    IsActive int,\r\n",
							"    ProductKey bigint\r\n",
							"    )\r\n",
							"    USING DELTA\r\n",
							"    LOCATION  '/delta/gold/dimProduct/'"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"INSERT INTO dimProduct  VALUES (-1,'Unknown',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,\r\n",
							"NULL,NULL,CURRENT_TIMESTAMP(),cast('9099-01-01' as timestamp),1,-1)"
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dimWatermarks')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "df3a5759-842e-45b5-b39c-1f06fdf5e8a8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparksql",
						"display_name": "sql"
					},
					"language_info": {
						"name": "sql"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark31",
						"name": "lgnsynspark31",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"CREATE TABLE IF NOT EXISTS dimWatermarks\r\n",
							"(\r\n",
							"    TableName string,\r\n",
							"    LastKey bigint\r\n",
							")\r\n",
							"USING DELTA\r\n",
							"LOCATION \"/delta/gold/dimWatermarks\";\r\n",
							"\r\n",
							"\r\n",
							"Insert into dimWatermarks VALUES (\"DimProduct\",0);\r\n",
							"Insert into dimWatermarks VALUES (\"DimCustomer\",0);\r\n",
							"Insert into dimWatermarks VALUES (\"DimAddress\",0); \r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"SELECT * from dimwatermarks"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/rana_data_profiling')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "lgnsynspark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9d424378-1636-4138-a7d6-75f02ad3f877"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/65c04546-fce4-4b3c-be44-dac0e5324e24/resourceGroups/lgnsynapse/providers/Microsoft.Synapse/workspaces/lgnsynapse/bigDataPools/lgnsynspark32",
						"name": "lgnsynspark32",
						"type": "Spark",
						"endpoint": "https://lgnsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/lgnsynspark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://openhack6srvfilesystem@openhack6srvdatalake.dfs.core.windows.net/cloudsales/RAW/orderdetails/dbo.OrderDetails.parquet', format='parquet')\r\n",
							"#display(df.limit(10))\r\n",
							"df.createOrReplaceTempView('vw_OrderDetails')"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://openhack6srvfilesystem@openhack6srvdatalake.dfs.core.windows.net/cloudsales/RAW/Orders/dbo.Orders.parquet', format='parquet')\r\n",
							"#display(df.limit(10))\r\n",
							"df.createOrReplaceTempView('vw_Orders')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--select count(distinct OrderID) from vw_OrderDetails\r\n",
							"select count(distinct OrderID) from vw_Orders"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select count(*) from vw_OrderDetails a join vw_Orders b on a.orderId = b.orderId"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://openhack6srvfilesystem@openhack6srvdatalake.dfs.core.windows.net/curated/dvdsales/part-00000-2b2d45bf-5014-40e5-ad6f-37223c33ffe1-c000.snappy.parquet', format='parquet')\r\n",
							"df.count()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"create database southridget6db"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"create table southridget6db.dvdsales\r\n",
							"  USING DELTA\r\n",
							"  LOCATION 'abfss://openhack6srvfilesystem@openhack6srvdatalake.dfs.core.windows.net/curated/dvdsales'"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from southridget6db.dvdsales limit 10;"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://openhack6srvfilesystem@openhack6srvdatalake.dfs.core.windows.net/cloudsales/RAW/Customers/dbo.Customers.parquet', format='parquet')\r\n",
							"df.printSchema()\r\n",
							"#display(df.limit(10))\r\n",
							"#df.createOrReplaceTempView('vw_cloudsales_customers')"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://openhack6srvfilesystem@openhack6srvdatalake.dfs.core.windows.net/cloudstreaming/RAW/dboCustomers.parquet', format='parquet')\r\n",
							"df.printSchema()\r\n",
							"#display(df.limit(10))\r\n",
							"#df.createOrReplaceTempView('vw_cloudstreaming_customers')"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--select * from vw_cloudsales_customers\r\n",
							"select * from vw_cloudstreaming_customers"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://openhack6srvfilesystem@openhack6srvdatalake.dfs.core.windows.net/fourthcoffee/RAW/Customers.csv', format='csv', header=True)\r\n",
							"df.printSchema()\r\n",
							"#display(df.limit(10))\r\n",
							"df.createOrReplaceTempView('vw_fourthcoffee_customers')"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://openhack6srvfilesystem@openhack6srvdatalake.dfs.core.windows.net/vanarsdel/RAW/dboCustomers.parquet', format='parquet')\r\n",
							"df.printSchema()\r\n",
							"#display(df.limit(10))\r\n",
							"df.createOrReplaceTempView('vw_vanarsdel_customers')"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--select  customerID, count(*) from southridget6db.customers group by 1 having count(*)>1\r\n",
							"--select * from southridget6db.customers where customerID = '6b1d5569-c5a6-499e-8297-cb494bdc2dff'\r\n",
							"\r\n",
							"    describe extended southridget6db.customers"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"--unit tests\r\n",
							"select * from southridget6db.customers\r\n",
							"where \r\n",
							"LastName is null Or\r\n",
							"FirstName is null OR\r\n",
							"AddressLine1 is null OR\r\n",
							"City is null OR\r\n",
							"State is null OR\r\n",
							"ZipCode is null OR\r\n",
							"PhoneNumber is null"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tutorial-predict-nyc-taxi-tips-onnx')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "PySpark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "01db0aec-3ba3-40c9-8aef-5aea427de8b7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Predict NYC Taxi Tips \r\n",
							"The notebook ingests, prepares and then trains a model based on an Open Dataset that tracks NYC Yellow Taxi trips and various attributes around them. The goal is to for a given trip, predict whether there will be a tip or not. The model then will be converted to ONNX format and tracked by MLFlow.\r\n",
							"We will later use the ONNX model for inferencing in Azure Synapse SQL Pool using the new model scoring wizard.\r\n",
							"## Note:\r\n",
							"**Please note that for successful conversion to ONNX, this notebook requires using  Scikit-learn version 0.20.3.**\r\n",
							"Run the first cell to list the packages installed and check your sklearn version. Uncomment the pip install command to install the correct version\r\n",
							"\r\n",
							"%pip install scikit-learn==0.20.3\r\n",
							"\r\n",
							""
						],
						"attachments": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Load data\r\n",
							"Get a sample data of nyc yellow taxi from Azure Open Datasets"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"#%pip list\n",
							"#%pip install scikit-learn==0.20.3"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"gather": {
								"logged": 1599713958224
							}
						},
						"source": [
							"from azureml.opendatasets import NycTlcYellow\r\n",
							"from datetime import datetime\r\n",
							"from dateutil import parser\r\n",
							"\r\n",
							"start_date = parser.parse('2018-05-01')\r\n",
							"end_date = parser.parse('2018-05-07')\r\n",
							"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\r\n",
							"nyc_tlc_df = nyc_tlc.to_pandas_dataframe()\r\n",
							"nyc_tlc_df.info()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1599713959314
							},
							"collapsed": true
						},
						"source": [
							"from IPython.display import display\r\n",
							"\r\n",
							"sampled_df = nyc_tlc_df.sample(n=10000, random_state=123)\r\n",
							"display(sampled_df.head(5))"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Prepare and featurize data\r\n",
							"- There are extra dimensions that are not going to be useful in the model. We just take the dimensions that we need and put them into the featurised dataframe. \r\n",
							"- There are also a bunch of outliers in the data so we need to filter them out."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1599713966451
							},
							"collapsed": true
						},
						"source": [
							"import numpy\r\n",
							"import pandas\r\n",
							"\r\n",
							"def get_pickup_time(df):\r\n",
							"    pickupHour = df['pickupHour'];\r\n",
							"    if ((pickupHour >= 7) & (pickupHour <= 10)):\r\n",
							"        return 'AMRush'\r\n",
							"    elif ((pickupHour >= 11) & (pickupHour <= 15)):\r\n",
							"        return 'Afternoon'\r\n",
							"    elif ((pickupHour >= 16) & (pickupHour <= 19)):\r\n",
							"        return 'PMRush'\r\n",
							"    else:\r\n",
							"        return 'Night'\r\n",
							"\r\n",
							"featurized_df = pandas.DataFrame()\r\n",
							"featurized_df['tipped'] = (sampled_df['tipAmount'] > 0).astype('int')\r\n",
							"featurized_df['fareAmount'] = sampled_df['fareAmount'].astype('float32')\r\n",
							"featurized_df['paymentType'] = sampled_df['paymentType'].astype('int')\r\n",
							"featurized_df['passengerCount'] = sampled_df['passengerCount'].astype('int')\r\n",
							"featurized_df['tripDistance'] = sampled_df['tripDistance'].astype('float32')\r\n",
							"featurized_df['pickupHour'] = sampled_df['tpepPickupDateTime'].dt.hour.astype('int')\r\n",
							"featurized_df['tripTimeSecs'] = ((sampled_df['tpepDropoffDateTime'] - sampled_df['tpepPickupDateTime']) / numpy.timedelta64(1, 's')).astype('int')\r\n",
							"\r\n",
							"featurized_df['pickupTimeBin'] = featurized_df.apply(get_pickup_time, axis=1)\r\n",
							"featurized_df = featurized_df.drop(columns='pickupHour')\r\n",
							"\r\n",
							"display(featurized_df.head(5))\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1599713971477
							},
							"collapsed": true
						},
						"source": [
							"filtered_df = featurized_df[(featurized_df.tipped >= 0) & (featurized_df.tipped <= 1)\\\r\n",
							"    & (featurized_df.fareAmount >= 1) & (featurized_df.fareAmount <= 250)\\\r\n",
							"    & (featurized_df.paymentType >= 1) & (featurized_df.paymentType <= 2)\\\r\n",
							"    & (featurized_df.passengerCount > 0) & (featurized_df.passengerCount < 8)\\\r\n",
							"    & (featurized_df.tripDistance >= 0) & (featurized_df.tripDistance <= 100)\\\r\n",
							"    & (featurized_df.tripTimeSecs >= 30) & (featurized_df.tripTimeSecs <= 7200)]\r\n",
							"\r\n",
							"filtered_df.info()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Split training and testing data sets\r\n",
							"- 70% of the data is used to train the model.\r\n",
							"- 30% of the data is used to test the model."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1599713980823
							},
							"collapsed": true
						},
						"source": [
							"from sklearn.model_selection import train_test_split\r\n",
							"\r\n",
							"train_df, test_df = train_test_split(filtered_df, test_size=0.3, random_state=123)\r\n",
							"\r\n",
							"x_train = pandas.DataFrame(train_df.drop(['tipped'], axis = 1))\r\n",
							"y_train = pandas.DataFrame(train_df.iloc[:,train_df.columns.tolist().index('tipped')])\r\n",
							"\r\n",
							"x_test = pandas.DataFrame(test_df.drop(['tipped'], axis = 1))\r\n",
							"y_test = pandas.DataFrame(test_df.iloc[:,test_df.columns.tolist().index('tipped')])"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Export test data as CSV\r\n",
							"Export the test data as a CSV file. Later, we will load the CSV file into Synapse SQL pool to test the model."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1598830320180
							},
							"collapsed": true
						},
						"source": [
							"test_df.to_csv('test_data.csv', index=False)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Train model\r\n",
							"Train a bi-classifier to predict whether a taxi trip will be a tipped or not."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1599713996871
							},
							"collapsed": true
						},
						"source": [
							"from sklearn.compose import ColumnTransformer\r\n",
							"from sklearn.linear_model import LogisticRegression\r\n",
							"from sklearn.pipeline import Pipeline\r\n",
							"from sklearn.impute import SimpleImputer\r\n",
							"from sklearn.preprocessing import StandardScaler, OneHotEncoder\r\n",
							"\r\n",
							"float_features = ['fareAmount', 'tripDistance']\r\n",
							"float_transformer = Pipeline(steps=[\r\n",
							"    ('imputer', SimpleImputer(strategy='median')),\r\n",
							"    ('scaler', StandardScaler())])\r\n",
							"\r\n",
							"integer_features = ['paymentType', 'passengerCount', 'tripTimeSecs']\r\n",
							"integer_transformer = Pipeline(steps=[\r\n",
							"    ('imputer', SimpleImputer(strategy='median')),\r\n",
							"    ('scaler', StandardScaler())])\r\n",
							"\r\n",
							"categorical_features = ['pickupTimeBin']\r\n",
							"categorical_transformer = Pipeline(steps=[\r\n",
							"    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\r\n",
							"\r\n",
							"preprocessor = ColumnTransformer(\r\n",
							"    transformers=[\r\n",
							"        ('float', float_transformer, float_features),\r\n",
							"        ('integer', integer_transformer, integer_features),\r\n",
							"        ('cat', categorical_transformer, categorical_features)\r\n",
							"    ])\r\n",
							"\r\n",
							"clf = Pipeline(steps=[('preprocessor', preprocessor),\r\n",
							"                      ('classifier', LogisticRegression(solver='lbfgs'))])\r\n",
							"\r\n",
							"# Train the model\r\n",
							"clf.fit(x_train, y_train)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1599714001990
							},
							"collapsed": true
						},
						"source": [
							"# Evalute the model\r\n",
							"score = clf.score(x_test, y_test)\r\n",
							"print(score)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Convert the model to ONNX format\r\n",
							"Currently, T-SQL scoring only supports ONNX model format (https://onnx.ai/)."
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1598830324781
							},
							"collapsed": true
						},
						"source": [
							"from skl2onnx import convert_sklearn\r\n",
							"from skl2onnx.common.data_types import FloatTensorType, Int64TensorType, DoubleTensorType, StringTensorType\r\n",
							"\r\n",
							"def convert_dataframe_schema(df, drop=None):\r\n",
							"    inputs = []\r\n",
							"    for k, v in zip(df.columns, df.dtypes):\r\n",
							"        if drop is not None and k in drop:\r\n",
							"            continue\r\n",
							"        if v == 'int64':\r\n",
							"            t = Int64TensorType([1, 1])\r\n",
							"        elif v == 'float32':\r\n",
							"            t = FloatTensorType([1, 1])\r\n",
							"        elif v == 'float64':\r\n",
							"            t = DoubleTensorType([1, 1])\r\n",
							"        else:\r\n",
							"            t = StringTensorType([1, 1])\r\n",
							"        inputs.append((k, t))\r\n",
							"    return inputs\r\n",
							"\r\n",
							"model_inputs = convert_dataframe_schema(x_train)\r\n",
							"onnx_model = convert_sklearn(clf, \"nyc_taxi_tip_predict\", model_inputs)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Register the model with MLFlow"
						],
						"attachments": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1598830390704
							},
							"collapsed": true
						},
						"source": [
							"from azureml.core import Workspace\r\n",
							"\r\n",
							"ws = Workspace.from_config()\r\n",
							"print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"gather": {
								"logged": 1598830404736
							},
							"collapsed": true
						},
						"source": [
							"import mlflow\r\n",
							"import mlflow.onnx\r\n",
							"\r\n",
							"from mlflow.models.signature import infer_signature\r\n",
							"\r\n",
							"experiment_name = 'nyc_taxi_tip_predict_exp'\r\n",
							"artifact_path = 'nyc_taxi_tip_predict_artifact'\r\n",
							"\r\n",
							"mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\r\n",
							"mlflow.set_experiment(experiment_name)\r\n",
							"\r\n",
							"with mlflow.start_run() as run:\r\n",
							"    # Infer signature\r\n",
							"    input_sample = x_train.head(1)\r\n",
							"    output_sample = pandas.DataFrame(columns=['output_label'], data=[1])\r\n",
							"    signature = infer_signature(input_sample, output_sample)\r\n",
							"\r\n",
							"    # Save the model to the outputs directory for capture\r\n",
							"    mlflow.onnx.log_model(onnx_model, artifact_path, signature=signature, input_example=input_sample)\r\n",
							"\r\n",
							"    # Register the model to AML model registry\r\n",
							"    mlflow.register_model('runs:/' + run.info.run_id + '/' + artifact_path, 'nyc_taxi_tip_predict')\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/lgnsynspark31')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/lgnsynspark32')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SampleSQL')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		}
	]
}